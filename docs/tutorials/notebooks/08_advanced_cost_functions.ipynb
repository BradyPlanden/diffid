{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8: Advanced Cost Functions\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand built-in cost metrics (SSE, RMSE, GaussianNLL)\n",
    "- Implement custom cost functions\n",
    "- Apply weighted fitting for heteroscedastic data\n",
    "- Use regularisation to prevent over-fitting\n",
    "- Combine multiple objectives\n",
    "\n",
    "**Prerequisites:** Tutorials 1-2, basic statistics\n",
    "\n",
    "**Runtime:** ~15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The **cost function** (or loss function) quantifies how well model predictions match observations. Chronopt provides built-in metrics, but real-world problems often require:\n",
    "\n",
    "- **Weighted fitting** when measurement errors vary\n",
    "- **Custom metrics** for domain-specific requirements\n",
    "- **Regularisation** to prevent over-fitting\n",
    "- **Multi-objective** combinations\n",
    "\n",
    "This tutorial demonstrates advanced cost function techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import plotting utilities\nimport chronopt as chron\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Cost Metrics\n",
    "\n",
    "Chronopt provides three standard metrics:\n",
    "\n",
    "### 1. Sum of Squared Errors (SSE)\n",
    "$$\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "- Default metric\n",
    "- Penalises large errors heavily\n",
    "- Assumes constant variance\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)  \n",
    "$$\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "- Normalised by sample size\n",
    "- Same units as data\n",
    "- Better for comparing across datasets\n",
    "\n",
    "### 3. Gaussian Negative Log-Likelihood (GaussianNLL)\n",
    "$$\\text{NLL} = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "- Statistically principled\n",
    "- Enables Bayesian inference\n",
    "- Can estimate noise parameter $\\sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple test data\n",
    "x_data = np.linspace(0, 10, 50)\n",
    "y_true = 2.5 * x_data + 1.0\n",
    "y_observed = y_true + np.random.normal(0, 2.0, len(x_data))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, y_observed, \"o\", label=\"Observed\", alpha=0.6, markersize=6)\n",
    "plt.plot(x_data, y_true, \"--\", label=\"True\", linewidth=2)\n",
    "plt.xlabel(\"x\", fontsize=12)\n",
    "plt.ylabel(\"y\", fontsize=12)\n",
    "plt.title(\"Linear Model Test Data\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare built-in metrics\n",
    "def linear_model(params):\n",
    "    \"\"\"Simple linear model: y = slope * x + intercept\"\"\"\n",
    "    slope, intercept = params\n",
    "    return slope * x_data + intercept\n",
    "\n",
    "\n",
    "# Define problem\n",
    "builder = (\n",
    "    chron.VectorBuilder()\n",
    "    .with_objective(linear_model)\n",
    "    .with_data(y_observed)\n",
    "    .with_parameter(\"slope\", 1.0)\n",
    "    .with_parameter(\"intercept\", 0.0)\n",
    ")\n",
    "\n",
    "# Test each metric\n",
    "metrics = {\"SSE\": chron.SSE(), \"RMSE\": chron.RMSE(), \"GaussianNLL\": chron.GaussianNLL()}\n",
    "\n",
    "results = {}\n",
    "for name, metric in metrics.items():\n",
    "    result = builder.with_cost(metric).build().optimise()\n",
    "    results[name] = result\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COST METRIC COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"True parameters:    [2.5, 1.0]\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<15} {'Slope':<12} {'Intercept':<12} {'Cost Value'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<15} {result.x[0]:<12.4f} {result.x[1]:<12.4f} {result.value:<12.3e}\")\n",
    "\n",
    "print(\"\\nüí° Note: All metrics produce similar parameter estimates!\")\n",
    "print(\"   The cost values differ in scale, but optima are equivalent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Fitting: Heteroscedastic Data\n",
    "\n",
    "Real data often has **non-uniform measurement errors** (heteroscedasticity). Points with larger errors should contribute less to the cost function.\n",
    "\n",
    "### Weighted Least Squares\n",
    "$$\\text{WSSE} = \\sum_{i=1}^{n} w_i (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "where $w_i = 1/\\sigma_i^2$ (inverse variance weighting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heteroscedastic data (error increases with x)\n",
    "x_hetero = np.linspace(0, 10, 50)\n",
    "y_true_hetero = 2.0 * x_hetero + 3.0\n",
    "\n",
    "# Error increases linearly with x\n",
    "error_std = 0.5 + 0.3 * x_hetero\n",
    "y_hetero = y_true_hetero + np.random.normal(0, error_std)\n",
    "\n",
    "# Visualize heteroscedastic data\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Error bars showing measurement uncertainty\n",
    "ax.errorbar(\n",
    "    x_hetero,\n",
    "    y_hetero,\n",
    "    yerr=error_std,\n",
    "    fmt=\"o\",\n",
    "    alpha=0.6,\n",
    "    label=\"Observed (with error bars)\",\n",
    "    capsize=3,\n",
    "    markersize=6,\n",
    ")\n",
    "ax.plot(x_hetero, y_true_hetero, \"r--\", linewidth=2, label=\"True\")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=12)\n",
    "ax.set_ylabel(\"y\", fontsize=12)\n",
    "ax.set_title(\n",
    "    \"Heteroscedastic Data (Error Increases with x)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Error range: [{error_std.min():.2f}, {error_std.max():.2f}]\")\n",
    "print(\"Notice: Uncertainty increases from left to right!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weighted SSE cost function\n",
    "class WeightedSSE:\n",
    "    \"\"\"Weighted sum of squared errors.\"\"\"\n",
    "\n",
    "    def __init__(self, weights):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : array_like\n",
    "            Weight for each data point (typically 1/sigma^2)\n",
    "        \"\"\"\n",
    "        self.weights = np.asarray(weights)\n",
    "\n",
    "    def __call__(self, predicted, observed):\n",
    "        \"\"\"Compute weighted SSE.\"\"\"\n",
    "        residuals = observed - predicted\n",
    "        return np.sum(self.weights * residuals**2)\n",
    "\n",
    "\n",
    "# Weights: inverse variance (1/sigma^2)\n",
    "weights = 1.0 / error_std**2\n",
    "\n",
    "\n",
    "def linear_model_hetero(params):\n",
    "    slope, intercept = params\n",
    "    return slope * x_hetero + intercept\n",
    "\n",
    "\n",
    "# Fit WITHOUT weights (standard SSE)\n",
    "result_unweighted = (\n",
    "    chron.VectorBuilder()\n",
    "    .with_objective(linear_model_hetero)\n",
    "    .with_data(y_hetero)\n",
    "    .with_parameter(\"slope\", 1.0)\n",
    "    .with_parameter(\"intercept\", 0.0)\n",
    "    .with_cost(chron.SSE())  # Standard SSE\n",
    "    .build()\n",
    "    .optimise()\n",
    ")\n",
    "\n",
    "# Fit WITH weights\n",
    "result_weighted = (\n",
    "    chron.VectorBuilder()\n",
    "    .with_objective(linear_model_hetero)\n",
    "    .with_data(y_hetero)\n",
    "    .with_parameter(\"slope\", 1.0)\n",
    "    .with_parameter(\"intercept\", 0.0)\n",
    "    .with_cost(WeightedSSE(weights))  # Custom weighted cost\n",
    "    .build()\n",
    "    .optimise()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WEIGHTED vs UNWEIGHTED FITTING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"True parameters:        [2.0, 3.0]\")\n",
    "print(f\"Unweighted fit:         {result_unweighted.x}\")\n",
    "print(f\"Weighted fit:           {result_weighted.x}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"Unweighted error:       {np.linalg.norm(result_unweighted.x - [2.0, 3.0]):.4f}\")\n",
    "print(f\"Weighted error:         {np.linalg.norm(result_weighted.x - [2.0, 3.0]):.4f}\")\n",
    "print(\"\\nüí° Weighted fit is more accurate!\")\n",
    "print(\"   It down-weights noisy (high-x) points appropriately.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "x_plot = np.linspace(0, 10, 200)\n",
    "y_true_plot = 2.0 * x_plot + 3.0\n",
    "y_unweighted = result_unweighted.x[0] * x_plot + result_unweighted.x[1]\n",
    "y_weighted = result_weighted.x[0] * x_plot + result_weighted.x[1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Data with error bars\n",
    "ax.errorbar(\n",
    "    x_hetero,\n",
    "    y_hetero,\n",
    "    yerr=error_std,\n",
    "    fmt=\"o\",\n",
    "    alpha=0.5,\n",
    "    label=\"Data (with uncertainties)\",\n",
    "    capsize=3,\n",
    "    markersize=6,\n",
    "    color=\"gray\",\n",
    ")\n",
    "\n",
    "# Fits\n",
    "ax.plot(x_plot, y_true_plot, \"k--\", linewidth=3, label=\"True\", alpha=0.8)\n",
    "ax.plot(x_plot, y_unweighted, linewidth=2.5, label=\"Unweighted fit\", color=\"#ff7f0e\")\n",
    "ax.plot(x_plot, y_weighted, linewidth=2.5, label=\"Weighted fit\", color=\"#2ca02c\")\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=13)\n",
    "ax.set_ylabel(\"y\", fontsize=13)\n",
    "ax.set_title(\"Weighted vs Unweighted Fitting\", fontsize=15, fontweight=\"bold\")\n",
    "ax.legend(fontsize=12, loc=\"upper left\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight the difference in high-error region\n",
    "ax.axvspan(7, 10, alpha=0.1, color=\"red\", label=\"High uncertainty region\")\n",
    "ax.text(\n",
    "    8.5,\n",
    "    5,\n",
    "    \"High uncertainty\\nregion\",\n",
    "    ha=\"center\",\n",
    "    fontsize=10,\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation: Preventing Over-fitting\n",
    "\n",
    "When fitting complex models with many parameters, **regularisation** prevents over-fitting by penalising large parameter values.\n",
    "\n",
    "### L2 Regularisation (Ridge)\n",
    "$$\\text{Cost} = \\text{Data Term} + \\lambda \\sum_{i=1}^{p} \\theta_i^2$$\n",
    "\n",
    "### L1 Regularisation (Lasso)  \n",
    "$$\\text{Cost} = \\text{Data Term} + \\lambda \\sum_{i=1}^{p} |\\theta_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for polynomial fitting\n",
    "np.random.seed(123)\n",
    "x_poly = np.linspace(-1, 1, 20)\n",
    "y_true_poly = 2 * x_poly - 3 * x_poly**2  # True: quadratic\n",
    "y_poly = y_true_poly + np.random.normal(0, 0.3, len(x_poly))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_poly, y_poly, \"o\", label=\"Observed\", alpha=0.7, markersize=8)\n",
    "plt.plot(x_poly, y_true_poly, \"r--\", linewidth=2, label=\"True (quadratic)\")\n",
    "plt.xlabel(\"x\", fontsize=12)\n",
    "plt.ylabel(\"y\", fontsize=12)\n",
    "plt.title(\"Polynomial Fitting Test Data\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit high-degree polynomial (degree 8) - prone to over-fitting\n",
    "degree = 8\n",
    "\n",
    "\n",
    "def polynomial_model(params):\n",
    "    \"\"\"Polynomial model: sum of coefficients * x^i\"\"\"\n",
    "    y_pred = np.zeros_like(x_poly)\n",
    "    for i, coef in enumerate(params):\n",
    "        y_pred += coef * x_poly**i\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "# Custom cost with L2 regularisation\n",
    "class RegularisedSSE:\n",
    "    \"\"\"SSE with L2 regularisation (Ridge).\"\"\"\n",
    "\n",
    "    def __init__(self, lambda_reg=0.0):\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def __call__(self, predicted, observed, params=None):\n",
    "        # Data term\n",
    "        sse = np.sum((observed - predicted) ** 2)\n",
    "\n",
    "        # Regularisation term (note: params must be passed separately)\n",
    "        if params is not None and self.lambda_reg > 0:\n",
    "            reg_term = self.lambda_reg * np.sum(params**2)\n",
    "            return sse + reg_term\n",
    "\n",
    "        return sse\n",
    "\n",
    "\n",
    "# Note: Chronopt's built-in costs don't support parameter access yet,\n",
    "# so we'll compare by manually adding regularisation to parameter update\n",
    "\n",
    "# Unregularised fit\n",
    "initial_params = np.zeros(degree + 1)\n",
    "initial_params[0] = np.mean(y_poly)\n",
    "\n",
    "builder_poly = chron.VectorBuilder().with_objective(polynomial_model).with_data(y_poly)\n",
    "\n",
    "for i in range(degree + 1):\n",
    "    builder_poly = builder_poly.with_parameter(f\"c{i}\", initial_params[i])\n",
    "\n",
    "result_unreg = (\n",
    "    builder_poly.with_cost(chron.SSE())\n",
    "    .with_optimiser(chron.NelderMead().with_max_iter(2000))\n",
    "    .build()\n",
    "    .optimise()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"POLYNOMIAL FITTING (Degree 8)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Unregularised coefficients:\")\n",
    "print(f\"  {result_unreg.x}\")\n",
    "print(f\"  Max |coef|: {np.max(np.abs(result_unreg.x)):.2f}\")\n",
    "print(f\"  SSE: {result_unreg.value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fits\n",
    "x_dense = np.linspace(-1, 1, 200)\n",
    "\n",
    "\n",
    "def eval_poly(x, coeffs):\n",
    "    y = np.zeros_like(x)\n",
    "    for i, c in enumerate(coeffs):\n",
    "        y += c * x**i\n",
    "    return y\n",
    "\n",
    "\n",
    "y_true_dense = 2 * x_dense - 3 * x_dense**2\n",
    "y_unreg_dense = eval_poly(x_dense, result_unreg.x)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Training data fit\n",
    "ax1.plot(\n",
    "    x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\"\n",
    ")\n",
    "ax1.plot(\n",
    "    x_dense, y_true_dense, \"k--\", linewidth=2.5, label=\"True (quadratic)\", alpha=0.8\n",
    ")\n",
    "ax1.plot(x_dense, y_unreg_dense, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\")\n",
    "ax1.set_xlabel(\"x\", fontsize=12)\n",
    "ax1.set_ylabel(\"y\", fontsize=12)\n",
    "ax1.set_title(\"Training Data: Over-fitting\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(-4, 3)\n",
    "\n",
    "# Right: Extrapolation (shows over-fitting clearly)\n",
    "x_extrap = np.linspace(-1.5, 1.5, 200)\n",
    "y_true_extrap = 2 * x_extrap - 3 * x_extrap**2\n",
    "y_unreg_extrap = eval_poly(x_extrap, result_unreg.x)\n",
    "\n",
    "ax2.plot(\n",
    "    x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\"\n",
    ")\n",
    "ax2.plot(x_extrap, y_true_extrap, \"k--\", linewidth=2.5, label=\"True\", alpha=0.8)\n",
    "ax2.plot(x_extrap, y_unreg_extrap, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\")\n",
    "ax2.axvline(x=-1, color=\"red\", linestyle=\":\", alpha=0.5)\n",
    "ax2.axvline(x=1, color=\"red\", linestyle=\":\", alpha=0.5)\n",
    "ax2.set_xlabel(\"x\", fontsize=12)\n",
    "ax2.set_ylabel(\"y\", fontsize=12)\n",
    "ax2.set_title(\"Extrapolation: Catastrophic Failure\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(-10, 5)\n",
    "ax2.text(-1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\")\n",
    "ax2.text(1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Over-fitting Alert!\")\n",
    "print(\"   The high-degree polynomial fits training data well but\")\n",
    "print(\"   extrapolates poorly. Regularisation would help!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Objective Cost Functions\n",
    "\n",
    "Sometimes we want to balance **multiple objectives** simultaneously:\n",
    "\n",
    "$$\\text{Cost} = w_1 \\cdot \\text{Fit Quality} + w_2 \\cdot \\text{Smoothness} + w_3 \\cdot \\text{Physical Constraints}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Fit with smoothness penalty\n",
    "class MultiObjectiveCost:\n",
    "    \"\"\"Combined data fit + smoothness penalty.\"\"\"\n",
    "\n",
    "    def __init__(self, weight_fit=1.0, weight_smooth=0.1):\n",
    "        self.weight_fit = weight_fit\n",
    "        self.weight_smooth = weight_smooth\n",
    "\n",
    "    def __call__(self, predicted, observed):\n",
    "        # Data fit term (SSE)\n",
    "        fit_cost = np.sum((observed - predicted) ** 2)\n",
    "\n",
    "        # Smoothness term (penalise large second derivatives)\n",
    "        second_deriv = np.diff(predicted, n=2)\n",
    "        smooth_cost = np.sum(second_deriv**2)\n",
    "\n",
    "        return self.weight_fit * fit_cost + self.weight_smooth * smooth_cost\n",
    "\n",
    "\n",
    "# Fit with different smoothness weights\n",
    "weights_smooth = [0.0, 0.1, 1.0, 10.0]\n",
    "results_multi = {}\n",
    "\n",
    "for w_smooth in weights_smooth:\n",
    "    result = chron.VectorBuilder().with_objective(polynomial_model).with_data(y_poly)\n",
    "\n",
    "    for i in range(degree + 1):\n",
    "        result = result.with_parameter(f\"c{i}\", initial_params[i])\n",
    "\n",
    "    result = (\n",
    "        result.with_cost(MultiObjectiveCost(weight_fit=1.0, weight_smooth=w_smooth))\n",
    "        .with_optimiser(chron.NelderMead().with_max_iter(2000))\n",
    "        .build()\n",
    "        .optimise()\n",
    "    )\n",
    "\n",
    "    results_multi[w_smooth] = result\n",
    "    print(\n",
    "        f\"Smoothness weight={w_smooth:5.1f}: SSE={np.sum((eval_poly(x_poly, result.x) - y_poly) ** 2):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize effect of smoothness weight\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (w_smooth, result) in enumerate(results_multi.items()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    y_fit = eval_poly(x_dense, result.x)\n",
    "\n",
    "    ax.plot(x_poly, y_poly, \"o\", label=\"Data\", alpha=0.6, markersize=7, color=\"gray\")\n",
    "    ax.plot(x_dense, y_true_dense, \"k--\", linewidth=2, label=\"True\", alpha=0.7)\n",
    "    ax.plot(x_dense, y_fit, linewidth=2.5, label=f\"Fit (Œª={w_smooth})\", color=\"#2ca02c\")\n",
    "\n",
    "    ax.set_xlabel(\"x\", fontsize=11)\n",
    "    ax.set_ylabel(\"y\", fontsize=11)\n",
    "    ax.set_title(f\"Smoothness Weight Œª = {w_smooth}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-4, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Smoothness Penalty Effect:\")\n",
    "print(\"   Œª=0.0:  No penalty ‚Üí over-fitting\")\n",
    "print(\"   Œª=0.1:  Slight smoothing\")\n",
    "print(\"   Œª=1.0:  Balanced fit\")\n",
    "print(\"   Œª=10.0: Too smooth ‚Üí under-fitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Constraints as Cost Penalties\n",
    "\n",
    "In scientific applications, we often have **physical constraints**:\n",
    "- Parameters must be positive (e.g., rate constants)\n",
    "- Conservation laws (e.g., mass balance)\n",
    "- Monotonicity requirements\n",
    "\n",
    "These can be enforced via penalty terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstrainedCost:\n",
    "    \"\"\"Cost with soft constraints via penalties.\"\"\"\n",
    "\n",
    "    def __init__(self, penalty_weight=1000.0):\n",
    "        self.penalty_weight = penalty_weight\n",
    "\n",
    "    def __call__(self, predicted, observed, params=None):\n",
    "        # Data fit term\n",
    "        sse = np.sum((observed - predicted) ** 2)\n",
    "\n",
    "        # Example constraint: parameters should sum to a target value\n",
    "        if params is not None:\n",
    "            # Soft constraint: sum(params) ‚âà 0\n",
    "            constraint_violation = (np.sum(params) - 0.0) ** 2\n",
    "\n",
    "            # Positivity constraint: penalise negative parameters\n",
    "            negative_penalty = np.sum(np.minimum(0, params) ** 2)\n",
    "\n",
    "            penalty = self.penalty_weight * (constraint_violation + negative_penalty)\n",
    "            return sse + penalty\n",
    "\n",
    "        return sse\n",
    "\n",
    "\n",
    "print(\"Example constraint penalties:\")\n",
    "print(\"  ‚Ä¢ Sum constraint: forces Œ£Œ∏·µ¢ ‚âà target\")\n",
    "print(\"  ‚Ä¢ Positivity: penalises Œ∏·µ¢ < 0\")\n",
    "print(\"  ‚Ä¢ Bounds: penalises Œ∏·µ¢ outside [a, b]\")\n",
    "print(\"  ‚Ä¢ Monotonicity: penalises Œ∏·µ¢‚Çä‚ÇÅ < Œ∏·µ¢\")\n",
    "print(\"\\nüí° Tip: Start with large penalty weights, then tune.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Choosing a Cost Function\n",
    "\n",
    "1. **Default (SSE)**: Good starting point for most problems\n",
    "2. **RMSE**: When comparing across datasets with different sizes\n",
    "3. **GaussianNLL**: For Bayesian inference and uncertainty quantification\n",
    "4. **Weighted**: When measurement errors vary (heteroscedastic data)\n",
    "5. **Custom**: For domain-specific requirements\n",
    "\n",
    "### Regularisation Guidelines\n",
    "\n",
    "- **When to use**: High-dimensional problems, limited data, polynomial fitting\n",
    "- **L2 (Ridge)**: Shrinks all coefficients smoothly\n",
    "- **L1 (Lasso)**: Promotes sparsity (some coefficients ‚Üí 0)\n",
    "- **Tuning Œª**: Cross-validation or information criteria (AIC, BIC)\n",
    "\n",
    "### Multi-Objective Balancing\n",
    "\n",
    "- **Start simple**: Fit data first, add penalties incrementally\n",
    "- **Scale matters**: Normalise objectives to similar magnitudes\n",
    "- **Weight tuning**: Use logarithmic search (0.001, 0.01, 0.1, 1, 10, ...)\n",
    "- **Validation**: Check if constraints are actually satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Cost Function Design Pattern\n",
    "\n",
    "```python\n",
    "class CustomCost:\n",
    "    def __init__(self, **hyperparameters):\n",
    "        # Store configuration\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, predicted, observed, params=None):\n",
    "        # 1. Data fidelity term\n",
    "        fit_cost = compute_fit(predicted, observed)\n",
    "        \n",
    "        # 2. Regularisation term (optional)\n",
    "        reg_cost = compute_regularisation(params)\n",
    "        \n",
    "        # 3. Physical constraints (optional)\n",
    "        constraint_cost = compute_penalties(params)\n",
    "        \n",
    "        # 4. Combine with weights\n",
    "        return w1*fit_cost + w2*reg_cost + w3*constraint_cost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Built-in metrics** (SSE, RMSE, GaussianNLL) cover most use cases\n",
    "2. **Weighted fitting** essential for heteroscedastic data (varying errors)\n",
    "3. **Regularisation** prevents over-fitting in high-dimensional problems\n",
    "4. **Multi-objective costs** balance competing goals (fit vs smoothness)\n",
    "5. **Constraint penalties** enforce physical requirements\n",
    "6. **Custom costs** are easy to implement via `__call__` interface\n",
    "7. **Scale and normalisation** critical for multi-term objectives\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Guide: Cost Metrics](../../guides/cost-metrics.md) - Detailed metric selection\n",
    "- [Tutorial 3: Parameter Uncertainty](03_parameter_uncertainty.ipynb) - Bayesian inference with GaussianNLL\n",
    "- [API Reference: Cost Metrics](../../api-reference/python/cost-metrics.md) - Complete API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Implement L1 Regularisation**: Create a `LassoSSE` class and compare with L2\n",
    "\n",
    "2. **Cross-Validation**: Implement k-fold cross-validation to tune regularisation strength\n",
    "\n",
    "3. **Robust Fitting**: Implement Huber loss (robust to outliers):\n",
    "   $$L_\\delta(r) = \\begin{cases} \\frac{1}{2}r^2 & \\text{for } |r| \\leq \\delta \\\\ \\delta(|r| - \\frac{1}{2}\\delta) & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "4. **Time-Series Smoothness**: Add a penalty on parameter changes over time for dynamic fitting\n",
    "\n",
    "5. **Information Criteria**: Compute AIC and BIC for model selection:\n",
    "   - AIC = $2k - 2\\ln(\\mathcal{L})$\n",
    "   - BIC = $\\ln(n)k - 2\\ln(\\mathcal{L})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}