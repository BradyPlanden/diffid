{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Optimization Basics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the ScalarBuilder API\n",
    "- Optimize the classic Rosenbrock function\n",
    "- Visualize optimization landscapes with contour plots\n",
    "- Compare different optimizers\n",
    "\n",
    "**Prerequisites:** Basic Python, NumPy\n",
    "\n",
    "**Runtime:** ~5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Rosenbrock function is a classic test problem in optimization. It's defined as:\n",
    "\n",
    "$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$\n",
    "\n",
    "The global minimum is at $(1, 1)$ with $f(1, 1) = 0$. Despite being simple to state, it's challenging for optimizers because of its narrow, curved valley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting utilities\n",
    "import sys\n",
    "\n",
    "import chronopt as chron\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from utils import plot_contour_2d, setup_plotting\n",
    "\n",
    "setup_plotting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Objective Function\n",
    "\n",
    "In Chronopt, objective functions must:\n",
    "1. Accept a NumPy array as input\n",
    "2. Return a NumPy array as output (even for scalar values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    \"\"\"The Rosenbrock function - a classic optimization test problem.\"\"\"\n",
    "    value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n",
    "    return np.array([value], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Problem\n",
    "\n",
    "Use `ScalarBuilder` for direct function optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = (\n",
    "    chron.ScalarBuilder()\n",
    "    .with_callable(rosenbrock)\n",
    "    .with_parameter(\"x\", 1.0)  # Initial guess\n",
    "    .with_parameter(\"y\", 1.0)  # Initial guess\n",
    ")\n",
    "problem = builder.build()\n",
    "\n",
    "print(\"Problem built successfully!\")\n",
    "print(\"Number of parameters: 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with Default Settings\n",
    "\n",
    "The `optimise()` method uses Nelder-Mead by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = problem.optimise(initial=[10.0, 10.0])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Optimal parameters: {result.x}\")\n",
    "print(f\"Objective value: {result.value:.3e}\")\n",
    "print(f\"Iterations: {result.iterations}\")\n",
    "print(f\"Function evaluations: {result.evaluations}\")\n",
    "print(f\"Message: {result.message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Optimization Landscape\n",
    "\n",
    "Let's create a contour plot to see the function's shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contour plot\n",
    "fig, ax = plot_contour_2d(\n",
    "    rosenbrock,\n",
    "    xlim=(-2, 2),\n",
    "    ylim=(-1, 3),\n",
    "    levels=np.logspace(-1, 3.5, 20),\n",
    "    optimum=(1.0, 1.0),\n",
    "    found=(result.x[0], result.x[1]),\n",
    "    title=\"Rosenbrock Function Landscape\",\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Distance from true optimum: {np.linalg.norm(result.x - [1.0, 1.0]):.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Optimizers\n",
    "\n",
    "Let's compare Nelder-Mead, CMA-ES, and Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "optimizers = {\n",
    "    \"Nelder-Mead\": chron.NelderMead().with_max_iter(1000),\n",
    "    \"CMA-ES\": chron.CMAES().with_max_iter(300).with_step_size(0.5),\n",
    "    \"Adam\": chron.Adam().with_max_iter(1000).with_step_size(0.01),\n",
    "}\n",
    "\n",
    "# Test starting point\n",
    "initial_guess = [-1.5, -0.5]\n",
    "\n",
    "# Run all optimizers\n",
    "results = {}\n",
    "for name, optimizer in optimizers.items():\n",
    "    result = optimizer.run(problem, initial_guess)\n",
    "    results[name] = result\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OPTIMIZER COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    f\"{'Optimizer':<15} {'Success':<10} {'Final Value':<15} {'Iterations':<12} {'Evaluations'}\"\n",
    ")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(\n",
    "        f\"{name:<15} {str(result.success):<10} {result.value:<15.3e} \"\n",
    "        f\"{result.iterations:<12} {result.evaluations}\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFinal parameters:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<15} x = {result.x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contour plot with all optimizers' results\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rosenbrock([X[i, j], Y[i, j]])[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Plot contours\n",
    "levels = np.logspace(-1, 3.5, 20)\n",
    "cs = ax.contour(X, Y, Z, levels=levels, cmap=\"viridis\", alpha=0.6)\n",
    "ax.clabel(cs, inline=True, fontsize=8)\n",
    "\n",
    "# Plot true optimum\n",
    "ax.plot(1.0, 1.0, \"r*\", markersize=20, label=\"True minimum\", zorder=5)\n",
    "\n",
    "# Plot starting point\n",
    "ax.plot(\n",
    "    initial_guess[0],\n",
    "    initial_guess[1],\n",
    "    \"kx\",\n",
    "    markersize=15,\n",
    "    markeredgewidth=3,\n",
    "    label=\"Starting point\",\n",
    "    zorder=5,\n",
    ")\n",
    "\n",
    "# Plot optimizer results\n",
    "colors = {\"Nelder-Mead\": \"blue\", \"CMA-ES\": \"green\", \"Adam\": \"orange\"}\n",
    "markers = {\"Nelder-Mead\": \"o\", \"CMA-ES\": \"s\", \"Adam\": \"^\"}\n",
    "\n",
    "for name, result in results.items():\n",
    "    ax.plot(\n",
    "        result.x[0],\n",
    "        result.x[1],\n",
    "        marker=markers[name],\n",
    "        color=colors[name],\n",
    "        markersize=12,\n",
    "        label=name,\n",
    "        zorder=5,\n",
    "        markeredgecolor=\"black\",\n",
    "        markeredgewidth=1.5,\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"x\", fontsize=12)\n",
    "ax.set_ylabel(\"y\", fontsize=12)\n",
    "ax.set_title(\"Optimizer Comparison on Rosenbrock Function\", fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(loc=\"upper left\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Starting Point\n",
    "\n",
    "Let's see how starting position affects convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple starting points\n",
    "starting_points = [\n",
    "    [-1.5, -0.5],\n",
    "    [1.5, 1.5],\n",
    "    [0.0, 2.0],\n",
    "    [-1.0, 1.0],\n",
    "]\n",
    "\n",
    "optimizer = chron.NelderMead().with_max_iter(1000)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING POINT SENSITIVITY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, start in enumerate(starting_points, 1):\n",
    "    result = optimizer.run(problem, start)\n",
    "    error = np.linalg.norm(result.x - [1.0, 1.0])\n",
    "\n",
    "    print(f\"\\nStart {i}: {start}\")\n",
    "    print(f\"  Final:      {result.x}\")\n",
    "    print(f\"  Iterations: {result.iterations}\")\n",
    "    print(f\"  Error:      {error:.3e}\")\n",
    "    print(f\"  Success:    {result.success}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **ScalarBuilder** is used for direct function optimization\n",
    "2. **Nelder-Mead** is the default optimizer - good for small problems\n",
    "3. **CMA-ES** is more robust for global search but uses more evaluations\n",
    "4. **Adam** can be fast on smooth problems but may struggle on complex landscapes\n",
    "5. Starting point can significantly affect convergence speed\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Tutorial 2: ODE Fitting with DiffSL](02_ode_fitting_diffsol.ipynb) - Learn how to fit differential equations\n",
    "- [Choosing an Optimizer](../../guides/choosing-optimizer.md) - Detailed optimizer selection guide\n",
    "- [API Reference: Optimizers](../../api-reference/python/optimizers.md) - Complete API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these challenges:\n",
    "\n",
    "1. **Rastrigin Function**: Implement and optimize:\n",
    "   $$f(x, y) = 20 + x^2 + y^2 - 10(\\cos(2\\pi x) + \\cos(2\\pi y))$$\n",
    "   \n",
    "2. **Parameter Tuning**: Experiment with CMA-ES `step_size` parameter (try 0.1, 0.5, 1.0, 2.0)\n",
    "\n",
    "3. **Constraint Handling**: How would you restrict the search to $x, y \\in [-2, 2]$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
