{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#diffid","title":"Diffid","text":"<p>differential identification is a Rust-first toolkit for time-series inference and optimisation with ergonomic Python bindings. It couples high-performance solvers with a highly customisable builder API for identification and optimisation of differential systems.</p>"},{"location":"#why-diffid","title":"Why Diffid?","text":"<p>Diffid offers a different paradigm for a parameter inference library. Conventionally, Python-based inference libraries are constructed via python bindings to a high-performance forward model with the inference algorithms implemented in Python. Alongside this approach, Diffid introduces an alternative, where the Python layer acts purely as a declarative configuration interface, while all computationally intensive work (the optimisation / sampling loop, gradient calculations, etc.) happens entirely within the Rust runtime without crossing the FFI boundary repeatedly. This is architecture is presented visually below,</p> <p></p> Conventional vs configuration approach: the optimisation loop moves from Python to Rust"},{"location":"#core-capabilities","title":"Core Capabilities","text":"<ul> <li> <p>Optimisation Algorithms</p> <p>Gradient-free (Nelder-Mead, CMA-ES) and gradient-based (Adam) optimisers with configurable convergence criteria</p> </li> <li> <p>High-Performance ODE Fitting</p> <p>Multi-threaded differential equation fitting via DiffSL with dense or sparse Diffsol backends</p> </li> <li> <p>Uncertainty Quantification</p> <p>Customisable likelihood/cost metrics and Monte-Carlo sampling for posterior exploration</p> </li> <li> <p>Flexible Integration</p> <p>Integration with state-of-the-art differential solvers: Diffrax, DifferentialEquations.jl</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Diffid targets Python &gt;= 3.11. Windows builds are currently marked experimental.</p> pipuv <pre><code>pip install diffid\n\n# Optional extras for plotting\npip install \"diffid[plotting]\"\n</code></pre> <pre><code>uv pip install diffid\n\n# Optional extras for plotting\nuv pip install \"diffid[plotting]\"\n</code></pre>"},{"location":"#example-scalar-optimisation","title":"Example: Scalar Optimisation","text":"<pre><code>import numpy as np\nimport diffid\n\ndef rosenbrock(x):\n    value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n    return np.asarray([value])\n\nbuilder = (\n    diffid.ScalarBuilder()\n    .with_objective(rosenbrock)\n    .with_parameter(\"x\", 1.5)\n    .with_parameter(\"y\", -1.5)\n)\nproblem = builder.build()\nresult = problem.optimise()\n\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\nprint(f\"Success: {result.success}\")\n</code></pre>"},{"location":"#example-ode-fitting","title":"Example: ODE Fitting","text":"<pre><code>import numpy as np\nimport diffid\n\n# Logistic growth model in DiffSL\ndsl = \"\"\"\nin_i {r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\"\"\"\n\nt = np.linspace(0.0, 5.0, 51)\nobservations = np.exp(-1.3 * t)\ndata = np.column_stack((t, observations))\n\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_backend(\"dense\")\n)\nproblem = builder.build()\n\noptimiser = diffid.CMAES().with_max_iter(1000)\nresult = optimiser.run(problem, [0.5, 0.5])\n\nprint(result.x)\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Tutorials</p> <p>Interactive Jupyter notebooks for hands-on learning</p> </li> <li> <p> User Guides</p> <p>In-depth guides on choosing and tuning algorithms</p> </li> <li> <p> Examples Gallery</p> <p>Visual gallery of example applications</p> </li> <li> <p> Development</p> <p>Contributing, architecture, and building from source</p> </li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p>"},{"location":"changelog/#030-2026-01-25","title":"[0.3.0] - 2026-01-25","text":""},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Python package renamed from <code>chronopt</code> to <code>diffid</code></li> <li>Ask-Tell interface for optimisers and samplers, enabling stateful step-by-step execution</li> <li>Optimisers unified into an <code>Optimiser</code> enum with shared <code>run()</code> API</li> <li><code>ParameterRange</code> type with improved Bounds API supporting unbounded parameters</li> <li>Python bindings enhanced with magic methods (<code>__repr__</code>, <code>__str__</code>, etc.) for better REPL experience</li> <li>Cached Diffsol parallelisation with single solver build per thread and improved multithreaded error management</li> <li>Improved error hierarchy with <code>EvaluationError</code>, <code>TellError</code>, and <code>ProblemBuilderError</code> distinctions</li> <li>Added MCMC proposals for Dynamic Nested Sampling with corresponding benchmarks</li> </ul>"},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Result attributes renamed: <code>sigma0</code> \u2192 <code>step_size</code>, standardised to <code>evaluations</code> and <code>iterations</code></li> <li>Codebase aligned to British English spelling conventions</li> <li>Samplers refactored with Ask-Tell interface, matching optimiser patterns</li> <li>Module restructure: <code>types.rs</code> moved to library level, error types consolidated</li> <li><code>ScalarEvaluation</code> and <code>GradientEvaluation</code> now use <code>TryFrom</code> trait implementations</li> <li><code>Objective</code> trait extended with <code>has_gradient()</code> method</li> <li>Optimiser <code>.tell()</code> and <code>.run()</code> methods use relaxed error management</li> </ul>"},{"location":"changelog/#fixes","title":"Fixes","text":"<ul> <li>Incorrectly indexed proposal storage in Dynamic Nested Sampling</li> <li>Patience convergence criterion now uses proper type conversions</li> </ul>"},{"location":"changelog/#020-2025-12-01","title":"[0.2.0] - 2025-12-01","text":""},{"location":"changelog/#features_1","title":"Features","text":"<ul> <li>Dynamic Nested Sampling (DNS) sampler with Rust and Python bindings, including a dedicated scheduler, proposal generator, result struct, and corresponding tests and stubs.</li> <li>Support for sensitivities in <code>DiffsolProblem</code> across dense and sparse backends, exposing gradient support through <code>CostMetric</code> and <code>problem.evaluate_with_grad(...)</code>.</li> <li>Problem-level parallel evaluation control, parallel execution support for DNS, and a <code>time</code> attribute on sampler results, with parallel sampler tests.</li> <li>Adds ADAM optimisation algorithm with Python bindings and stub support.</li> <li>Multi-cost / vectored cost support, including cost weighting utilities for composing and weighting multiple metrics.</li> <li>Diffsol backend configuration updated to use LLVM-18 for sensitivity calculations and improved performance.</li> <li>Adds benchmarking suite for Diffsol, Optimiser, and Samplers via <code>cargo bench</code></li> </ul>"},{"location":"changelog/#fixes_1","title":"Fixes","text":"<ul> <li>Refactor DiffsolProblem's evaluate function for reduced clone's and aligned error management</li> <li>Several robustness fixes to Diffsol integration, including catching panics during failure cases and rebuilding with penalties.</li> <li>CMA-ES bug fixes and bound handling improvements, with associated tests.</li> <li>Corrected <code>PyProblem.evaluate</code> gradient argument handling on the Python side.</li> <li>Miscellaneous clippy warnings and CI-related issues.</li> </ul>"},{"location":"changelog/#breaking","title":"Breaking","text":"<ul> <li>Vector-valued optimisation via <code>VectorProblemBuilder</code> and associated <code>Problem</code> type, alongside renamed <code>ScalarProblemBuilder</code> for scalar objectives.</li> <li><code>build</code> methods on problem builders no longer consume the builder, enabling repeated <code>build</code> calls and multi-build workflows.</li> </ul>"},{"location":"changelog/#examples","title":"Examples","text":"<ul> <li>New dynamic nested sampling and model evidence examples with updated evidence values using more reasonable live point counts.</li> <li>New bicycle model identification example with model selection via evidence.</li> <li>Expanded predator\u2013prey example suite, including a dedicated subdirectory comparing solver backends and a Diffeqpy-based identification workflow.</li> <li>Updated JAX predator\u2013prey example to use Diffrax.</li> </ul>"},{"location":"changelog/#010-2025-11-05","title":"[0.1.0] - 2025-11-05","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial Chronopt release combining the Rust optimisation core with Python bindings via PyO3 and maturin packaging.</li> <li>Diffsol integration with configurable dense and sparse backends, gradient support, and builder APIs for solver configuration.</li> <li>Suite of optimisation algorithms including Nelder\u2013Mead, CMA-ES, patience-based halting, and parallel evaluation helpers.</li> <li>Cost metric implementations (SSE, RMSE, NLL) with Python-facing builder helpers and validation.</li> <li>Sampling module with the initial Metropolis\u2013Hastings implementation and worked Python examples (e.g., bouncy ball, predator\u2013prey, Lotka\u2013Volterra).</li> <li>Extensive unit and integration tests covering builders, diffsol backends, sampling, and mathematical benchmarking suites.</li> <li>Automated stub generation helpers, documentation updates, and GitHub Actions workflows for CI and trusted-publisher PyPI releases.</li> </ul>"},{"location":"algorithms/","title":"Algorithms","text":"<p>Detailed documentation for each optimisation and sampling algorithm in Diffid.</p>"},{"location":"algorithms/#optimisers","title":"Optimisers","text":"<p>Gradient-free and gradient-based algorithms for finding optimal parameters.</p> <ul> <li> <p> Nelder-Mead</p> <p>Simplex-based gradient-free optimiser for local search.</p> <p> Details</p> </li> <li> <p> CMA-ES</p> <p>Covariance Matrix Adaptation Evolution Strategy for global optimisation.</p> <p> Details</p> </li> <li> <p> Adam</p> <p>Adaptive Moment Estimation for gradient-based optimisation.</p> <p> Details</p> </li> </ul>"},{"location":"algorithms/#samplers","title":"Samplers","text":"<p>MCMC and nested sampling for uncertainty quantification and model comparison.</p> <ul> <li> <p> Metropolis-Hastings</p> <p>MCMC sampling for posterior exploration.</p> <p> Details</p> </li> <li> <p> Dynamic Nested Sampling</p> <p>Evidence calculation for Bayesian model comparison.</p> <p> Details</p> </li> </ul>"},{"location":"algorithms/#algorithm-comparison","title":"Algorithm Comparison","text":"Algorithm Type Gradients Best For Parallelisable Nelder-Mead Local No &lt; 10 params, noisy No CMA-ES Global No 10-100+ params Yes Adam Local Yes Smooth objectives No Metropolis-Hastings MCMC No Uncertainty No Nested Sampling Evidence No Model comparison Yes"},{"location":"algorithms/#choosing-an-algorithm","title":"Choosing an Algorithm","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Need uncertainty?}\n    B --&gt;|No| C{Gradients available?}\n    B --&gt;|Yes| D{Need evidence?}\n    C --&gt;|Yes| E[Adam]\n    C --&gt;|No| F{Problem size?}\n    D --&gt;|Yes| G[Nested Sampling]\n    D --&gt;|No| H[Metropolis-Hastings]\n    F --&gt;|&lt; 10 params| I[Nelder-Mead]\n    F --&gt;|&gt; 10 params| J[CMA-ES]</code></pre>"},{"location":"algorithms/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"algorithms/#convergence-speed","title":"Convergence Speed","text":"<p>Fast \u2192 Slow:</p> <ol> <li>Adam (with gradients)</li> <li>Nelder-Mead (small problems)</li> <li>CMA-ES (large problems)</li> <li>MCMC samplers (many evaluations)</li> <li>Nested sampling (most evaluations)</li> </ol>"},{"location":"algorithms/#robustness-to-local-minima","title":"Robustness to Local Minima","text":"<p>Least \u2192 Most robust:</p> <ol> <li>Adam (gradient descent)</li> <li>Nelder-Mead (local search)</li> <li>CMA-ES (global search)</li> <li>MCMC (explores posterior)</li> <li>Nested sampling (explores full space)</li> </ol>"},{"location":"algorithms/#implementation-details","title":"Implementation Details","text":"<p>All algorithms are implemented in Rust for performance:</p> <ul> <li>Zero-copy: Efficient memory usage</li> <li>Parallel: Where applicable (CMA-ES, nested sampling)</li> <li>Numerically stable: Careful floating-point handling</li> <li>Well-tested: Comprehensive test suite</li> </ul> <p>Source code: rust/src/optimisers/</p>"},{"location":"algorithms/#references","title":"References","text":"<p>Each algorithm page includes references to original papers and implementation details.</p>"},{"location":"algorithms/#see-also","title":"See Also","text":"<ul> <li>Choosing an Optimiser</li> <li>Tuning Optimisers</li> <li>API Reference</li> <li>Tutorials</li> </ul>"},{"location":"algorithms/optimisers/adam/","title":"Adam Algorithm","text":"<p>Adaptive Moment Estimation (Adam) is a first-order gradient-based optimiser that maintains adaptive learning rates for each parameter using estimates of first and second moments of the gradients.</p>"},{"location":"algorithms/optimisers/adam/#algorithm-overview","title":"Algorithm Overview","text":"<p>Adam combines the benefits of AdaGrad (adapting to sparse gradients) and RMSprop (adapting to non-stationary objectives) by tracking exponential moving averages of both the gradient and squared gradient.</p>"},{"location":"algorithms/optimisers/adam/#key-properties","title":"Key Properties","text":"Property Value Type Local, gradient-based Parallelisable No Function evaluations 1 per iteration Best for Smooth, differentiable objectives"},{"location":"algorithms/optimisers/adam/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>At each iteration \\(t\\), Adam updates parameters \\(\\theta\\) using:</p> <p>Gradient computation:</p> \\[ g_t = \\nabla_\\theta f(\\theta_{t-1}) \\] <p>Biased moment estimates: $$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$ $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$</p> <p>Bias correction: $$ \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} $$ $$ \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t} $$</p> <p>Parameter update: $$ \\theta_t = \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} $$</p> <p>Where \\(\\alpha\\) is the learning rate (step size).</p>"},{"location":"algorithms/optimisers/adam/#parameters","title":"Parameters","text":"Parameter Default Description <code>max_iter</code> 1000 Maximum iterations <code>step_size</code> 0.01 Learning rate \\(\\alpha\\) <code>beta1</code> 0.9 First moment decay rate \\(\\beta_1\\) <code>beta2</code> 0.999 Second moment decay rate \\(\\beta_2\\) <code>eps</code> 1e-8 Numerical stability constant \\(\\epsilon\\) <code>threshold</code> 1e-6 Objective convergence tolerance <code>gradient_threshold</code> None Gradient norm convergence tolerance <code>patience</code> None Timeout in seconds"},{"location":"algorithms/optimisers/adam/#convergence-criteria","title":"Convergence Criteria","text":"<p>The algorithm terminates when any condition is met:</p> <ol> <li>Iteration limit: <code>iteration &gt;= max_iter</code></li> <li>Gradient norm: \\(\\|g_t\\| &lt;\\) <code>gradient_threshold</code></li> <li>Objective change: Change in objective below <code>threshold</code></li> <li>Patience: Elapsed time exceeds <code>patience</code> seconds</li> </ol>"},{"location":"algorithms/optimisers/adam/#tuning-guidance","title":"Tuning Guidance","text":"<p>Learning rate (<code>step_size</code>) is the most critical parameter:</p> <ul> <li>Start with a conservative step-size (i.e, 1e-3)</li> <li>Try orders of magnitude: (1e-2 to 1e-4)</li> <li>Too large: oscillation, divergence, or overshooting</li> <li>Too small: slow convergence</li> </ul> <p>Beta parameters rarely need adjustment:</p> <ul> <li>\\(\\beta_1 = 0.9\\): controls momentum (gradient smoothing)</li> <li>\\(\\beta_2 = 0.999\\): controls adaptive scaling (variance smoothing)</li> <li>Lower \\(\\beta_1\\) for less momentum, more responsiveness</li> <li>Lower \\(\\beta_2\\) for faster adaptation to gradient scale changes</li> </ul> <p>Epsilon almost never needs tuning:</p> <ul> <li>Prevents division by zero when gradients are near zero</li> <li>Default 1e-8 works for most cases</li> </ul>"},{"location":"algorithms/optimisers/adam/#when-to-use","title":"When to Use","text":"<p>Strengths:</p> <ul> <li>Fast convergence on smooth objectives</li> <li>Per-parameter adaptive learning rates</li> <li>Handles sparse gradients well</li> <li>Works with noisy gradients (e.g., mini-batches)</li> <li>Low memory overhead</li> </ul> <p>Limitations:</p> <ul> <li>Requires gradient computation (automatic differentiation or numerical)</li> <li>Converges to local minima (no global search)</li> <li>Sensitive to learning rate choice</li> <li>May oscillate near optima</li> </ul>"},{"location":"algorithms/optimisers/adam/#example","title":"Example","text":"<pre><code>import diffid as chron\n\noptimiser = (\n    diffid.Adam()\n    .with_max_iter(5000)\n    .with_step_size(0.001)\n    .with_betas(0.9, 0.999)\n    .with_threshold(1e-8)\n)\n\nresult = optimiser.run(problem, initial_guess=[1.0, 2.0])\n</code></pre>"},{"location":"algorithms/optimisers/adam/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Supports automatic numerical gradient computation via central differences</li> <li>Bias correction is essential for early iterations</li> <li>Tracks both current position and best-found position</li> </ul>"},{"location":"algorithms/optimisers/adam/#references","title":"References","text":"<ol> <li>Kingma, D.P. and Ba, J. (2015). \"Adam: A Method for Stochastic Optimization\". ICLR 2015. arXiv:1412.6980.</li> <li>Reddi, S.J. et al. (2018). \"On the Convergence of Adam and Beyond\". ICLR 2018.</li> </ol>"},{"location":"algorithms/optimisers/adam/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Choosing an Optimiser</li> <li>Tuning Optimisers</li> </ul>"},{"location":"algorithms/optimisers/cmaes/","title":"CMA-ES Algorithm","text":"<p>Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a stochastic, derivative-free algorithm for global optimisation. It maintains a multivariate Gaussian distribution that adapts its covariance structure to the objective landscape.</p>"},{"location":"algorithms/optimisers/cmaes/#algorithm-overview","title":"Algorithm Overview","text":"<p>CMA-ES samples a population of candidate solutions from a multivariate normal distribution, ranks them by fitness, and updates the distribution parameters to bias future samples towards better regions.</p>"},{"location":"algorithms/optimisers/cmaes/#key-properties","title":"Key Properties","text":"Property Value Type Global, gradient-free Parallelisable Yes (population evaluation) Function evaluations \\(\\lambda\\) per generation Best dimensions 10-100+ parameters"},{"location":"algorithms/optimisers/cmaes/#algorithm","title":"Algorithm","text":"<p>The algorithm samples offspring from:</p> \\[ x_k \\sim m + \\sigma \\cdot \\mathcal{N}(0, C) \\] <p>Where:</p> <ul> <li>\\(m\\) is the distribution mean (current best estimate)</li> <li>\\(\\sigma\\) is the global step size</li> <li>\\(C\\) is the covariance matrix</li> </ul>"},{"location":"algorithms/optimisers/cmaes/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Sample: Generate \\(\\lambda\\) offspring from \\(\\mathcal{N}(m, \\sigma^2 C)\\)</li> <li>Evaluate: Compute fitness for all offspring (parallelisable)</li> <li>Select: Rank offspring; select best \\(\\mu\\) as parents</li> <li>Update mean: \\(m \\leftarrow \\sum_{i=1}^{\\mu} w_i x_{i:\\lambda}\\)</li> <li>Update evolution paths:<ul> <li>Conjugate path: \\(p_\\sigma \\leftarrow (1-c_\\sigma) p_\\sigma + \\sqrt{c_\\sigma(2-c_\\sigma)\\mu_\\text{eff}} \\cdot C^{-1/2}(m - m_\\text{old})/\\sigma\\)</li> <li>Covariance path: \\(p_c \\leftarrow (1-c_c) p_c + \\sqrt{c_c(2-c_c)\\mu_\\text{eff}} \\cdot (m - m_\\text{old})/\\sigma\\)</li> </ul> </li> <li>Update covariance: Rank-one + rank-\\(\\mu\\) update</li> <li>Update step size: Based on evolution path length vs expected length</li> <li>Repeat: Until convergence</li> </ol>"},{"location":"algorithms/optimisers/cmaes/#strategy-parameters","title":"Strategy Parameters","text":"<p>The algorithm automatically computes these from dimension \\(n\\):</p> Parameter Formula Purpose \\(\\lambda\\) \\(\\max(4, \\lfloor 4 + 3\\ln(n) \\rfloor)\\) Population size \\(\\mu\\) \\(\\lfloor \\lambda / 2 \\rfloor\\) Parent count \\(\\mu_\\text{eff}\\) \\(1 / \\sum w_i^2\\) Effective parent count \\(c_\\sigma\\) \\((\u03bc_\\text{eff} + 2) / (n + \u03bc_\\text{eff} + 5)\\) Step size learning rate \\(c_c\\) \\((4 + \u03bc_\\text{eff}/n) / (n + 4 + 2\u03bc_\\text{eff}/n)\\) Covariance path learning rate"},{"location":"algorithms/optimisers/cmaes/#parameters","title":"Parameters","text":"Parameter Default Description <code>max_iter</code> 1000 Maximum generations <code>threshold</code> 1e-6 Objective convergence tolerance <code>step_size</code> 0.5 Initial search radius \\(\\sigma\\) <code>population_size</code> Auto Offspring per generation \\(\\lambda\\) <code>seed</code> None Random seed for reproducibility <code>patience</code> None Timeout in seconds"},{"location":"algorithms/optimisers/cmaes/#convergence-criteria","title":"Convergence Criteria","text":"<p>The algorithm terminates when any condition is met:</p> <ol> <li>Iteration limit: <code>generation &gt;= max_iter</code></li> <li>Function tolerance: Best objective value below <code>threshold</code></li> <li>Patience: Elapsed time exceeds <code>patience</code> seconds</li> </ol>"},{"location":"algorithms/optimisers/cmaes/#tuning-guidance","title":"Tuning Guidance","text":"<p>Step size (\\(\\sigma\\)) is the most important parameter:</p> <ul> <li>Set to ~\u2153 of the expected distance to the optimum</li> <li>Too large: slow convergence, overshooting</li> <li>Too small: premature convergence, stuck in local optima</li> </ul> <p>Population size affects exploration vs exploitation:</p> <ul> <li>Default formula works well for most problems</li> <li>Increase for highly multi-modal landscapes</li> <li>Decrease for faster convergence on simpler problems</li> <li>Match to available parallel compute resources</li> </ul>"},{"location":"algorithms/optimisers/cmaes/#when-to-use","title":"When to Use","text":"<p>Strengths:</p> <ul> <li>Global search capability</li> <li>Scales well to high dimensions (10-100+ parameters)</li> <li>Self-adapting covariance learns problem structure</li> <li>Parallelisable population evaluation</li> <li>Robust to local minima</li> </ul> <p>Limitations:</p> <ul> <li>More evaluations than gradient methods</li> <li>Stochastic results (use seed for reproducibility)</li> <li>Memory scales as \\(O(n^2)\\) for covariance matrix</li> <li>Overkill for simple, low-dimensional problems</li> </ul>"},{"location":"algorithms/optimisers/cmaes/#example","title":"Example","text":"<pre><code>import diffid as chron\n\noptimiser = (\n    diffid.CMAES()\n    .with_max_iter(500)\n    .with_step_size(0.3)\n    .with_population_size(20)\n    .with_seed(42)\n)\n\nresult = optimiser.run(problem, initial_guess=[0.5] * n_params)\n</code></pre>"},{"location":"algorithms/optimisers/cmaes/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Full covariance matrix (not diagonal approximation)</li> <li>Lazy eigendecomposition for efficient sampling</li> <li>Bounds enforced via clamping after each sample</li> </ul>"},{"location":"algorithms/optimisers/cmaes/#references","title":"References","text":"<ol> <li>Hansen, N. (2016). \"The CMA Evolution Strategy: A Tutorial\". arXiv:1604.00772.</li> <li>Hansen, N. and Ostermeier, A. (2001). \"Completely Derandomized Self-Adaptation in Evolution Strategies\". Evolutionary Computation, 9(2), 159-195.</li> <li>Hansen, N. et al. (2003). \"Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)\". Evolutionary Computation, 11(1), 1-18.</li> </ol>"},{"location":"algorithms/optimisers/cmaes/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Choosing an Optimiser</li> <li>Parallel Execution</li> </ul>"},{"location":"algorithms/optimisers/nelder-mead/","title":"Nelder-Mead Algorithm","text":"<p>The Nelder-Mead algorithm (downhill simplex method) is a gradient-free local optimisation algorithm that uses a geometric simplex to navigate the parameter space.</p>"},{"location":"algorithms/optimisers/nelder-mead/#algorithm-overview","title":"Algorithm Overview","text":"<p>Nelder-Mead maintains a simplex of \\(n+1\\) vertices in \\(n\\)-dimensional space. At each iteration, it transforms the simplex through reflection, expansion, contraction, or shrinking operations to move towards lower objective values.</p>"},{"location":"algorithms/optimisers/nelder-mead/#key-properties","title":"Key Properties","text":"Property Value Type Local, gradient-free Parallelisable No Function evaluations Sequential Best dimensions 2-10 parameters"},{"location":"algorithms/optimisers/nelder-mead/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Initialise: Create simplex from initial point using step size</li> <li>Order: Sort vertices by objective value: \\(f(x_1) \\leq f(x_2) \\leq \\cdots \\leq f(x_{n+1})\\)</li> <li>Reflect: Compute reflection point \\(x_r = \\bar{x} + \\alpha(\\bar{x} - x_{n+1})\\)</li> <li>Transform: Based on \\(f(x_r)\\):<ul> <li>If \\(f(x_1) \\leq f(x_r) &lt; f(x_n)\\): accept reflection</li> <li>If \\(f(x_r) &lt; f(x_1)\\): try expansion \\(x_e = \\bar{x} + \\gamma(x_r - \\bar{x})\\)</li> <li>If \\(f(x_r) \\geq f(x_n)\\): try contraction \\(x_c = \\bar{x} + \\rho(x_{n+1} - \\bar{x})\\)</li> </ul> </li> <li>Shrink: If contraction fails, shrink towards best vertex</li> <li>Repeat: Until convergence criteria met</li> </ol> <p>Where \\(\\bar{x}\\) is the centroid of all vertices except the worst.</p>"},{"location":"algorithms/optimisers/nelder-mead/#parameters","title":"Parameters","text":"Parameter Default Description <code>max_iter</code> 1000 Maximum iterations <code>threshold</code> 1e-6 Objective convergence tolerance <code>step_size</code> 0.1 Initial simplex size <code>position_tolerance</code> 1e-6 Parameter space convergence tolerance <code>patience</code> None Timeout in seconds"},{"location":"algorithms/optimisers/nelder-mead/#simplex-coefficients","title":"Simplex Coefficients","text":"Coefficient Symbol Default Purpose Reflection \\(\\alpha\\) 1.0 Scale of reflection step Expansion \\(\\gamma\\) 2.0 Scale of expansion step Contraction \\(\\rho\\) 0.5 Scale of contraction step Shrinking \\(\\sigma\\) 0.5 Scale of shrink operation"},{"location":"algorithms/optimisers/nelder-mead/#convergence-criteria","title":"Convergence Criteria","text":"<p>The algorithm terminates when any of the following conditions is met:</p> <ol> <li>Iteration limit: <code>iteration &gt;= max_iter</code></li> <li>Function tolerance: Change in objective value below <code>threshold</code></li> <li>Position tolerance: Simplex diameter below <code>position_tolerance</code></li> <li>Patience: Elapsed time exceeds <code>patience</code> seconds</li> </ol>"},{"location":"algorithms/optimisers/nelder-mead/#tuning-guidance","title":"Tuning Guidance","text":"<p>Step size controls the initial simplex scale:</p> <ul> <li>Set to ~10-50% of the expected parameter range</li> <li>Larger values explore more broadly</li> <li>Smaller values for local refinement near a known solution</li> </ul> <p>Thresholds control precision vs speed:</p> <ul> <li>Tighter tolerances (1e-8) for high precision</li> <li>Looser tolerances (1e-4) for quick estimates</li> </ul>"},{"location":"algorithms/optimisers/nelder-mead/#when-to-use","title":"When to Use","text":"<p>Strengths:</p> <ul> <li>No gradient computation required</li> <li>Robust to moderate noise</li> <li>Simple and reliable for small problems</li> <li>Low memory footprint</li> </ul> <p>Limitations:</p> <ul> <li>Convergence slows significantly beyond 10 parameters</li> <li>Can converge to local minima</li> <li>Not parallelisable</li> </ul>"},{"location":"algorithms/optimisers/nelder-mead/#example","title":"Example","text":"<pre><code>import diffid as chron\n\noptimiser = (\n    diffid.NelderMead()\n    .with_max_iter(5000)\n    .with_step_size(0.1)\n    .with_threshold(1e-8)\n)\n\nresult = optimiser.run(problem, initial_guess=[1.0, 2.0])\n</code></pre>"},{"location":"algorithms/optimisers/nelder-mead/#references","title":"References","text":"<ol> <li>Nelder, J.A. and Mead, R. (1965). \"A Simplex Method for Function Minimization\". The Computer Journal, 7(4), 308-313.</li> <li>Lagarias, J.C. et al. (1998). \"Convergence Properties of the Nelder-Mead Simplex Method in Low Dimensions\". SIAM Journal on Optimization, 9(1), 112-147.</li> </ol>"},{"location":"algorithms/optimisers/nelder-mead/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Choosing an Optimiser</li> <li>Tuning Optimisers</li> </ul>"},{"location":"algorithms/samplers/dynamic-nested-sampling/","title":"Dynamic Nested Sampling Algorithm","text":"<p>Dynamic Nested Sampling is a Bayesian inference algorithm that computes the model evidence (marginal likelihood) while simultaneously generating posterior samples. It is particularly suited for model comparison via Bayes factors. </p>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#algorithm-overview","title":"Algorithm Overview","text":"<p>Nested sampling transforms the multi-dimensional evidence integral into a one-dimensional integral over prior volume. It maintains a set of \"live points\" that progressively shrink the prior volume while tracking the likelihood threshold. The \"dynamic\" aspect allows the algorithm to allocate more live points in regions that contribute most to either the evidence or posterior, improving efficiency.</p>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#key-properties","title":"Key Properties","text":"Property Value Type Evidence sampler Parallelisable Yes (batch proposals) Output Evidence + posterior samples Best for Model comparison"},{"location":"algorithms/samplers/dynamic-nested-sampling/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>The model evidence (marginal likelihood) is:</p> \\[ \\mathcal{Z} = \\int \\mathcal{L}(\\theta) \\pi(\\theta) \\, d\\theta \\] <p>Nested sampling transforms this by defining the prior volume:</p> \\[ X(\\lambda) = \\int_{\\mathcal{L}(\\theta) &gt; \\lambda} \\pi(\\theta) \\, d\\theta \\] <p>The evidence becomes a one-dimensional integral:</p> \\[ \\mathcal{Z} = \\int_0^1 \\mathcal{L}(X) \\, dX \\] <p>This is approximated by iteratively shrinking the prior volume:</p> \\[ \\mathcal{Z} \\approx \\sum_{i=1}^{N} \\mathcal{L}_i \\, \\Delta X_i \\] <p>Where \\(\\Delta X_i = X_{i-1} - X_i\\) and the shrinkage ratio is estimated statistically.</p>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Initialise: Sample \\(K\\) live points uniformly from the prior</li> <li>Iterate:<ul> <li>Find lowest-likelihood live point \\(\\mathcal{L}^*\\)</li> <li>Record point as \"dead point\" with prior volume estimate</li> <li>Replace with new point sampled uniformly from prior with \\(\\mathcal{L} &gt; \\mathcal{L}^*\\)</li> </ul> </li> <li>Terminate: When remaining evidence contribution is negligible</li> <li>Compute: Sum contributions to estimate \\(\\mathcal{Z}\\) and uncertainty</li> <li>Return: Log-evidence, evidence error, and posterior samples</li> </ol>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#parameters","title":"Parameters","text":"Parameter Default Description <code>live_points</code> 64 Number of live points \\(K\\) <code>expansion_factor</code> 0.5 Live set expansion aggressiveness <code>termination_tol</code> 1e-3 Evidence convergence tolerance <code>mcmc_batch_size</code> 8 Proposals generated per iteration <code>mcmc_step_size</code> 0.01 MCMC proposal step size <code>seed</code> None Random seed for reproducibility"},{"location":"algorithms/samplers/dynamic-nested-sampling/#tuning-guidance","title":"Tuning Guidance","text":"<p>Live points control accuracy vs cost:</p> <ul> <li>More live points = better evidence estimate but more evaluations</li> <li>Minimum recommended: \\(25 \u00d7 n_{\\text{params}}\\)</li> <li>For accurate posteriors: \\(&gt;50 \u00d7 n_{\\text{params}}\\)</li> </ul> <p>Termination tolerance affects precision:</p> <ul> <li>Smaller values = more accurate evidence but more iterations</li> <li>Default 1e-3 sufficient for most model comparisons</li> </ul> <p>MCMC parameters affect replacement efficiency:</p> <ul> <li><code>mcmc_batch_size</code>: larger batches for parallel evaluation</li> <li><code>mcmc_step_size</code>: tune for ~20-50% acceptance in constrained sampling</li> </ul>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#evidence-interpretation","title":"Evidence Interpretation","text":"<p>The log-evidence can be used for model comparison via Bayes factors:</p> \\[ \\ln B_{12} = \\ln \\mathcal{Z}_1 - \\ln \\mathcal{Z}_2 \\] \\(\\ln B_{12}\\) \\(B_{12}\\) Evidence strength &lt; 1 &lt; 3 Inconclusive 1-2.5 3-12 Positive 2.5-5 12-150 Strong &gt; 5 &gt; 150 Decisive"},{"location":"algorithms/samplers/dynamic-nested-sampling/#when-to-use","title":"When to Use","text":"<p>Strengths:</p> <ul> <li>Computes model evidence directly</li> <li>Handles multi-modal posteriors well</li> <li>Provides posterior samples as byproduct</li> <li>Robust to complex likelihood landscapes</li> <li>Parallelisable</li> </ul> <p>Limitations:</p> <ul> <li>More expensive than MCMC for posterior-only inference</li> <li>Constrained sampling can be challenging in high dimensions</li> <li>Evidence accuracy depends on live point count</li> </ul>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#cost-metric-requirement","title":"Cost Metric Requirement","text":"<p>Nested sampling requires a negative log-likelihood cost metric:</p> <pre><code>problem = (\n    diffid.ScalarProblemBuilder()\n    .with_objective(model_fn)\n    .with_cost_metric(diffid.CostMetric.GaussianNLL)  # Required\n    .build()\n)\n</code></pre> <p>The objective function should return negative log-likelihood; the algorithm internally negates to work with log-likelihood.</p>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#example","title":"Example","text":"<pre><code>import diffid as chron\n\nsampler = (\n    diffid.DynamicNestedSampling()\n    .with_live_points(128)\n    .with_termination_tol(1e-3)\n    .with_seed(42)\n)\n\nresult = sampler.run(problem, initial_guess=[1.0, 2.0])\n\n# Access results\nlog_evidence = result.log_evidence\nevidence_error = result.evidence_error\nsamples = result.samples\n</code></pre>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#implementation-notes","title":"Implementation Notes","text":"<ul> <li>Uses MCMC for constrained prior sampling</li> <li>Batch evaluation for parallel efficiency</li> <li>Automatic termination based on remaining evidence contribution</li> </ul>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#references","title":"References","text":"<ol> <li>Skilling, J. (2006). \"Nested Sampling for General Bayesian Computation\". Bayesian Analysis, 1(4), 833-860.</li> <li>Higson, E. et al. (2019). \"Dynamic Nested Sampling: An Improved Algorithm for Parameter Estimation and Evidence Calculation\". Statistics and Computing, 29, 891-913.</li> <li>Speagle, J.S. (2020). \"dynesty: A Dynamic Nested Sampling Package for Estimating Bayesian Posteriors and Evidences\". Monthly Notices of the Royal Astronomical Society, 493(3), 3132-3158.</li> </ol>"},{"location":"algorithms/samplers/dynamic-nested-sampling/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Choosing a Sampler</li> <li>Cost Metrics</li> </ul>"},{"location":"algorithms/samplers/metropolis-hastings/","title":"Metropolis-Hastings Algorithm","text":"<p>Metropolis-Hastings is a Markov Chain Monte Carlo (MCMC) algorithm for sampling from posterior distributions. It enables uncertainty quantification by generating samples that characterise the probability distribution over parameters.</p>"},{"location":"algorithms/samplers/metropolis-hastings/#algorithm-overview","title":"Algorithm Overview","text":"<p>The algorithm constructs a Markov chain whose stationary distribution is the target posterior. By proposing random moves and accepting/rejecting them based on the likelihood ratio, it explores the parameter space in proportion to posterior probability.</p>"},{"location":"algorithms/samplers/metropolis-hastings/#key-properties","title":"Key Properties","text":"Property Value Type MCMC sampler Parallelisable Yes (across chains) Output Posterior samples Best for Uncertainty quantification"},{"location":"algorithms/samplers/metropolis-hastings/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Given a target distribution \\(\\pi(\\theta)\\) (the posterior), the algorithm:</p> <ol> <li>Proposes a new state from proposal distribution \\(q(\\theta' | \\theta)\\)</li> <li>Computes acceptance probability:</li> </ol> \\[ \\alpha = \\min\\left(1, \\frac{\\pi(\\theta') q(\\theta | \\theta')}{\\pi(\\theta) q(\\theta' | \\theta)}\\right) \\] <p>For the symmetric random walk proposal used here (\\(q(\\theta' | \\theta) = q(\\theta | \\theta')\\)):</p> \\[ \\alpha = \\min\\left(1, \\frac{\\pi(\\theta')}{\\pi(\\theta)}\\right) = \\min\\left(1, \\exp(\\log\\pi(\\theta') - \\log\\pi(\\theta))\\right) \\]"},{"location":"algorithms/samplers/metropolis-hastings/#algorithm-steps","title":"Algorithm Steps","text":"<ol> <li>Initialise: Start at initial point \\(\\theta_0\\)</li> <li>Propose: Generate candidate \\(\\theta' = \\theta_t + \\sigma \\cdot \\mathcal{N}(0, I)\\)</li> <li>Evaluate: Compute log-likelihood \\(\\log\\mathcal{L}(\\theta')\\)</li> <li>Accept/Reject:<ul> <li>Draw \\(u \\sim \\text{Uniform}(0, 1)\\)</li> <li>If \\(u &lt; \\exp(\\log\\mathcal{L}(\\theta') - \\log\\mathcal{L}(\\theta_t))\\): accept \\(\\theta_{t+1} = \\theta'\\)</li> <li>Else: reject \\(\\theta_{t+1} = \\theta_t\\)</li> </ul> </li> <li>Repeat: For specified number of iterations</li> <li>Return: Chain of samples \\(\\{\\theta_0, \\theta_1, \\ldots, \\theta_T\\}\\)</li> </ol>"},{"location":"algorithms/samplers/metropolis-hastings/#parameters","title":"Parameters","text":"Parameter Default Description <code>iterations</code> 1000 MCMC iterations per chain <code>num_chains</code> 1 Number of independent chains <code>step_size</code> 0.1 Proposal distribution width \\(\\sigma\\) <code>seed</code> None Random seed for reproducibility"},{"location":"algorithms/samplers/metropolis-hastings/#tuning-guidance","title":"Tuning Guidance","text":"<p>Step size controls exploration efficiency:</p> <ul> <li>Target acceptance rate: 20-50% (optimal ~23% for high dimensions)</li> <li>Too large: low acceptance rate, chain gets stuck</li> <li>Too small: high acceptance rate but slow mixing</li> <li>Tune by monitoring acceptance ratio</li> </ul> <p>Number of chains aids convergence diagnostics:</p> <ul> <li>Multiple chains from different starting points detect convergence issues</li> <li>Enables Gelman-Rubin \\(\\hat{R}\\) diagnostic</li> <li>More chains = more robust inference but higher cost</li> </ul> <p>Iterations should be sufficient for:</p> <ul> <li>Burn-in period (discard initial samples)</li> <li>Effective sample size for reliable estimates</li> </ul>"},{"location":"algorithms/samplers/metropolis-hastings/#convergence-diagnostics","title":"Convergence Diagnostics","text":"<p>After sampling, assess convergence:</p> <ul> <li>Trace plots: Visual inspection for stationarity</li> <li>Acceptance rate: Should be 20-50%</li> <li>Autocorrelation: Lower is better; high autocorrelation means inefficient sampling</li> <li>\\(\\hat{R}\\) statistic: Should be &lt; 1.1 for all parameters (requires multiple chains)</li> </ul>"},{"location":"algorithms/samplers/metropolis-hastings/#when-to-use","title":"When to Use","text":"<p>Strengths:</p> <ul> <li>Full posterior characterisation</li> <li>Uncertainty quantification (credible intervals)</li> <li>Parameter correlation analysis</li> <li>Simple and robust</li> </ul> <p>Limitations:</p> <ul> <li>Many function evaluations required</li> <li>Can be slow for high-dimensional problems</li> <li>Requires careful tuning for efficiency</li> <li>Does not compute model evidence (use nested sampling)</li> </ul>"},{"location":"algorithms/samplers/metropolis-hastings/#cost-metric-requirement","title":"Cost Metric Requirement","text":"<p>Metropolis-Hastings requires a negative log-likelihood cost metric:</p> <pre><code>problem = (\n    diffid.ScalarProblemBuilder()\n    .with_objective(model_fn)\n    .with_cost_metric(diffid.CostMetric.GaussianNLL)  # Required\n    .build()\n)\n</code></pre>"},{"location":"algorithms/samplers/metropolis-hastings/#example","title":"Example","text":"<pre><code>import diffid as chron\n\nsampler = (\n    diffid.MetropolisHastings()\n    .with_iterations(5000)\n    .with_num_chains(4)\n    .with_step_size(0.05)\n    .with_seed(42)\n)\n\nresult = sampler.run(problem, initial_guess=[1.0, 2.0])\n\n# Access samples\nsamples = result.samples  # Shape: (n_chains * iterations, n_params)\n</code></pre>"},{"location":"algorithms/samplers/metropolis-hastings/#references","title":"References","text":"<ol> <li>Metropolis, N. et al. (1953). \"Equation of State Calculations by Fast Computing Machines\". Journal of Chemical Physics, 21(6), 1087-1092.</li> <li>Hastings, W.K. (1970). \"Monte Carlo Sampling Methods Using Markov Chains and Their Applications\". Biometrika, 57(1), 97-109.</li> <li>Gelman, A. et al. (2013). Bayesian Data Analysis. 3<sup>rd</sup> ed. Chapman &amp; Hall/CRC.</li> </ol>"},{"location":"algorithms/samplers/metropolis-hastings/#see-also","title":"See Also","text":"<ul> <li>API Reference</li> <li>Choosing a Sampler</li> <li>Cost Metrics</li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for Diffid's Python and Rust interfaces.</p>"},{"location":"api-reference/#python-api","title":"Python API","text":"<p>Diffid provides a comprehensive Python API with full type hints and automatic documentation.</p> <ul> <li> <p> Builders</p> <p>Problem builders for different use cases: scalar functions, ODE fitting, and custom solvers.</p> <p> Builders</p> </li> <li> <p> Optimisers</p> <p>Gradient-free (Nelder-Mead, CMA-ES) and gradient-based (Adam) optimisation algorithms.</p> <p> Optimisers</p> </li> <li> <p> Samplers</p> <p>MCMC and nested sampling for posterior exploration and model comparison.</p> <p> Samplers</p> </li> <li> <p> Cost Metrics</p> <p>Metrics for comparing model predictions to observations (SSE, RMSE, GaussianNLL).</p> <p> Cost Metrics</p> </li> <li> <p> Results</p> <p>Result objects returned by optimisers and samplers with diagnostics.</p> <p> Results</p> </li> </ul>"},{"location":"api-reference/#rust-api","title":"Rust API","text":"<p>The Rust core provides high-performance implementations of all algorithms.</p> <p> Rust Documentation on docs.rs</p>"},{"location":"api-reference/#module-structure","title":"Module Structure","text":"<pre><code>diffid/\n\u251c\u2500\u2500 ScalarBuilder        # Direct function optimisation\n\u251c\u2500\u2500 DiffsolBuilder       # ODE fitting with DiffSL/Diffsol\n\u251c\u2500\u2500 VectorBuilder        # Custom solver integration\n\u251c\u2500\u2500 Problem              # Built problem instance\n\u251c\u2500\u2500 Optimisers\n\u2502   \u251c\u2500\u2500 NelderMead       # Simplex gradient-free optimiser\n\u2502   \u251c\u2500\u2500 CMAES            # Covariance matrix adaptation\n\u2502   \u2514\u2500\u2500 Adam             # Adaptive moment estimation\n\u251c\u2500\u2500 Samplers\n\u2502   \u251c\u2500\u2500 MetropolisHastings  # MCMC sampling\n\u2502   \u2514\u2500\u2500 DynamicNestedSampling  # Evidence calculation\n\u251c\u2500\u2500 CostMetric           # Cost/likelihood metrics\n\u2514\u2500\u2500 OptimisationResults  # Result container\n</code></pre>"},{"location":"api-reference/#type-hints","title":"Type Hints","text":"<p>All Python functions include comprehensive type hints:</p> <pre><code>from diffid import ScalarBuilder, CMAES, OptimisationResults\nimport numpy.typing as npt\n\ndef optimize_function(\n    func: Callable[[npt.NDArray[np.float64]], npt.NDArray[np.float64]],\n    initial_params: dict[str, float]\n) -&gt; OptimisationResults:\n    builder = ScalarBuilder().with_objective(func)\n    for name, value in initial_params.items():\n        builder = builder.with_parameter(name, value)\n\n    problem = builder.build()\n    optimiser = CMAES().with_max_iter(1000)\n    return optimiser.run(problem, list(initial_params.values()))\n</code></pre> <p>Type stubs are automatically generated from the Rust implementation and are available in IDE completions.</p>"},{"location":"api-reference/#conventions","title":"Conventions","text":""},{"location":"api-reference/#parameter-order","title":"Parameter Order","text":"<p>All builders maintain parameter order based on the sequence of <code>.with_parameter()</code> calls:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_parameter(\"x\", 1.0)  # Index 0\n    .with_parameter(\"y\", 2.0)  # Index 1\n)\n\nresult = problem.optimise()\nprint(result.x)  # [optimal_x, optimal_y]\n</code></pre>"},{"location":"api-reference/#return-types","title":"Return Types","text":"<ul> <li>Optimisers return <code>OptimisationResults</code> with <code>.x</code>, <code>.value</code>, <code>.success</code>, etc.</li> <li>Samplers return sampling-specific results with <code>.samples</code>, <code>.log_likelihood</code>, etc.</li> <li>All results include diagnostic information</li> </ul>"},{"location":"api-reference/#array-conventions","title":"Array Conventions","text":"<ul> <li>Input: Python callables accept NumPy arrays</li> <li>Output: Python callables must return NumPy arrays</li> <li>Data: Data arrays are <code>(n_timepoints, n_variables + 1)</code> with time in first column</li> </ul>"},{"location":"api-reference/#next-steps","title":"Next Steps","text":"<ul> <li>Builders API - Start building problems</li> <li>Optimisers API - Configure optimisation algorithms</li> <li>User Guides - Learn when to use each component</li> <li>Tutorials - Interactive examples</li> </ul>"},{"location":"api-reference/python/builders/","title":"Builders","text":"<p>Builders provide a fluent API for constructing optimisation problems. Choose the appropriate builder based on your problem type.</p>"},{"location":"api-reference/python/builders/#overview","title":"Overview","text":"Builder Use Case Example <code>ScalarBuilder</code> Direct function optimisation Rosenbrock, Rastrigin <code>DiffsolBuilder</code> ODE fitting with DiffSL Parameter identification <code>VectorBuilder</code> Custom solver integration JAX, Julia, external simulators"},{"location":"api-reference/python/builders/#scalarbuilder","title":"ScalarBuilder","text":""},{"location":"api-reference/python/builders/#diffid.ScalarBuilder","title":"ScalarBuilder","text":"<p>High-level builder for optimisation <code>Problem</code> instances exposed to Python.</p>"},{"location":"api-reference/python/builders/#diffid.ScalarBuilder.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create an empty builder with no objective, parameters, or default optimiser.</p>"},{"location":"api-reference/python/builders/#diffid.ScalarBuilder.with_objective","title":"with_objective","text":"<pre><code>with_objective(obj)\n</code></pre> <p>Attach the objective function callable executed during optimisation.</p>"},{"location":"api-reference/python/builders/#diffid.ScalarBuilder.with_parameter","title":"with_parameter","text":"<pre><code>with_parameter(name, initial_value, bounds=None)\n</code></pre> <p>Register a named optimisation variable in the order it appears in vectors.</p>"},{"location":"api-reference/python/builders/#diffid.ScalarBuilder.build","title":"build","text":"<pre><code>build()\n</code></pre> <p>Finalize the builder into an executable <code>Problem</code>.</p>"},{"location":"api-reference/python/builders/#example-usage","title":"Example Usage","text":"<pre><code>import numpy as np\nimport diffid as chron\n\ndef rosenbrock(x):\n    return np.asarray([(1 - x[0])**2 + 100*(x[1] - x[0]**2)**2])\n\nbuilder = (\n    diffid.ScalarBuilder()\n    .with_objective(rosenbrock)\n    .with_parameter(\"x\", 1.5)\n    .with_parameter(\"y\", -1.5)\n)\nproblem = builder.build()\nresult = problem.optimise()\n</code></pre>"},{"location":"api-reference/python/builders/#when-to-use","title":"When to Use","text":"<ul> <li>You have a Python function to minimise directly</li> <li>No differential equations involved</li> <li>Simple parameter optimisation or test functions</li> </ul>"},{"location":"api-reference/python/builders/#diffsolbuilder","title":"DiffsolBuilder","text":""},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder","title":"DiffsolBuilder","text":"<p>Differential equation solver builder.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create an empty differential solver builder.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.with_diffsl","title":"with_diffsl","text":"<pre><code>with_diffsl(dsl)\n</code></pre> <p>Register the DiffSL program describing the system dynamics.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.with_data","title":"with_data","text":"<pre><code>with_data(data)\n</code></pre> <p>Attach observed data used to fit the differential equation.</p> <p>The first column must contain the time samples (t_span) and the remaining columns the observed trajectories.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.with_parameter","title":"with_parameter","text":"<pre><code>with_parameter(name, initial_value, bounds=None)\n</code></pre> <p>Register a named optimisation variable in the order it appears in vectors.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.with_backend","title":"with_backend","text":"<pre><code>with_backend(backend)\n</code></pre> <p>Choose whether to use dense or sparse diffusion solvers.</p>"},{"location":"api-reference/python/builders/#diffid.DiffsolBuilder.build","title":"build","text":"<pre><code>build()\n</code></pre> <p>Create a <code>Problem</code> representing the differential solver model.</p>"},{"location":"api-reference/python/builders/#example-usage_1","title":"Example Usage","text":"<pre><code>import numpy as np\nimport diffid as chron\n\ndsl = \"\"\"\nin { r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\"\"\"\n\nt = np.linspace(0.0, 5.0, 51)\nobservations = np.exp(-1.3 * t)\ndata = np.column_stack((t, observations))\n\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_backend(\"dense\")\n)\nproblem = builder.build()\noptimiser = diffid.CMAES().with_max_iter(1000)\nresult = optimiser.run(problem, [0.5, 0.5])\n</code></pre>"},{"location":"api-reference/python/builders/#backend-options","title":"Backend Options","text":"<ul> <li><code>\"dense\"</code> (default): For systems with &lt; 100 variables, dense Jacobian</li> <li><code>\"sparse\"</code>: For large systems (&gt; 100 variables), sparse Jacobian</li> </ul>"},{"location":"api-reference/python/builders/#diffsl-syntax","title":"DiffSL Syntax","text":"<p>DiffSL is a domain-specific language for ODEs:</p> <pre><code>in_i { param1 = default1, param2 = default 2 }           # Parameters to fit with defaults\nu_i { state1 = init1 }          # Initial conditions\nF_i { derivative_expr }         # dy/dt expressions\nout_i { state1, state2 }        # Optional: output variables\n</code></pre>"},{"location":"api-reference/python/builders/#when-to-use_1","title":"When to Use","text":"<ul> <li>Fitting ODE parameters to time-series data</li> <li>Using Diffid's built-in high-performance solver</li> <li>Models expressible in DiffSL syntax</li> </ul>"},{"location":"api-reference/python/builders/#vectorbuilder","title":"VectorBuilder","text":""},{"location":"api-reference/python/builders/#diffid.VectorBuilder","title":"VectorBuilder","text":"<p>Time-series problem builder for vector-valued objectives.</p>"},{"location":"api-reference/python/builders/#diffid.VectorBuilder.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create an empty vector problem builder.</p>"},{"location":"api-reference/python/builders/#diffid.VectorBuilder.with_objective","title":"with_objective","text":"<pre><code>with_objective(objective)\n</code></pre> <p>Register a callable that produces predictions matching the data shape.</p> <p>The callable should accept a parameter vector and return a numpy array of the same shape as the observed data.</p>"},{"location":"api-reference/python/builders/#diffid.VectorBuilder.with_data","title":"with_data","text":"<pre><code>with_data(data)\n</code></pre> <p>Attach observed data used to fit the model.</p> <p>The data should be a 1D numpy array. The shape will be inferred from the data length.</p>"},{"location":"api-reference/python/builders/#diffid.VectorBuilder.with_parameter","title":"with_parameter","text":"<pre><code>with_parameter(name, initial_value, bounds=None)\n</code></pre> <p>Register a named optimisation variable in the order it appears in vectors.</p>"},{"location":"api-reference/python/builders/#diffid.VectorBuilder.build","title":"build","text":"<pre><code>build()\n</code></pre> <p>Create a <code>Problem</code> representing the vector optimisation model.</p>"},{"location":"api-reference/python/builders/#example-usage_2","title":"Example Usage","text":"<pre><code>import numpy as np\nimport diffid as chron\n\ndef custom_solver(params):\n    \"\"\"Your custom ODE solver (e.g., using JAX/Diffrax).\"\"\"\n    # Solve ODE with params\n    # Return predictions at observation times\n    return predictions  # NumPy array\n\n# Observed data\nt = np.linspace(0, 10, 100)\nobservations = ...  # Your experimental data\ndata = np.column_stack((t, observations))\n\nbuilder = (\n    diffid.VectorBuilder()\n    .with_objective(custom_solver)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n    .with_parameter(\"beta\", 0.5)\n)\nproblem = builder.build()\nresult = problem.optimise()\n</code></pre>"},{"location":"api-reference/python/builders/#callable-requirements","title":"Callable Requirements","text":"<p>Your callable must:</p> <ol> <li>Accept a NumPy array of parameters</li> <li>Return a NumPy array of predictions</li> <li>Predictions must match observation times in the data</li> </ol>"},{"location":"api-reference/python/builders/#when-to-use_2","title":"When to Use","text":"<ul> <li>Need specific solver features (stiff solvers, event detection, etc.)</li> <li>Using JAX/Diffrax for automatic differentiation</li> <li>Using Julia/DifferentialEquations.jl via diffeqpy</li> <li>Custom forward models beyond ODEs (PDEs, agent-based models, etc.)</li> </ul> <p>See the Custom Solvers Guide for examples with Diffrax and DifferentialEquations.jl.</p>"},{"location":"api-reference/python/builders/#problem","title":"Problem","text":"<p>The <code>Problem</code> class is created by calling <code>.build()</code> on a builder. It represents a fully configured optimisation problem.</p>"},{"location":"api-reference/python/builders/#diffid.Problem","title":"Problem","text":"<p>Executable optimisation problem wrapping the Diffid core implementation.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.evaluate","title":"evaluate","text":"<pre><code>evaluate(x)\n</code></pre> <p>Evaluate the configured objective function at <code>x</code>.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.evaluate_gradient","title":"evaluate_gradient","text":"<pre><code>evaluate_gradient(x)\n</code></pre> <p>Evaluate the gradient of the objective function at <code>x</code> if available.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.optimise","title":"optimise","text":"<pre><code>optimise(initial=None, optimiser=None)\n</code></pre> <p>Solve the problem starting from <code>initial</code> using the supplied optimiser.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.sample","title":"sample","text":"<pre><code>sample(initial=None, sampler=None)\n</code></pre> <p>Sample from the problem starting from <code>initial</code> using the supplied sampler.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.get_config","title":"get_config","text":"<pre><code>get_config(_key)\n</code></pre> <p>Return the numeric configuration value stored under <code>key</code> if present.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.dimension","title":"dimension","text":"<pre><code>dimension()\n</code></pre> <p>Return the number of parameters the problem expects.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.bounds","title":"bounds","text":"<pre><code>bounds()\n</code></pre> <p>Return the parameter bounds for the problem as a list of (lower, upper) tuples.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.default_parameters","title":"default_parameters","text":"<pre><code>default_parameters()\n</code></pre> <p>Return the default parameter vector implied by the builder.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.config","title":"config","text":"<pre><code>config()\n</code></pre> <p>Return a copy of the problem configuration dictionary.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Call the problem as a function (shorthand for evaluate).</p> <p>Allows using <code>problem(x)</code> instead of <code>problem.evaluate(x)</code>.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a detailed string representation of the problem.</p>"},{"location":"api-reference/python/builders/#diffid.Problem.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return a concise string representation of the problem.</p>"},{"location":"api-reference/python/builders/#common-methods","title":"Common Methods","text":"<pre><code># Optimise with default settings (Nelder-Mead)\nresult = problem.optimise()\n\n# Evaluate objective at specific parameters\nvalue = problem.evaluate(params)\n</code></pre>"},{"location":"api-reference/python/builders/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/python/builders/#chaining-methods","title":"Chaining Methods","text":"<p>Builders use a fluent interface - chain methods in any order:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_objective(func)\n    .with_parameter(\"x\", 1.0)\n    .with_parameter(\"y\", 2.0)\n    .with_cost_metric(diffid.RMSE())\n)\n</code></pre>"},{"location":"api-reference/python/builders/#reusing-builders","title":"Reusing Builders","text":"<p>Builders are immutable - each method returns a new builder:</p> <pre><code>base = diffid.ScalarBuilder().with_objective(func)\n\nproblem1 = base.with_parameter(\"x\", 1.0).build()\nproblem2 = base.with_parameter(\"x\", 2.0).build()  # Different initial guess\n</code></pre>"},{"location":"api-reference/python/builders/#parameter-order-matters","title":"Parameter Order Matters","text":"<p>Parameters are indexed in the order they're added:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_parameter(\"y\", 2.0)  # Index 0\n    .with_parameter(\"x\", 1.0)  # Index 1\n)\n\nresult = problem.optimise()\nprint(result.x)  # [optimal_y, optimal_x]\n</code></pre>"},{"location":"api-reference/python/builders/#see-also","title":"See Also","text":"<ul> <li>Getting Started: Core Concepts</li> <li>Cost Metrics</li> <li>Optimisers</li> <li>Custom Solvers Guide</li> </ul>"},{"location":"api-reference/python/cost-metrics/","title":"Cost Metrics","text":"<p>Cost metrics define how model predictions are compared to observations. They determine the objective function that optimisers minimise.</p>"},{"location":"api-reference/python/cost-metrics/#overview","title":"Overview","text":"Metric Formula Use Case <code>SSE</code> \\(\\sum (y_i - \\hat{y}_i)^2\\) Standard least squares <code>RMSE</code> \\(\\sqrt{\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2}\\) Normalised error <code>GaussianNLL</code> \\(-\\log p(data \\mid params)\\) Bayesian inference"},{"location":"api-reference/python/cost-metrics/#costmetric","title":"CostMetric","text":""},{"location":"api-reference/python/cost-metrics/#diffid.CostMetric","title":"CostMetric","text":""},{"location":"api-reference/python/cost-metrics/#diffid.CostMetric.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Name of the cost metric.</p>"},{"location":"api-reference/python/cost-metrics/#sse-sum-of-squared-errors","title":"SSE (Sum of Squared Errors)","text":"<p>The default cost metric. Minimises the sum of squared residuals.</p> \\[\\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]"},{"location":"api-reference/python/cost-metrics/#example-usage","title":"Example Usage","text":"<pre><code>import diffid as chron\n\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    # SSE is used by default, but can be explicit:\n    # .with_cost_metric(diffid.SSE())\n)\n</code></pre>"},{"location":"api-reference/python/cost-metrics/#when-to-use","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Standard least squares approach</li> <li>Well-understood statistical properties</li> <li>Efficient to compute</li> </ul> <p>Limitations:</p> <ul> <li>Not normalised (sensitive to number of data points)</li> <li>Sensitive to outliers (squared errors magnify them)</li> <li>Assumes Gaussian errors with constant variance</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Standard parameter fitting</li> <li>When absolute error scale matters</li> <li>Comparing models with same number of points</li> </ul>"},{"location":"api-reference/python/cost-metrics/#rmse-root-mean-squared-error","title":"RMSE (Root Mean Squared Error)","text":"<p>Normalised version of SSE. Divides by number of points and takes square root.</p> \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\]"},{"location":"api-reference/python/cost-metrics/#example-usage_1","title":"Example Usage","text":"<pre><code>import diffid as chron\n\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_cost_metric(diffid.RMSE())\n)\n</code></pre>"},{"location":"api-reference/python/cost-metrics/#when-to-use_1","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Normalised (independent of data size)</li> <li>Same units as observations</li> <li>Better for comparing models with different data sizes</li> </ul> <p>Limitations:</p> <ul> <li>Still sensitive to outliers</li> <li>Assumes Gaussian errors</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Comparing models with different numbers of observations</li> <li>Reporting error in interpretable units</li> <li>Cross-validation and model selection</li> </ul>"},{"location":"api-reference/python/cost-metrics/#gaussiannll-gaussian-negative-log-likelihood","title":"GaussianNLL (Gaussian Negative Log-Likelihood)","text":"<p>Probabilistic cost metric for Bayesian inference. Assumes Gaussian observation noise.</p> \\[\\text{GaussianNLL} = -\\log p(data \\mid params) = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\] <p>where \\(\\sigma^2\\) is estimated from residuals.</p>"},{"location":"api-reference/python/cost-metrics/#example-usage_2","title":"Example Usage","text":"<pre><code>import diffid as chron\n\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_cost_metric(diffid.GaussianNLL())\n)\n\n# Use with MCMC sampling for Bayesian inference\nsampler = diffid.MetropolisHastings().with_max_iter(10000)\nresult = sampler.run(problem, initial_guess)\n</code></pre>"},{"location":"api-reference/python/cost-metrics/#when-to-use_2","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Proper probabilistic interpretation</li> <li>Required for Bayesian inference (MCMC, nested sampling)</li> <li>Automatically accounts for noise level</li> <li>Enables uncertainty quantification</li> </ul> <p>Limitations:</p> <ul> <li>Assumes Gaussian observation noise</li> <li>More computationally expensive than SSE</li> <li>Requires understanding of likelihood</li> </ul> <p>Typical Use Cases:</p> <ul> <li>MCMC sampling for uncertainty quantification</li> <li>Model comparison with Bayes factors</li> <li>When probabilistic interpretation is needed</li> <li>Nested sampling for evidence calculation</li> </ul>"},{"location":"api-reference/python/cost-metrics/#choosing-a-cost-metric","title":"Choosing a Cost Metric","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Using samplers?}\n    B --&gt;|Yes| C[GaussianNLL]\n    B --&gt;|No| D{Comparing models?}\n    D --&gt;|Different data sizes| E[RMSE]\n    D --&gt;|Same data size| F[SSE]\n    D --&gt;|No comparison| F</code></pre>"},{"location":"api-reference/python/cost-metrics/#decision-guide","title":"Decision Guide","text":"<p>Use SSE when:</p> <ul> <li>Standard least squares fitting</li> <li>Single model, fixed data</li> <li>Speed is critical</li> </ul> <p>Use RMSE when:</p> <ul> <li>Comparing models with different data sizes</li> <li>Want interpretable error in original units</li> <li>Cross-validation or model selection</li> </ul> <p>Use GaussianNLL when:</p> <ul> <li>MCMC sampling (Metropolis-Hastings)</li> <li>Nested sampling (model evidence)</li> <li>Need confidence intervals</li> <li>Bayesian inference required</li> </ul> <p>For more details, see the Cost Metrics Guide.</p>"},{"location":"api-reference/python/cost-metrics/#custom-cost-metrics","title":"Custom Cost Metrics","text":"<p>Currently, custom cost metrics must be implemented in Rust. Python-level custom metrics are planned for a future release.</p> <p>To implement a custom cost metric:</p> <ol> <li>Add implementation in <code>rust/src/cost/</code></li> <li>Expose through Python bindings</li> <li>Rebuild the package</li> </ol> <p>See the Development Guide for details on extending Diffid.</p>"},{"location":"api-reference/python/cost-metrics/#mathematical-background","title":"Mathematical Background","text":""},{"location":"api-reference/python/cost-metrics/#relationship-to-maximum-likelihood","title":"Relationship to Maximum Likelihood","text":"<p>Minimising SSE is equivalent to maximum likelihood estimation under Gaussian noise with known variance:</p> \\[\\hat{\\theta} = \\arg\\min_\\theta \\sum (y_i - f(x_i; \\theta))^2 = \\arg\\max_\\theta p(data \\mid \\theta)\\]"},{"location":"api-reference/python/cost-metrics/#weighted-least-squares","title":"Weighted Least Squares","text":"<p>For heteroscedastic data (varying noise), weighted metrics can be important. Contact the developers if you need this feature.</p>"},{"location":"api-reference/python/cost-metrics/#see-also","title":"See Also","text":"<ul> <li>Cost Metrics Guide</li> <li>Builders</li> <li>Samplers (for Bayesian inference)</li> <li>Parameter Uncertainty Tutorial</li> </ul>"},{"location":"api-reference/python/optimisers/","title":"Optimisers","text":"<p>Optimisation algorithms for finding parameter values that minimise the objective function.</p>"},{"location":"api-reference/python/optimisers/#overview","title":"Overview","text":"Optimiser Type Best For Parameters <code>NelderMead</code> Gradient-free Local search, &lt; 10 params, noisy 2-10 <code>CMAES</code> Gradient-free Global search, 10-100+ params 1-100+ <code>Adam</code> Gradient-based Smooth objectives, fast convergence Any"},{"location":"api-reference/python/optimisers/#nelder-mead","title":"Nelder-Mead","text":""},{"location":"api-reference/python/optimisers/#diffid.NelderMead","title":"NelderMead","text":"<p>Classic simplex-based direct search optimiser.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a Nelder-Mead optimiser with default coefficients.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_step_size","title":"with_step_size","text":"<pre><code>with_step_size(step_size)\n</code></pre> <p>Set the initial global step-size (standard deviation).</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_max_iter","title":"with_max_iter","text":"<pre><code>with_max_iter(max_iter)\n</code></pre> <p>Limit the number of simplex iterations.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_threshold","title":"with_threshold","text":"<pre><code>with_threshold(threshold)\n</code></pre> <p>Set the stopping threshold on simplex size or objective reduction.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_position_tolerance","title":"with_position_tolerance","text":"<pre><code>with_position_tolerance(tolerance)\n</code></pre> <p>Stop once simplex vertices fall within the supplied positional tolerance.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_max_evaluations","title":"with_max_evaluations","text":"<pre><code>with_max_evaluations(max_evaluations)\n</code></pre> <p>Abort after evaluating the objective <code>max_evaluations</code> times.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_coefficients","title":"with_coefficients","text":"<pre><code>with_coefficients(alpha, gamma, rho, sigma)\n</code></pre> <p>Override the reflection, expansion, contraction, and shrink coefficients.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.with_patience","title":"with_patience","text":"<pre><code>with_patience(patience)\n</code></pre> <p>Abort if the objective fails to improve within the allotted time.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>float or timedelta</code> <p>Either seconds (float) or a timedelta object</p> required"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.run","title":"run","text":"<pre><code>run(problem, initial)\n</code></pre> <p>Optimise the given problem starting from the provided initial simplex centre.</p>"},{"location":"api-reference/python/optimisers/#diffid.NelderMead.init","title":"init","text":"<pre><code>init(initial, bounds=None)\n</code></pre> <p>Initialize ask-tell optimization state.</p> <p>Returns a NelderMeadState object that can be used for incremental optimization via the ask-tell interface.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>list[float]</code> <p>Initial parameter vector (simplex center)</p> required <code>bounds</code> <code>list[tuple[float, float]]</code> <p>Parameter bounds as [(lower, upper), ...]. If None, unbounded.</p> <code>None</code> <p>Returns:</p> Type Description <code>NelderMeadState</code> <p>State object for ask-tell optimization</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; optimiser = diffid.NelderMead()\n&gt;&gt;&gt; state = optimiser.init(initial=[1.0, 2.0])\n&gt;&gt;&gt; while True:\n...     result = state.ask()\n...     if isinstance(result, diffid.Done):\n...         break\n...     values = [evaluate(pt) for pt in result.points]\n...     state.tell(values)\n</code></pre>"},{"location":"api-reference/python/optimisers/#example-usage","title":"Example Usage","text":"<pre><code>import diffid as chron\n\n# Create optimiser with custom settings\noptimiser = (\n    diffid.NelderMead()\n    .with_max_iter(5000)\n    .with_step_size(0.1)\n    .with_threshold(1e-6)\n)\n\n# Run optimisation\nresult = optimiser.run(problem, initial_guess=[1.0, 2.0])\n</code></pre>"},{"location":"api-reference/python/optimisers/#when-to-use","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>No gradient computation required</li> <li>Robust to noisy objectives</li> <li>Simple and reliable for small problems</li> <li>Default choice for quick exploration</li> </ul> <p>Limitations:</p> <ul> <li>Slow convergence for &gt; 10 parameters</li> <li>Can get stuck in local minima</li> <li>Performance degrades with dimensionality</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Initial parameter exploration</li> <li>Noisy experimental data</li> <li>Small-scale problems (&lt; 10 parameters)</li> </ul>"},{"location":"api-reference/python/optimisers/#parameter-tuning","title":"Parameter Tuning","text":"<ul> <li> <p><code>step_size</code>: Initial simplex size (default: 1.0)</p> <ul> <li>Larger values explore more globally</li> <li>Smaller values for local refinement</li> <li>Start with 10-50% of parameter range</li> </ul> </li> <li> <p><code>threshold</code>: Convergence tolerance (default: 1e-6)</p> <ul> <li>Smaller for higher precision</li> <li>Larger for faster termination</li> </ul> </li> <li> <p><code>max_iter</code>: Maximum iterations (default: 1000)</p> <ul> <li>Rule of thumb: <code>100 * n_parameters</code> minimum</li> </ul> </li> </ul> <p>See the Nelder-Mead Algorithm Guide for more details.</p>"},{"location":"api-reference/python/optimisers/#cma-es","title":"CMA-ES","text":""},{"location":"api-reference/python/optimisers/#diffid.CMAES","title":"CMAES","text":"<p>Covariance Matrix Adaptation Evolution Strategy optimiser.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create a CMA-ES optimiser with library defaults.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_max_iter","title":"with_max_iter","text":"<pre><code>with_max_iter(max_iter)\n</code></pre> <p>Limit the number of iterations/generations before termination.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_threshold","title":"with_threshold","text":"<pre><code>with_threshold(threshold)\n</code></pre> <p>Set the stopping threshold on the best objective value.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_step_size","title":"with_step_size","text":"<pre><code>with_step_size(step_size)\n</code></pre> <p>Set the initial global step-size (standard deviation).</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_patience","title":"with_patience","text":"<pre><code>with_patience(patience)\n</code></pre> <p>Abort the run if no improvement occurs for the given wall-clock duration.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>float or timedelta</code> <p>Either seconds (float) or a timedelta object</p> required"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_population_size","title":"with_population_size","text":"<pre><code>with_population_size(population_size)\n</code></pre> <p>Specify the number of offspring evaluated per generation.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.with_seed","title":"with_seed","text":"<pre><code>with_seed(seed)\n</code></pre> <p>Initialise the internal RNG for reproducible runs.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.run","title":"run","text":"<pre><code>run(problem, initial)\n</code></pre> <p>Optimise the given problem starting from the provided mean vector.</p>"},{"location":"api-reference/python/optimisers/#diffid.CMAES.init","title":"init","text":"<pre><code>init(initial, bounds=None)\n</code></pre> <p>Initialize ask-tell optimization state.</p> <p>Returns a CMAESState object that can be used for incremental optimization via the ask-tell interface.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>list[float]</code> <p>Initial mean vector for the search distribution</p> required <code>bounds</code> <code>list[tuple[float, float]]</code> <p>Parameter bounds as [(lower, upper), ...]. If None, unbounded.</p> <code>None</code> <p>Returns:</p> Type Description <code>CMAESState</code> <p>State object for ask-tell optimization</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; optimiser = diffid.CMAES()\n&gt;&gt;&gt; state = optimiser.init(initial=[1.0, 2.0])\n&gt;&gt;&gt; while True:\n...     result = state.ask()\n...     if isinstance(result, diffid.Done):\n...         break\n...     values = [evaluate(pt) for pt in result.points]\n...     state.tell(values)\n</code></pre>"},{"location":"api-reference/python/optimisers/#example-usage_1","title":"Example Usage","text":"<pre><code>import diffid as chron\n\n# Create CMA-ES optimiser\noptimiser = (\n    diffid.CMAES()\n    .with_max_iter(1000)\n    .with_step_size(0.5)\n    .with_population_size(20)\n    .with_seed(42)  # For reproducibility\n)\n\nresult = optimiser.run(problem, initial_guess=[0.5, 0.5])\n</code></pre>"},{"location":"api-reference/python/optimisers/#when-to-use_1","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Global optimisation (avoids local minima)</li> <li>Scales to high dimensions (10-100+ parameters)</li> <li>Parallelisable (evaluates population in parallel)</li> <li>Self-adapting (no gradient tuning needed)</li> </ul> <p>Limitations:</p> <ul> <li>More function evaluations than gradient methods</li> <li>Requires population-sized memory</li> <li>Stochastic (results vary between runs)</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Global parameter search</li> <li>High-dimensional problems (&gt; 10 parameters)</li> <li>Multi-modal landscapes</li> <li>Parallel hardware available</li> </ul>"},{"location":"api-reference/python/optimisers/#parameter-tuning_1","title":"Parameter Tuning","text":"<ul> <li> <p><code>step_size</code>: Initial search radius (default: 1.0)</p> <ul> <li>Start with ~\u2153 of expected parameter range</li> <li>Too large: slow convergence</li> <li>Too small: premature convergence</li> </ul> </li> <li> <p><code>population_size</code>: Offspring per generation (default: <code>4 + floor(3*ln(n_params))</code>)</p> <ul> <li>Larger populations explore more but cost more</li> <li>Typical range: 10-100</li> <li>Must match available parallelism</li> </ul> </li> <li> <p><code>max_iter</code>: Maximum generations (default: 1000)</p> <ul> <li>Each iteration evaluates <code>population_size</code> candidates</li> <li>Total evaluations = <code>max_iter * population_size</code></li> </ul> </li> <li> <p><code>threshold</code>: Objective value threshold (default: 1e-6)</p> <ul> <li>Stop when best value &lt; threshold</li> </ul> </li> <li> <p><code>seed</code>: Random seed for reproducibility</p> <ul> <li>Omit for non-deterministic runs</li> <li>Set for reproducible benchmarks</li> </ul> </li> </ul> <p>See the CMA-ES Algorithm Guide for more details.</p>"},{"location":"api-reference/python/optimisers/#adam","title":"Adam","text":""},{"location":"api-reference/python/optimisers/#diffid.Adam","title":"Adam","text":"<p>Adaptive Moment Estimation (Adam) gradient-based optimiser.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.__new__","title":"__new__","text":"<pre><code>__new__()\n</code></pre> <p>Create an Adam optimiser with library defaults.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_max_iter","title":"with_max_iter","text":"<pre><code>with_max_iter(max_iter)\n</code></pre> <p>Limit the maximum number of optimisation iterations.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_threshold","title":"with_threshold","text":"<pre><code>with_threshold(threshold)\n</code></pre> <p>Set the stopping threshold on the gradient norm.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_step_size","title":"with_step_size","text":"<pre><code>with_step_size(step_size)\n</code></pre> <p>Configure the base learning rate / step size.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_betas","title":"with_betas","text":"<pre><code>with_betas(beta1, beta2)\n</code></pre> <p>Override the exponential decay rates for the first and second moments.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_eps","title":"with_eps","text":"<pre><code>with_eps(eps)\n</code></pre> <p>Override the numerical stability constant added to the denominator.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.with_patience","title":"with_patience","text":"<pre><code>with_patience(patience)\n</code></pre> <p>Abort the run once the patience window has elapsed.</p> <p>Parameters:</p> Name Type Description Default <code>patience</code> <code>float or timedelta</code> <p>Either seconds (float) or a timedelta object</p> required"},{"location":"api-reference/python/optimisers/#diffid.Adam.run","title":"run","text":"<pre><code>run(problem, initial)\n</code></pre> <p>Optimise the given problem using Adam starting from the provided point.</p>"},{"location":"api-reference/python/optimisers/#diffid.Adam.init","title":"init","text":"<pre><code>init(initial, bounds=None)\n</code></pre> <p>Initialize ask-tell optimization state.</p> <p>Returns an AdamState object that can be used for incremental optimization via the ask-tell interface.</p> <p>Parameters:</p> Name Type Description Default <code>initial</code> <code>list[float]</code> <p>Initial parameter vector</p> required <code>bounds</code> <code>list[tuple[float, float]]</code> <p>Parameter bounds as [(lower, upper), ...]. If None, unbounded.</p> <code>None</code> <p>Returns:</p> Type Description <code>AdamState</code> <p>State object for ask-tell optimization</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; optimiser = diffid.Adam()\n&gt;&gt;&gt; state = optimiser.init(initial=[1.0, 2.0])\n&gt;&gt;&gt; while True:\n...     result = state.ask()\n...     if isinstance(result, diffid.Done):\n...         break\n...     values = [evaluate_with_gradient(pt) for pt in result.points]\n...     state.tell(values)\n</code></pre>"},{"location":"api-reference/python/optimisers/#example-usage_2","title":"Example Usage","text":"<pre><code>import diffid as chron\n\n# Create Adam optimiser\noptimiser = (\n    diffid.Adam()\n    .with_max_iter(5000)\n    .with_step_size(0.01)  # Learning rate\n    .with_betas(0.9, 0.999)\n    .with_threshold(1e-6)\n)\n\nresult = optimiser.run(problem, initial_guess=[1.0, 2.0])\n</code></pre>"},{"location":"api-reference/python/optimisers/#when-to-use_2","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Fast convergence on smooth objectives</li> <li>Adaptive learning rate</li> <li>Well-suited for large-scale problems</li> <li>Efficient (uses gradients)</li> </ul> <p>Limitations:</p> <ul> <li>Requires automatic differentiation</li> <li>Can get stuck in local minima</li> <li>Sensitive to learning rate tuning</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Smooth, differentiable objectives</li> <li>Large-scale problems</li> <li>When gradients are available or cheap to compute</li> </ul>"},{"location":"api-reference/python/optimisers/#parameter-tuning_2","title":"Parameter Tuning","text":"<ul> <li> <p><code>step_size</code>: Learning rate (default: 0.001)</p> <ul> <li>Most critical parameter</li> <li>Too large: oscillation or divergence</li> <li>Too small: slow convergence</li> <li>Try: 0.1, 0.01, 0.001, 0.0001</li> </ul> </li> <li> <p><code>betas</code>: Momentum decay rates (default: (0.9, 0.999))</p> <ul> <li><code>beta1</code>: First moment (mean) decay</li> <li><code>beta2</code>: Second moment (variance) decay</li> <li>Rarely need tuning, defaults work well</li> </ul> </li> <li> <p><code>eps</code>: Numerical stability constant (default: 1e-8)</p> <ul> <li>Prevents division by zero</li> <li>Almost never needs tuning</li> </ul> </li> <li> <p><code>threshold</code>: Gradient norm threshold (default: 1e-6)</p> <ul> <li>Stop when gradient is small</li> <li>Smaller for higher precision</li> </ul> </li> </ul> <p>See the Adam Algorithm Guide for more details.</p>"},{"location":"api-reference/python/optimisers/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/python/optimisers/#running-optimisation","title":"Running Optimisation","text":"<p>All optimisers have a <code>.run()</code> method:</p> <pre><code>result = optimiser.run(problem, initial_guess)\n</code></pre> <p>For the default optimiser (Nelder-Mead):</p> <pre><code>result = problem.optimise()  # Uses Nelder-Mead with defaults\n</code></pre>"},{"location":"api-reference/python/optimisers/#configuring-stopping-criteria","title":"Configuring Stopping Criteria","text":"<pre><code>optimiser = (\n    diffid.CMAES()\n    .with_max_iter(10000)         # Maximum iterations\n    .with_threshold(1e-8)         # Objective threshold\n    .with_patience(300.0)         # Patience in seconds\n)\n</code></pre> <p>The optimiser stops when: 1. <code>max_iter</code> iterations reached, OR 2. Objective value &lt; <code>threshold</code>, OR 3. <code>patience</code> seconds elapsed without improvement</p>"},{"location":"api-reference/python/optimisers/#reproducibility","title":"Reproducibility","text":"<p>For stochastic optimisers (CMA-ES), set a seed:</p> <pre><code>optimiser = diffid.CMAES().with_seed(42)\nresult1 = optimiser.run(problem, [0.0, 0.0])\nresult2 = optimiser.run(problem, [0.0, 0.0])\n# result1 == result2 (same random sequence)\n</code></pre>"},{"location":"api-reference/python/optimisers/#warm-starts","title":"Warm Starts","text":"<p>Run multiple optimisations with different starting points:</p> <pre><code>import numpy as np\n\ninitial_guesses = [\n    [1.0, 1.0],\n    [-1.0, -1.0],\n    [0.0, 2.0],\n]\n\nresults = [optimiser.run(problem, guess) for guess in initial_guesses]\nbest_result = min(results, key=lambda r: r.value)\n</code></pre>"},{"location":"api-reference/python/optimisers/#choosing-an-optimiser","title":"Choosing an Optimiser","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Gradients available?}\n    B --&gt;|Yes| C[Adam]\n    B --&gt;|No| D{Problem size?}\n    D --&gt;|&lt; 10 params| E[Nelder-Mead]\n    D --&gt;|&gt; 10 params| F[CMA-ES]\n    D --&gt;|Need global search| F</code></pre> <p>For detailed guidance, see: - Choosing an Optimiser Guide - Tuning Optimisers Guide</p>"},{"location":"api-reference/python/optimisers/#see-also","title":"See Also","text":"<ul> <li>Algorithm Guides</li> <li>Results</li> <li>Tutorials</li> </ul>"},{"location":"api-reference/python/results/","title":"Results","text":"<p>Result objects returned by optimisers and samplers containing optimal parameters, diagnostics, and metadata.</p>"},{"location":"api-reference/python/results/#optimisationresults","title":"OptimisationResults","text":"<p>All optimisers return an <code>OptimisationResults</code> object with the following attributes:</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults","title":"OptimisationResults","text":"<p>Container for optimiser outputs and diagnostic metadata.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.x","title":"x  <code>property</code>","text":"<pre><code>x\n</code></pre> <p>Decision vector corresponding to the best-found objective value.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Best parameter vector as a NumPy array</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.value","title":"value  <code>property</code>","text":"<pre><code>value\n</code></pre> <p>Objective value evaluated at <code>x</code>.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.iterations","title":"iterations  <code>property</code>","text":"<pre><code>iterations\n</code></pre> <p>Number of iterations performed by the optimiser.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.evaluations","title":"evaluations  <code>property</code>","text":"<pre><code>evaluations\n</code></pre> <p>Total number of objective function evaluations.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.time","title":"time  <code>property</code>","text":"<pre><code>time\n</code></pre> <p>Total number of objective function evaluations.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.success","title":"success  <code>property</code>","text":"<pre><code>success\n</code></pre> <p>Whether the run satisfied its convergence criteria.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.message","title":"message  <code>property</code>","text":"<pre><code>message\n</code></pre> <p>Human-readable status message summarising the termination state.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.termination_reason","title":"termination_reason  <code>property</code>","text":"<pre><code>termination_reason\n</code></pre> <p>Structured termination flag describing why the run ended.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.final_simplex","title":"final_simplex  <code>property</code>","text":"<pre><code>final_simplex\n</code></pre> <p>Simplex vertices at termination, when provided by the optimiser.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.final_simplex_values","title":"final_simplex_values  <code>property</code>","text":"<pre><code>final_simplex_values\n</code></pre> <p>Objective values corresponding to <code>final_simplex</code>.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.covariance","title":"covariance  <code>property</code>","text":"<pre><code>covariance\n</code></pre> <p>Estimated covariance of the search distribution, if available.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Render a concise summary of the optimisation outcome.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Return a human-readable summary of the result.</p>"},{"location":"api-reference/python/results/#diffid.OptimisationResults.__bool__","title":"__bool__","text":"<pre><code>__bool__()\n</code></pre> <p>Return truthiness based on optimization success.</p> <p>Allows using <code>if result:</code> instead of <code>if result.success:</code>.</p>"},{"location":"api-reference/python/results/#example-usage","title":"Example Usage","text":"<pre><code>import diffid as chron\n\nproblem = builder.build()\nresult = problem.optimise()\n\n# Access results\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Evaluations: {result.evaluations}\")\nprint(f\"Message: {result.message}\")\n</code></pre>"},{"location":"api-reference/python/results/#attributes","title":"Attributes","text":""},{"location":"api-reference/python/results/#x","title":"<code>x</code>","text":"<p>Type: <code>numpy.ndarray</code></p> <p>Optimal parameter values found by the optimiser.</p> <pre><code>result.x  # e.g., array([1.0, 2.5, 0.8])\n</code></pre> <p>Parameters are ordered according to <code>.with_parameter()</code> calls:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_parameter(\"alpha\", 1.0)  # result.x[0]\n    .with_parameter(\"beta\", 2.0)   # result.x[1]\n)\n</code></pre>"},{"location":"api-reference/python/results/#value","title":"<code>value</code>","text":"<p>Type: <code>float</code></p> <p>Objective function value at the optimum (<code>result.x</code>).</p> <pre><code>result.value  # e.g., 1.234e-08\n</code></pre> <p>For minimisation problems, lower values are better.</p>"},{"location":"api-reference/python/results/#success","title":"<code>success</code>","text":"<p>Type: <code>bool</code></p> <p>Indicates whether optimisation terminated successfully.</p> <pre><code>if result.success:\n    print(\"Optimisation converged\")\nelse:\n    print(f\"Failed: {result.message}\")\n</code></pre> <p><code>True</code> when:</p> <ul> <li>Convergence criteria met</li> <li>Threshold reached</li> <li>Normal termination</li> </ul> <p><code>False</code> when:</p> <ul> <li>Maximum iterations exceeded without convergence</li> <li>Numerical errors occurred</li> <li>User-requested termination</li> </ul>"},{"location":"api-reference/python/results/#iterations","title":"<code>iterations</code>","text":"<p>Type: <code>int</code></p> <p>Number of iterations performed.</p> <pre><code>result.iterations  # e.g., 157\n</code></pre> <ul> <li>Nelder-Mead: Number of simplex iterations</li> <li>CMA-ES: Number of generations</li> <li>Adam: Number of gradient steps</li> </ul>"},{"location":"api-reference/python/results/#evaluations","title":"<code>evaluations</code>","text":"<p>Type: <code>int</code></p> <p>Total number of objective function evaluations.</p> <pre><code>result.evaluations  # e.g., 314\n</code></pre> <p>Note that <code>evaluations</code> can be much larger than <code>iterations</code>:</p> <ul> <li>CMA-ES: <code>evaluations \u2248 iterations \u00d7 population_size</code></li> <li>Nelder-Mead: <code>evaluations \u2248 iterations \u00d7 (n_params + 1)</code></li> <li>Adam: <code>evaluations \u2248 iterations</code> (one per step)</li> </ul>"},{"location":"api-reference/python/results/#message","title":"<code>message</code>","text":"<p>Type: <code>str</code></p> <p>Human-readable termination message.</p> <pre><code>result.message\n# e.g., \"Converged successfully\"\n# e.g., \"Maximum iterations reached\"\n# e.g., \"Threshold achieved\"\n</code></pre> <p>Use this for debugging and logging.</p>"},{"location":"api-reference/python/results/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/python/results/#checking-success","title":"Checking Success","text":"<pre><code>result = problem.optimise()\n\nif result.success:\n    print(f\"Found optimum: {result.x}\")\n    print(f\"Objective: {result.value:.3e}\")\nelse:\n    print(f\"Optimisation failed: {result.message}\")\n    print(f\"Best found: {result.x} (value: {result.value:.3e})\")\n</code></pre> <p>Even when <code>success=False</code>, <code>result.x</code> contains the best parameters found.</p>"},{"location":"api-reference/python/results/#extracting-parameters-by-name","title":"Extracting Parameters by Name","text":"<p>Results don't store parameter names. Track them manually if needed:</p> <pre><code>param_names = [\"alpha\", \"beta\", \"gamma\"]\n\nbuilder = diffid.ScalarBuilder().with_objective(func)\nfor name in param_names:\n    builder = builder.with_parameter(name, 1.0)\n\nproblem = builder.build()\nresult = problem.optimise()\n\n# Create a dictionary\nparams = dict(zip(param_names, result.x))\nprint(params)  # {'alpha': 1.05, 'beta': 2.31, 'gamma': 0.87}\n</code></pre>"},{"location":"api-reference/python/results/#comparing-multiple-runs","title":"Comparing Multiple Runs","text":"<pre><code>initial_guesses = [[1, 1], [-1, -1], [0, 2]]\n\nresults = [\n    optimiser.run(problem, guess)\n    for guess in initial_guesses\n]\n\n# Find best result\nbest = min(results, key=lambda r: r.value)\nprint(f\"Best from {len(results)} runs: {best.x}, value={best.value:.3e}\")\n</code></pre>"},{"location":"api-reference/python/results/#assessing-convergence","title":"Assessing Convergence","text":"<pre><code>result = optimiser.run(problem, initial_guess)\n\n# Check various indicators\nconverged = (\n    result.success\n    and result.value &lt; 1e-6\n    and result.iterations &lt; max_iters\n)\n\nif not converged:\n    print(f\"Warning: May not have converged\")\n    print(f\"  Iterations: {result.iterations}\")\n    print(f\"  Value: {result.value:.3e}\")\n    print(f\"  Message: {result.message}\")\n</code></pre>"},{"location":"api-reference/python/results/#sampler-results-future","title":"Sampler Results (Future)","text":"<p>When samplers are fully implemented, they will return different result types:</p>"},{"location":"api-reference/python/results/#mcmc-results","title":"MCMC Results","text":"<pre><code>result = mcmc_sampler.run(problem, initial_guess)\n\nresult.samples        # (n_samples, n_params) array\nresult.log_likelihood  # Log-likelihood values\nresult.acceptance_rate  # For diagnostics\n</code></pre>"},{"location":"api-reference/python/results/#nested-sampling-results","title":"Nested Sampling Results","text":"<pre><code>result = nested_sampler.run(problem, initial_guess)\n\nresult.log_evidence      # Log marginal likelihood\nresult.evidence_error    # Uncertainty in evidence\nresult.samples          # Posterior samples\nresult.weights          # Sample weights\n</code></pre>"},{"location":"api-reference/python/results/#diagnostics","title":"Diagnostics","text":""},{"location":"api-reference/python/results/#visualising-convergence","title":"Visualising Convergence","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Run optimisation with tracking (custom wrapper)\nvalues = []\n\ndef tracked_func(x):\n    val = original_func(x)\n    values.append(val[0])\n    return val\n\n# ... run optimisation ...\n\nplt.plot(values)\nplt.xlabel('Evaluation')\nplt.ylabel('Objective Value')\nplt.yscale('log')\nplt.title('Optimisation Convergence')\nplt.show()\n</code></pre>"},{"location":"api-reference/python/results/#checking-parameter-bounds","title":"Checking Parameter Bounds","text":"<pre><code>result = problem.optimise()\n\n# Check if parameters are in reasonable range\nfor i, val in enumerate(result.x):\n    if abs(val) &gt; 1e3:\n        print(f\"Warning: Parameter {i} has large magnitude: {val}\")\n</code></pre>"},{"location":"api-reference/python/results/#performance-metrics","title":"Performance Metrics","text":"<pre><code>import time\n\nstart = time.time()\nresult = optimiser.run(problem, initial_guess)\nelapsed = time.time() - start\n\nprint(f\"Total time: {elapsed:.2f}s\")\nprint(f\"Evaluations: {result.evaluations}\")\nprint(f\"Time per evaluation: {elapsed/result.evaluations*1000:.2f}ms\")\n</code></pre>"},{"location":"api-reference/python/results/#see-also","title":"See Also","text":"<ul> <li>Optimisers</li> <li>Samplers</li> <li>Choosing an Optimiser</li> <li>Troubleshooting</li> </ul>"},{"location":"api-reference/python/samplers/","title":"Samplers","text":"<p>MCMC and nested sampling algorithms for posterior exploration and model comparison.</p>"},{"location":"api-reference/python/samplers/#overview","title":"Overview","text":"Sampler Type Use Case <code>MetropolisHastings</code> MCMC Posterior exploration, uncertainty quantification <code>DynamicNestedSampling</code> Nested Model evidence, Bayes factors <p>Samplers complement optimisers by providing full posterior distributions rather than point estimates.</p>"},{"location":"api-reference/python/samplers/#metropolis-hastings","title":"Metropolis-Hastings","text":"<p>MCMC sampling for exploring parameter posterior distributions.</p> <pre><code>import diffid as chron\n\n# Will be available in a future release\nsampler = (\n    diffid.MetropolisHastings()\n    .with_max_iter(10000)\n    .with_step_size(0.1)\n    .with_burn_in(1000)\n    .with_seed(42)\n)\n\nresult = sampler.run(problem, initial_guess)\n\n# Access samples\nprint(result.samples.shape)  # (n_samples, n_params)\nprint(result.acceptance_rate)  # Target: 0.2-0.4\n</code></pre>"},{"location":"api-reference/python/samplers/#when-to-use","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Provides full posterior distribution</li> <li>Quantifies parameter uncertainty</li> <li>Captures correlations between parameters</li> <li>Enables credible intervals</li> </ul> <p>Limitations:</p> <ul> <li>Requires many function evaluations</li> <li>Need to assess convergence</li> <li>Requires likelihood (GaussianNLL cost metric)</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Uncertainty quantification</li> <li>Confidence intervals</li> <li>Posterior predictive distributions</li> <li>Parameter correlations</li> </ul>"},{"location":"api-reference/python/samplers/#dynamic-nested-sampling","title":"Dynamic Nested Sampling","text":"<p>Nested sampling for calculating model evidence (marginal likelihood) for model comparison.</p> <pre><code>import diffid as chron\n\n# Will be available in a future release\nsampler = (\n    diffid.DynamicNestedSampling()\n    .with_max_iter(5000)\n    .with_n_live_points(500)\n    .with_seed(42)\n)\n\nresult = sampler.run(problem, initial_guess)\n\n# Access evidence\nprint(f\"Log evidence: {result.log_evidence:.2f}\")\nprint(f\"Error: {result.evidence_error:.2f}\")\n\n# Posterior samples also available\nprint(result.samples.shape)\n</code></pre>"},{"location":"api-reference/python/samplers/#when-to-use_1","title":"When to Use","text":"<p>Advantages:</p> <ul> <li>Calculates marginal likelihood (evidence)</li> <li>Enables model comparison via Bayes factors</li> <li>Provides posterior samples as byproduct</li> <li>Efficient for multi-modal posteriors</li> </ul> <p>Limitations:</p> <ul> <li>More expensive than MCMC</li> <li>Requires careful tuning of live points</li> <li>Needs likelihood (GaussianNLL cost metric)</li> </ul> <p>Typical Use Cases:</p> <ul> <li>Model comparison</li> <li>Bayes factors</li> <li>Evidence calculation</li> <li>Multi-modal posteriors</li> </ul>"},{"location":"api-reference/python/samplers/#optimisers-vs-samplers","title":"Optimisers vs Samplers","text":"Optimisers Samplers Output Single best parameters Distribution of parameters Use case Point estimate Uncertainty quantification Cost metric SSE, RMSE, GaussianNLL GaussianNLL (required) Computational cost Lower Higher Uncertainty None Full posterior"},{"location":"api-reference/python/samplers/#when-to-use-which","title":"When to Use Which","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Need uncertainty?}\n    B --&gt;|No| C[Use Optimiser]\n    B --&gt;|Yes| D{Need model comparison?}\n    D --&gt;|No| E[Use MCMC]\n    D --&gt;|Yes| F[Use Nested Sampling]</code></pre>"},{"location":"api-reference/python/samplers/#cost-metric-requirement","title":"Cost Metric Requirement","text":"<p>Warning</p> <p>Samplers require a probabilistic cost metric (typically <code>GaussianNLL</code>):</p> <pre><code># Required for samplers\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_cost_metric(diffid.GaussianNLL())  # Required!\n)\n</code></pre> <p>SSE and RMSE cannot be used with samplers as they lack probabilistic interpretation.</p>"},{"location":"api-reference/python/samplers/#workflow-example","title":"Workflow Example","text":"<p>Typical workflow: optimise first, then sample for uncertainty:</p> <pre><code>import diffid as chron\n\n# 1. Build problem with GaussianNLL\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_cost_metric(diffid.GaussianNLL())\n)\nproblem = builder.build()\n\n# 2. Find MAP estimate with optimiser\noptimiser = diffid.CMAES().with_max_iter(1000)\nopt_result = optimiser.run(problem, [1.0])\n\nprint(f\"MAP estimate: {opt_result.x}\")\n\n# 3. Sample around MAP for uncertainty (future API)\n# sampler = diffid.MetropolisHastings().with_max_iter(10000)\n# sample_result = sampler.run(problem, opt_result.x)\n# print(f\"Posterior mean: {sample_result.samples.mean(axis=0)}\")\n# print(f\"Posterior std: {sample_result.samples.std(axis=0)}\")\n</code></pre>"},{"location":"api-reference/python/samplers/#tutorials","title":"Tutorials","text":"<p>The followings present sampler functionality:</p> <ul> <li>Parameter Uncertainty Tutorial</li> <li>Model Comparison Tutorial</li> <li>Choosing a Sampler Guide</li> </ul>"},{"location":"api-reference/python/samplers/#see-also","title":"See Also","text":"<ul> <li>Optimisers - For point estimates</li> <li>Cost Metrics - GaussianNLL required</li> <li>Choosing a Sampler Guide (future)</li> <li>Algorithm Guides</li> </ul>"},{"location":"api-reference/rust/","title":"Rust API Documentation","text":"<p>Diffid's Rust core provides high-performance implementations of all optimisation and sampling algorithms.</p>"},{"location":"api-reference/rust/#official-documentation","title":"Official Documentation","text":"<p>The complete Rust API documentation is hosted on docs.rs:</p> <p> Diffid Rust Documentation on docs.rs</p>"},{"location":"api-reference/rust/#when-to-use-the-rust-api","title":"When to Use the Rust API","text":"<p>Consider using the Rust crate directly when:</p> <ul> <li>Maximum performance is critical</li> <li>Building Rust-native applications</li> <li>Need zero-copy data handling</li> <li>Deploying to embedded systems or constrained environments</li> <li>Building custom tooling around Diffid</li> </ul> <p>For most users, the Python API provides excellent performance with easier integration.</p>"},{"location":"api-reference/rust/#crate-structure","title":"Crate Structure","text":"<pre><code>diffid/\n\u251c\u2500\u2500 builders/           # Problem builders (ScalarBuilder, DiffsolBuilder, etc.)\n\u251c\u2500\u2500 optimisers/         # Optimisation algorithms\n\u2502   \u251c\u2500\u2500 nelder_mead/    # Nelder-Mead simplex\n\u2502   \u251c\u2500\u2500 cmaes/          # CMA-ES evolution strategy\n\u2502   \u2514\u2500\u2500 adam/           # Adam gradient descent\n\u251c\u2500\u2500 sampler/            # MCMC and nested sampling\n\u251c\u2500\u2500 cost/               # Cost metrics (SSE, RMSE, GaussianNLL)\n\u251c\u2500\u2500 problem/            # Problem types and evaluation\n\u2514\u2500\u2500 common/             # Shared types and utilities\n</code></pre>"},{"location":"api-reference/rust/#quick-example","title":"Quick Example","text":"<pre><code>use diffid::prelude::*;\nuse ndarray::array;\n\n// Define objective function\nfn rosenbrock(x: &amp;[f64]) -&gt; f64 {\n    (1.0 - x[0]).powi(2) + 100.0 * (x[1] - x[0].powi(2)).powi(2)\n}\n\nfn main() {\n    // Build problem\n    let builder = ScalarBuilder::new()\n        .with_objective(rosenbrock)\n        .with_parameter(\"x\", 1.5)\n        .with_parameter(\"y\", -1.5);\n\n    let problem = builder.build();\n\n    // Run optimisation\n    let result = problem.optimise();\n\n    println!(\"Optimal parameters: {:?}\", result.x);\n    println!(\"Objective value: {:.3e}\", result.value);\n    println!(\"Success: {}\", result.success);\n}\n</code></pre>"},{"location":"api-reference/rust/#adding-diffid-to-your-project","title":"Adding Diffid to Your Project","text":"<p>Add to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\ndiffid = \"0.2\"\nndarray = \"0.15\"\n</code></pre> <p>For ODE support with DiffSL:</p> <pre><code>[dependencies]\ndiffid = { version = \"0.2\", features = [\"diffsol\"] }\n</code></pre>"},{"location":"api-reference/rust/#key-rust-features","title":"Key Rust Features","text":""},{"location":"api-reference/rust/#zero-copy-performance","title":"Zero-Copy Performance","text":"<p>The Rust API avoids unnecessary allocations and copies:</p> <pre><code>// Efficient in-place evaluation\nlet mut output = vec![0.0; n];\nproblem.evaluate_into(&amp;params, &amp;mut output)?;\n</code></pre>"},{"location":"api-reference/rust/#type-safety","title":"Type Safety","text":"<p>Strong typing catches errors at compile time:</p> <pre><code>// Compiler ensures correct types\nlet builder: ScalarBuilder = ScalarBuilder::new()\n    .with_objective(objective)\n    .with_parameter(\"x\", 1.0);\n\nlet problem: Problem = builder.build();\n</code></pre>"},{"location":"api-reference/rust/#parallel-execution","title":"Parallel Execution","text":"<p>Native Rayon parallelism:</p> <pre><code>use rayon::prelude::*;\n\nlet results: Vec&lt;_&gt; = initial_guesses\n    .par_iter()\n    .map(|guess| optimiser.run(&amp;problem, guess))\n    .collect();\n</code></pre>"},{"location":"api-reference/rust/#documentation-generation","title":"Documentation Generation","text":"<p>Generate local documentation:</p> <pre><code>cd rust\ncargo doc --open --no-deps\n</code></pre> <p>This builds and opens the full API documentation in your browser.</p>"},{"location":"api-reference/rust/#contributing-to-rust-core","title":"Contributing to Rust Core","text":"<p>See the Contributing Guide and Architecture docs for:</p> <ul> <li>Code organization and patterns</li> <li>Adding new optimisers or samplers</li> <li>Implementing custom cost metrics</li> <li>Testing strategies</li> <li>PyO3 binding guidelines</li> </ul>"},{"location":"api-reference/rust/#module-documentation","title":"Module Documentation","text":"<p>Key modules (click through on docs.rs for full details):</p>"},{"location":"api-reference/rust/#builders","title":"<code>builders</code>","text":"<p>Problem construction with fluent API.</p> <pre><code>pub use diffid::builders::{ScalarBuilder, DiffsolBuilder, VectorBuilder};\n</code></pre>"},{"location":"api-reference/rust/#optimisers","title":"<code>optimisers</code>","text":"<p>Optimisation algorithms.</p> <pre><code>pub use diffid::optimisers::{NelderMead, CMAES, Adam};\n</code></pre>"},{"location":"api-reference/rust/#cost","title":"<code>cost</code>","text":"<p>Cost metrics for objective functions.</p> <pre><code>pub use diffid::cost::{CostMetric, SSE, RMSE, GaussianNLL};\n</code></pre>"},{"location":"api-reference/rust/#problem","title":"<code>problem</code>","text":"<p>Problem types and evaluation.</p> <pre><code>pub use diffid::problem::{Problem, ScalarProblem, VectorProblem};\n</code></pre>"},{"location":"api-reference/rust/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use release builds: <code>cargo build --release</code> (10-100x faster than debug)</li> <li>Profile first: Use <code>cargo flamegraph</code> to identify bottlenecks</li> <li>Parallel backends: Enable Rayon for CMA-ES and Diffsol</li> <li>Sparse matrices: Use sparse backend for large ODE systems</li> <li>Avoid allocations: Reuse buffers in hot loops</li> </ol>"},{"location":"api-reference/rust/#examples","title":"Examples","text":"<p>The repository includes Rust examples:</p> <pre><code>cd rust\ncargo run --example rosenbrock\ncargo run --example ode_fitting\n</code></pre> <p>Browse examples on GitHub: rust/examples/</p>"},{"location":"api-reference/rust/#see-also","title":"See Also","text":"<ul> <li>Python API Reference</li> <li>Architecture</li> <li>Contributing</li> </ul>"},{"location":"development/","title":"Development","text":"<p>Resources for contributors and developers working with Diffid.</p>"},{"location":"development/#getting-started-with-development","title":"Getting Started with Development","text":"<ul> <li> <p> Contributing</p> <p>Guidelines for contributing code, documentation, and bug reports.</p> <p> Contributing Guide</p> </li> <li> <p> Architecture</p> <p>Understanding Diffid's Rust core and PyO3 bindings design.</p> <p> Architecture Overview</p> </li> </ul>"},{"location":"development/#quick-setup","title":"Quick Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/bradyplanden/diffid.git\ncd diffid\n\n# Create Python environment\nuv sync\n\n# Build Rust extension with Python bindings\nuv run maturin develop\n\n# Run tests\nuv run pytest -v      # Python tests\ncargo test            # Rust tests\n</code></pre>"},{"location":"development/#development-workflow","title":"Development Workflow","text":""},{"location":"development/#1-make-changes","title":"1. Make Changes","text":"<p>Edit Rust source in <code>rust/src/</code> or Python bindings in <code>python/</code>.</p>"},{"location":"development/#2-rebuild","title":"2. Rebuild","text":"<pre><code>uv run maturin develop  # For Python binding changes\n</code></pre>"},{"location":"development/#3-test","title":"3. Test","text":"<pre><code># Python tests\nuv run pytest -v\n\n# Rust tests\ncargo test\n\n# Both\ncargo test &amp;&amp; uv run pytest -v\n</code></pre>"},{"location":"development/#4-update-stubs","title":"4. Update Stubs","text":"<p>If you modified Python bindings:</p> <pre><code>uv run cargo run -p diffid-py --no-default-features --features stubgen --bin generate_stubs\n</code></pre>"},{"location":"development/#5-format-and-lint","title":"5. Format and Lint","text":"<pre><code># Rust\ncargo fmt\ncargo clippy\n\n# Python\nuv run ruff check .\nuv run ruff format .\n</code></pre>"},{"location":"development/#project-structure","title":"Project Structure","text":"<pre><code>diffid/\n\u251c\u2500\u2500 rust/                   # Rust core implementation\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u2502   \u251c\u2500\u2500 builders/       # Problem builders\n\u2502   \u2502   \u251c\u2500\u2500 optimisers/     # Optimisation algorithms\n\u2502   \u2502   \u251c\u2500\u2500 sampler/        # MCMC and nested sampling\n\u2502   \u2502   \u251c\u2500\u2500 cost/           # Cost metrics\n\u2502   \u2502   \u2514\u2500\u2500 problem/        # Problem types\n\u2502   \u251c\u2500\u2500 Cargo.toml\n\u2502   \u2514\u2500\u2500 tests/              # Rust tests\n\u251c\u2500\u2500 python/                 # Python bindings\n\u2502   \u251c\u2500\u2500 src/diffid/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 _diffid.pyi   # Generated type stubs\n\u2502   \u2514\u2500\u2500 diffid/           # PyO3 bindings source\n\u251c\u2500\u2500 examples/               # Example scripts\n\u251c\u2500\u2500 tests/                  # Python tests\n\u251c\u2500\u2500 docs/                   # Documentation (this site)\n\u251c\u2500\u2500 pyproject.toml          # Python package config\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"development/#key-technologies","title":"Key Technologies","text":"<ul> <li>Rust: Core algorithms, high performance</li> <li>PyO3: Python bindings with minimal overhead</li> <li>Maturin: Build system for Rust Python extensions</li> <li>uv: Fast Python package management</li> <li>MkDocs Material: Documentation site</li> </ul>"},{"location":"development/#testing-strategy","title":"Testing Strategy","text":""},{"location":"development/#unit-tests","title":"Unit Tests","text":"<p>Rust unit tests alongside code:</p> <pre><code>#[cfg(test)]\nmod tests {\n    use super::*;\n\n    #[test]\n    fn test_feature() {\n        // ...\n    }\n}\n</code></pre>"},{"location":"development/#integration-tests","title":"Integration Tests","text":"<p>Python integration tests in <code>tests/</code>:</p> <pre><code>def test_optimisation():\n    builder = diffid.ScalarBuilder().with_objective(func)\n    # ...\n    assert result.success\n</code></pre>"},{"location":"development/#continuous-integration","title":"Continuous Integration","text":"<p>GitHub Actions runs: - Rust tests and clippy - Python tests on multiple versions - Type checking with mypy - Linting with ruff - Documentation builds</p>"},{"location":"development/#documentation","title":"Documentation","text":""},{"location":"development/#rust-docs","title":"Rust Docs","text":"<pre><code>cargo doc --open --no-deps\n</code></pre>"},{"location":"development/#python-docs","title":"Python Docs","text":"<p>Edit markdown files in <code>docs/</code>:</p> <pre><code>mkdocs serve  # Live preview\nmkdocs build  # Build static site\n</code></pre>"},{"location":"development/#docstrings","title":"Docstrings","text":"<p>Use NumPy-style docstrings:</p> <pre><code>def example(x, y):\n    \"\"\"\n    Brief description.\n\n    Parameters\n    ----------\n    x : float\n        Description of x.\n    y : float\n        Description of y.\n\n    Returns\n    -------\n    float\n        Description of return value.\n    \"\"\"\n</code></pre>"},{"location":"development/#code-style","title":"Code Style","text":""},{"location":"development/#rust","title":"Rust","text":"<ul> <li>Follow Rust API Guidelines</li> <li>Use <code>cargo fmt</code> (automatic formatting)</li> <li>Pass <code>cargo clippy</code> (linting)</li> </ul>"},{"location":"development/#python","title":"Python","text":"<ul> <li>Follow PEP 8</li> <li>Use type hints</li> <li>Format with <code>ruff format</code></li> </ul>"},{"location":"development/#performance-profiling","title":"Performance Profiling","text":""},{"location":"development/#rust_1","title":"Rust","text":"<pre><code>cargo install flamegraph\ncargo flamegraph --example your_example\n</code></pre>"},{"location":"development/#python_1","title":"Python","text":"<pre><code>uv pip install py-spy\npy-spy record --native -- python examples/your_example.py\n</code></pre>"},{"location":"development/#see-also","title":"See Also","text":"<ul> <li>Architecture - System design</li> <li>Contributing - Contribution guidelines</li> <li>API Reference - API documentation</li> </ul>"},{"location":"development/architecture/","title":"Architecture","text":""},{"location":"development/architecture/#high-level-overview","title":"High-Level Overview","text":""},{"location":"development/architecture/#key-design-patterns","title":"Key Design Patterns","text":"<ul> <li>Builder Pattern: Fluent API for problem construction</li> <li>Zero-Copy: Efficient data handling between Python and Rust</li> <li>Trait-Based: Extensible algorithm interfaces</li> </ul>"},{"location":"development/architecture/#see-also","title":"See Also","text":"<ul> <li>Contributing</li> <li>Rust API</li> </ul>"},{"location":"development/contributing/","title":"Contributing to Diffid","text":"<p>Coming Soon</p> <p>Detailed contributing guidelines are being written.</p>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<pre><code># Fork and clone\ngit clone https://github.com/YOUR_USERNAME/diffid.git\ncd diffid\n\n# Set up environment\nuv sync\nuv run maturin develop\n\n# Run tests\nuv run pytest -v\ncargo test\n\n# Make changes and submit PR\n</code></pre>"},{"location":"development/contributing/#areas-to-contribute","title":"Areas to Contribute","text":"<ul> <li>Bug fixes</li> <li>New algorithms</li> <li>Documentation</li> <li>Examples</li> <li>Tests</li> </ul>"},{"location":"development/contributing/#see-also","title":"See Also","text":"<ul> <li>Architecture</li> <li>GitHub Issues</li> </ul>"},{"location":"examples/gallery/","title":"Examples Gallery","text":"<p>Visual gallery of Diffid applications and use cases.</p> <p>Gallery Under Construction</p> <p>This gallery is being populated with examples. Check the examples directory for current code.</p>"},{"location":"examples/gallery/#available-examples","title":"Available Examples","text":""},{"location":"examples/gallery/#scalar-optimisation","title":"Scalar Optimisation","text":""},{"location":"examples/gallery/#rosenbrock-function","title":"Rosenbrock Function","text":"<p>Classic 2D optimisation test problem.</p> <p>Files:</p> <ul> <li>python_problem.py</li> <li>python_contour.py</li> </ul> <p>Topics: ScalarBuilder, contour plots, optimiser comparison</p>"},{"location":"examples/gallery/#ode-parameter-fitting","title":"ODE Parameter Fitting","text":""},{"location":"examples/gallery/#logistic-growth","title":"Logistic Growth","text":"<p>Single-variable ODE with DiffSL.</p> <p>File: logistic_growth.py</p> <p>Topics: DiffsolBuilder, DiffSL syntax, data fitting</p>"},{"location":"examples/gallery/#bouncy-ball","title":"Bouncy Ball","text":"<p>Physics-based model with event handling.</p> <p>Files:</p> <ul> <li>bouncy_ball.py</li> <li>bouncy_ball_sampling.py</li> </ul> <p>Topics: Event detection, parameter uncertainty, MCMC</p>"},{"location":"examples/gallery/#model-comparison","title":"Model Comparison","text":""},{"location":"examples/gallery/#bicycle-model","title":"Bicycle Model","text":"<p>Comparing different bicycle dynamics formulations.</p> <p>Files:</p> <ul> <li>bicycle_model_diffsol.py</li> <li>bicycle_model_evidence.py</li> </ul> <p>Topics: Model selection, evidence calculation, Bayes factors</p>"},{"location":"examples/gallery/#multi-backend-ode-solving","title":"Multi-Backend ODE Solving","text":""},{"location":"examples/gallery/#predator-prey-models","title":"Predator-Prey Models","text":"<p>Lotka-Volterra equations with multiple solver backends.</p> <p>Files:</p> <ul> <li>predator_prey_diffsol.py</li> <li>predator_prey_diffrax.py</li> <li>predator_prey_diffeqpy.py</li> </ul> <p>Topics: VectorBuilder, JAX/Diffrax, Julia/DifferentialEquations.jl, performance comparison</p>"},{"location":"examples/gallery/#running-examples","title":"Running Examples","text":"<p>Clone the repository:</p> <pre><code>git clone https://github.com/bradyplanden/diffid.git\ncd diffid\n</code></pre> <p>Install dependencies:</p> <pre><code>pip install diffid matplotlib\n</code></pre> <p>Run an example:</p> <pre><code>python examples/python_problem.py\n</code></pre> <p>For ODE examples:</p> <pre><code>python examples/logistic_growth.py\n</code></pre> <p>For multi-backend examples (requires additional dependencies):</p> <pre><code># For Diffrax (JAX)\npip install jax diffrax\n\n# For DifferentialEquations.jl (Julia)\npip install diffeqpy\n# Then follow Julia setup instructions\n\npython examples/predator_prey/predator_prey_diffrax.py\n</code></pre>"},{"location":"examples/gallery/#contributing-examples","title":"Contributing Examples","text":"<p>Have an interesting use case? We'd love to include it!</p> <ol> <li>Fork the repository</li> <li>Add your example to <code>examples/</code></li> <li>Include a brief comment header explaining the example</li> <li>Open a pull request</li> </ol> <p>See the Contributing Guide for details.</p>"},{"location":"examples/gallery/#see-also","title":"See Also","text":"<ul> <li>Tutorials - Interactive Jupyter notebooks</li> <li>Getting Started - Core concepts</li> <li>API Reference - Complete API docs</li> </ul>"},{"location":"getting-started/","title":"Getting Started with Diffid","text":"<p>Welcome to Diffid! This section will help you get up and running with time-series inference and optimisation.</p>"},{"location":"getting-started/#learning-path","title":"Learning Path","text":"<p>We recommend following this sequence:</p> <ol> <li>Installation - Platform-specific installation instructions and troubleshooting</li> <li>5-Minute Quickstart - Simple scalar optimisation with the Rosenbrock function</li> <li>First ODE Fit - Fitting differential equations to data with DiffSL</li> <li>Core Concepts - Understanding builders, problems, and the ask/tell pattern</li> </ol>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python &gt;= 3.11</li> <li>Basic understanding of Python programming</li> <li>Familiarity with NumPy arrays (helpful but not required)</li> <li>For ODE fitting: basic knowledge of differential equations</li> </ul>"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this section, you will be able to:</p> <ul> <li>Install Diffid on your platform</li> <li>Create and solve scalar optimisation problems</li> <li>Fit differential equations to experimental data</li> <li>Understand the builder pattern and problem types</li> <li>Choose between optimisers and samplers for your use case</li> </ul>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<p>If you encounter issues:</p> <ol> <li>Check the Troubleshooting guide</li> <li>Browse the examples gallery</li> <li>Open an issue on GitHub</li> </ol> <p>Ready to begin? Start with Installation.</p>"},{"location":"getting-started/concepts/","title":"Core Concepts","text":"<p>This guide explains the fundamental concepts and patterns in Diffid.</p>"},{"location":"getting-started/concepts/#the-builder-pattern","title":"The Builder Pattern","text":"<p>Diffid uses the builder pattern for constructing problems. This provides a fluent, chainable API for configuration:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_objective(my_function)\n    .with_parameter(\"x\", 1.0)\n    .with_parameter(\"y\", 2.0)\n)\nproblem = builder.build()\n</code></pre> <p>Benefits:</p> <ul> <li>Clear and readable: Method names clearly describe what's being configured</li> <li>Flexible: Add components in any order</li> <li>Type-safe: Catch errors early with proper type hints</li> <li>Chainable: Fluent interface for concise code</li> </ul>"},{"location":"getting-started/concepts/#problem-types","title":"Problem Types","text":"<p>Diffid provides different builders for different problem types:</p>"},{"location":"getting-started/concepts/#scalarbuilder","title":"ScalarBuilder","text":"<p>For direct function optimisation where you have a Python callable:</p> <pre><code>def objective(x):\n    return np.asarray([x[0]**2 + x[1]**2])\n\nproblem = (\n    diffid.ScalarBuilder()\n    .with_objective(objective)\n    .with_parameter(\"x\", 0.0)\n    .with_parameter(\"y\", 0.0)\n    .build()\n)\n</code></pre> <p>Use when:</p> <ul> <li>You have a direct Python function to minimise</li> <li>No differential equations involved</li> <li>Simple parameter optimisation</li> </ul>"},{"location":"getting-started/concepts/#diffsolbuilder","title":"DiffsolBuilder","text":"<p>For ODE parameter fitting using the built-in DiffSL/Diffsol solver:</p> <pre><code>dsl = \"\"\"\nin_i {r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\"\"\"\n\nproblem = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_backend(\"dense\")\n    .build()\n)\n</code></pre> <p>Use when:</p> <ul> <li>Fitting ODE parameters to time-series data</li> <li>Using DiffSL for model definition</li> <li>Need high-performance multi-threaded solving</li> </ul>"},{"location":"getting-started/concepts/#vectorbuilder","title":"VectorBuilder","text":"<p>For custom ODE solvers (Diffrax, DifferentialEquations.jl, etc.):</p> <pre><code>def solve_ode(params):\n    # Your custom ODE solver\n    # Returns predictions at observation times\n    return predictions\n\nproblem = (\n    diffid.VectorBuilder()\n    .with_objective(solve_ode)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n    .with_parameter(\"beta\", 0.5)\n    .build()\n)\n</code></pre> <p>Use when:</p> <ul> <li>Need a specific solver (JAX/Diffrax, Julia/DifferentialEquations.jl)</li> <li>Complex ODEs not supported by DiffSL</li> <li>Custom forward models beyond ODEs</li> </ul> <p>See the Custom Solvers Guide for examples.</p>"},{"location":"getting-started/concepts/#parameters","title":"Parameters","text":"<p>Parameters are the decision variables you want to optimise:</p> <pre><code>builder = (\n    diffid.ScalarBuilder()\n    .with_parameter(\"x\", initial_value=1.0)  # Name and initial guess\n    .with_parameter(\"y\", initial_value=-1.0)\n)\n</code></pre> <p>Important:</p> <ul> <li>Parameters must have unique names</li> <li>Initial values are required</li> <li>Order matters: results will be returned in the same order</li> </ul>"},{"location":"getting-started/concepts/#optimisers-vs-samplers","title":"Optimisers vs Samplers","text":"<p>Diffid provides two types of algorithms:</p>"},{"location":"getting-started/concepts/#optimisers-finding-the-best-solution","title":"Optimisers: Finding the Best Solution","text":"<p>Goal: Find parameter values that minimise the objective function.</p> <p>Algorithms:</p> <ul> <li>Nelder-Mead: Gradient-free, local search</li> <li>CMA-ES: Gradient-free, global search</li> <li>Adam: Gradient-based (automatic differentiation)</li> </ul> <p>Usage:</p> <pre><code># Default optimiser (Nelder-Mead)\nresult = problem.optimise()\n\n# Specific optimiser\noptimiser = diffid.CMAES().with_max_iter(1000)\nresult = optimiser.run(problem, initial_guess)\n</code></pre> <p>Returns: A single best solution</p>"},{"location":"getting-started/concepts/#samplers-exploring-uncertainty","title":"Samplers: Exploring Uncertainty","text":"<p>Goal: Sample from the posterior distribution to quantify parameter uncertainty.</p> <p>Algorithms:</p> <ul> <li>Metropolis-Hastings: MCMC sampling for posterior exploration</li> <li>Dynamic Nested Sampling: Evidence calculation for model comparison</li> </ul> <p>Usage:</p> <pre><code>sampler = diffid.MetropolisHastings().with_max_iter(10000)\nresult = sampler.run(problem, initial_guess)\n\n# Result contains samples, not a single optimum\nprint(result.samples.shape)  # (n_samples, n_parameters)\n</code></pre> <p>Returns: A collection of samples from the posterior</p> <p>When to use:</p> Use Optimisers When Use Samplers When You want the single best fit You want to quantify uncertainty Point estimates are sufficient You need confidence intervals Computation budget is limited You need full posterior distributions Comparing multiple models (Bayes factors) <p>See Choosing an Optimiser and Choosing a Sampler for detailed guidance.</p>"},{"location":"getting-started/concepts/#the-asktell-pattern","title":"The Ask/Tell Pattern","text":"<p>For advanced use cases, Diffid supports the ask/tell pattern for manual control of the optimisation loop:</p> <pre><code>optimiser = diffid.CMAES().with_max_iter(1000)\n\n# Ask for candidates\ncandidates = optimiser.ask(n_candidates=10)\n\n# Evaluate them (potentially in parallel or on remote machines)\nevaluations = [problem.evaluate(c) for c in candidates]\n\n# Tell the optimiser the results\noptimiser.tell(candidates, evaluations)\n\n# Repeat until convergence\nwhile not optimiser.should_stop():\n    candidates = optimiser.ask()\n    evaluations = [problem.evaluate(c) for c in candidates]\n    optimiser.tell(candidates, evaluations)\n\nresult = optimiser.get_result()\n</code></pre> <p>Use cases:</p> <ul> <li>Distributed optimisation across multiple machines</li> <li>Custom evaluation pipelines</li> <li>Hybrid optimisation strategies</li> <li>Integration with external simulators</li> </ul>"},{"location":"getting-started/concepts/#cost-metrics","title":"Cost Metrics","text":"<p>Cost metrics define how model predictions are compared to observations:</p> <pre><code>from diffid import SSE, RMSE, GaussianNLL\n\n# Sum of squared errors (default)\nbuilder = builder.with_cost_metric(SSE())\n\n# Root mean squared error (normalised)\nbuilder = builder.with_cost_metric(RMSE())\n\n# Gaussian negative log-likelihood (for probabilistic inference)\nbuilder = builder.with_cost_metric(GaussianNLL())\n</code></pre> <p>Common metrics:</p> <ul> <li>SSE (Sum of Squared Errors): Standard least squares, sensitive to outliers</li> <li>RMSE (Root Mean Squared Error): Normalised by number of points</li> <li>GaussianNLL: For Bayesian inference and sampling</li> </ul> <p>See the Cost Metrics Guide for more details.</p>"},{"location":"getting-started/concepts/#results","title":"Results","text":"<p>All optimisers and samplers return result objects with standard attributes:</p>"},{"location":"getting-started/concepts/#optimiser-results","title":"Optimiser Results","text":"<pre><code>result = problem.optimise()\n\nprint(result.x)           # Optimal parameters (NumPy array)\nprint(result.value)       # Objective value at optimum (float)\nprint(result.success)     # Whether optimisation succeeded (bool)\nprint(result.iterations)  # Number of iterations (int)\nprint(result.evaluations) # Number of function evaluations (int)\nprint(result.message)     # Termination message (str)\n</code></pre>"},{"location":"getting-started/concepts/#sampler-results","title":"Sampler Results","text":"<pre><code>result = sampler.run(problem, initial_guess)\n\nprint(result.samples)     # MCMC samples (NumPy array, shape: (n_samples, n_params))\nprint(result.log_likelihood)  # Log-likelihood values\nprint(result.acceptance_rate) # Acceptance rate (for diagnostics)\n</code></pre> <p>For nested sampling:</p> <pre><code>result = dns_sampler.run(problem, initial_guess)\n\nprint(result.log_evidence)     # Log marginal likelihood\nprint(result.evidence_error)   # Uncertainty in evidence\nprint(result.samples)          # Posterior samples\n</code></pre>"},{"location":"getting-started/concepts/#parallelisation","title":"Parallelisation","text":"<p>Diffid automatically parallelises where possible:</p> <ul> <li>DiffsolBuilder: Multi-threaded ODE solving</li> <li>CMA-ES: Parallel candidate evaluation</li> <li>Dynamic Nested Sampling: Parallel live point evaluation</li> </ul> <p>Control parallelism:</p> <pre><code># Limit threads for ODE solving\nbuilder = builder.with_max_threads(4)\n\n# Population size for CMA-ES (larger = more parallel work)\noptimiser = diffid.CMAES().with_population_size(20)\n</code></pre> <p>See the Parallel Execution Guide for details.</p>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorials: Interactive Jupyter notebooks</li> <li>Choosing an Optimiser: Learn when to use each algorithm</li> <li>API Reference: Browse complete API documentation</li> <li>Examples Gallery: Visual gallery of applications</li> </ul>"},{"location":"getting-started/first-ode-fit/","title":"First ODE Fit","text":"<p>This tutorial demonstrates how to fit ordinary differential equations (ODEs) to data using Diffid's DiffSL integration with the Diffsol solver.</p>"},{"location":"getting-started/first-ode-fit/#the-problem-logistic-growth","title":"The Problem: Logistic Growth","text":"<p>We'll fit a logistic growth model to synthetic population data. The logistic growth equation is:</p> \\[\\frac{dy}{dt} = r \\cdot y \\cdot \\left(1 - \\frac{y}{k}\\right)\\] <p>where: \\(r\\) is the growth rate, \\(k\\) is the carrying capacity, and \\(y\\) is the population.</p>"},{"location":"getting-started/first-ode-fit/#complete-example","title":"Complete Example","text":"<pre><code>import numpy as np\nimport diffid as chron\n\n# Define the ODE model in DiffSL syntax\ndsl = \"\"\"\nin_i { r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\"\"\"\n\n# Generate synthetic data from the logistic model with noise\nfrom scipy.integrate import odeint\n\ndef logistic(y, t, r, k):\n    return r * y * (1 - y / k)\n\nt = np.linspace(0.0, 10.0, 51)\ny_true = odeint(logistic, 0.1, t, args=(0.8, 2.0)).flatten()  # True: r=0.8, k=2.0\nnp.random.seed(42)\nobservations = y_true + 0.05 * np.random.randn(len(t))\ndata = np.column_stack((t, observations))\n\n# Build the problem\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"r\", 0.5)  # Initial guess for growth rate\n    .with_parameter(\"k\", 1.0)  # Initial guess for carrying capacity\n    .with_backend(\"dense\")\n)\nproblem = builder.build()\n\n# Run optimisation with CMA-ES\noptimiser = diffid.CMAES().with_max_iter(1000)\nresult = optimiser.run(problem, [0.5, 1.0])\n\n# Display results\nprint(f\"Fitted parameters:\")\nprint(f\"  r (growth rate) = {result.x[0]:.4f}\")\nprint(f\"  k (capacity) = {result.x[1]:.4f}\")\nprint(f\"Objective value: {result.value:.3e}\")\nprint(f\"Success: {result.success}\")\n</code></pre>"},{"location":"getting-started/first-ode-fit/#understanding-diffsl-syntax","title":"Understanding DiffSL Syntax","text":"<p>DiffSL is a domain-specific language for defining differential equations. Let's break down the syntax:</p> <pre><code>in_i { r = 1, k = 1 }            # Input parameter tensor with defaults\nu_i { y = 0.1 }                  # Initial conditions\nF_i { (r * y) * (1 - (y / k)) }  # Right-hand side of dy/dt = ...\n</code></pre>"},{"location":"getting-started/first-ode-fit/#data-format","title":"Data Format","text":"<p>Diffid expects data as a 2D NumPy array where:</p> <ul> <li>First column: Time points</li> <li>Remaining columns: Observed values for each variable</li> </ul> <pre><code># Example: 51 time points, 1 state variable\nt = np.linspace(0.0, 10.0, 51)\nobservations = ...  # Your observed data\ndata = np.column_stack((t, observations))  # Shape: (51, 2)\n</code></pre> <p>For multi-variable systems:</p> <pre><code># Example: 2 state variables\ndata = np.column_stack((t, obs_var1, obs_var2))  # Shape: (n, 3)\n</code></pre>"},{"location":"getting-started/first-ode-fit/#visualising-results","title":"Visualising Results","text":"<pre><code>import matplotlib.pyplot as plt\n\n# Generate fitted curve using optimised parameters\nt_dense = np.linspace(0.0, 10.0, 200)\ny_fitted = odeint(logistic, 0.1, t_dense, args=(result.x[0], result.x[1])).flatten()\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(data[:, 0], data[:, 1], 'o', label='Observed data', alpha=0.6)\nplt.plot(t_dense, y_fitted, '-', label='Fitted model', linewidth=2)\nplt.xlabel('Time')\nplt.ylabel('y')\nplt.title('Logistic Growth Model Fit')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre>"},{"location":"getting-started/first-ode-fit/#common-ode-patterns","title":"Common ODE Patterns","text":""},{"location":"getting-started/first-ode-fit/#multiple-state-variables","title":"Multiple State Variables","text":"<pre><code>dsl = \"\"\"\nin_i { alpha = 1, beta = 1 }\nu_i {\n    x = 1.0\n    y = 0.5\n}\nF_i {\n    alpha * x - beta * x * y,\n    -beta * y + alpha * x * y\n}\nout_i { x, y }\n\"\"\"\n</code></pre>"},{"location":"getting-started/first-ode-fit/#with-algebraic-variables","title":"With Algebraic Variables","text":"<pre><code>dsl = \"\"\"\nin_i {k1 = 1, k2 = 1 }\nu_i { A = 1.0 }\ndudt_i { -k1 * A }\nF_i {\n    -k1 * A,\n    k1 * A - k2 * B\n}\nout_i { A, B }\n\"\"\"\n</code></pre>"},{"location":"getting-started/first-ode-fit/#time-dependent-forcing","title":"Time-Dependent Forcing","text":"<pre><code>dsl = \"\"\"\nin_i { k = 1 }\nu_i { y = 0.0 }\nF_i { k * sin(t) - y }\n\"\"\"\n</code></pre>"},{"location":"getting-started/first-ode-fit/#cost-metrics","title":"Cost Metrics","text":"<p>By default, Diffid uses sum of squared errors (SSE). You can specify different cost metrics as shown below. See the Cost Metrics Guide for more details.</p> <pre><code>from diffid import GaussianNLL, RMSE\n\n# Use Gaussian negative log-likelihood\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_parameter(\"k\", 1.0)\n    .with_cost_metric(GaussianNLL())  # For probabilistic inference\n)\n\n# Or use root mean squared error\nbuilder = builder.with_cost_metric(RMSE())  # Normalised by number of points\n</code></pre>"},{"location":"getting-started/first-ode-fit/#optimiser-selection","title":"Optimiser Selection","text":"<p>Different optimisers work better for different problems. For a detailed comparison, see Choosing an Optimiser.</p>"},{"location":"getting-started/first-ode-fit/#nelder-mead-default","title":"Nelder-Mead (Default)","text":"<pre><code>result = problem.optimise()  # Uses Nelder-Mead\n</code></pre> <p>Best for: Small problems (&lt; 10 parameters), noisy objectives</p>"},{"location":"getting-started/first-ode-fit/#cma-es","title":"CMA-ES","text":"<pre><code>optimiser = diffid.CMAES().with_max_iter(1000).with_step_size(0.5)\nresult = optimiser.run(problem, initial_guess)\n</code></pre> <p>Best for: Global optimisation, 10-100 parameters, parallelisable</p>"},{"location":"getting-started/first-ode-fit/#adam","title":"Adam","text":"<pre><code>optimiser = diffid.Adam().with_max_iter(1000).with_step_size(0.01)\nresult = optimiser.run(problem, initial_guess)\n</code></pre> <p>Best for: Smooth problems, fast convergence on well-behaved objectives</p>"},{"location":"getting-started/first-ode-fit/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/first-ode-fit/#poor-fit-quality","title":"Poor Fit Quality","text":"<ol> <li>Check initial conditions: Ensure they're physically reasonable</li> <li>Try different optimisers: CMA-ES is often more robust than Nelder-Mead</li> <li>Increase iterations: Use <code>.with_max_iter(10000)</code></li> <li>Check data scale: Normalise data if variables have very different magnitudes</li> </ol>"},{"location":"getting-started/first-ode-fit/#solver-errors","title":"Solver Errors","text":"<ol> <li>Stiff equations: Try changing the solver tolerance</li> <li>Numerical instability: Check for divide-by-zero or exp overflow in your ODE</li> <li>Backend mismatch: Try switching between <code>dense</code> and <code>sparse</code></li> </ol> <p>For more help, see the Troubleshooting Guide.</p>"},{"location":"getting-started/first-ode-fit/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts: Understand builders, problems, and the ask/tell pattern</li> <li>ODE Fitting Tutorial: Interactive notebook with more examples</li> <li>Custom Solvers: Integrate Diffrax or DifferentialEquations.jl</li> <li>Parameter Uncertainty: Use MCMC sampling to quantify uncertainty</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Diffid is available as a Python package with pre-built wheels for most platforms.</p>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":"uvpip <p>uv is a fast Python package installer and resolver.</p> <pre><code>uv pip install diffid\n</code></pre> <pre><code>pip install diffid\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Diffid has optional plotting support via matplotlib:</p> uvpip <pre><code>uv pip install \"diffid[plotting]\"\n</code></pre> <pre><code>pip install \"diffid[plotting]\"\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify that Diffid is working correctly:</p> <pre><code>import diffid\nimport numpy as np\n\n# Simple test\ndef test_func(x):\n    return np.asarray([(x[0] - 1.0) ** 2])\n\nbuilder = diffid.ScalarBuilder().with_objective(test_func).with_parameter(\"x\", 0.0)\nproblem = builder.build()\nresult = problem.optimise()\n\nprint(f\"Success: {result.success}\")\nprint(f\"Optimal x: {result.x[0]:.3f}\")\n</code></pre> <p>Expected output: <pre><code>Success: True\nOptimal x: 1.000\n</code></pre></p>"},{"location":"getting-started/installation/#platform-specific-notes","title":"Platform-Specific Notes","text":""},{"location":"getting-started/installation/#linux","title":"Linux","text":"<p>Pre-built wheels are available for x86_64 and aarch64 architectures. No additional setup required.</p>"},{"location":"getting-started/installation/#macos","title":"macOS","text":"<p>Pre-built wheels are available for both Intel (x86_64) and Apple Silicon (arm64) Macs.</p> <p>If you're using Apple Silicon and encounter issues, ensure you're using a native arm64 Python installation rather than running under Rosetta.</p>"},{"location":"getting-started/installation/#windows","title":"Windows","text":"<p>Windows builds are marked experimental. Pre-built wheels are available but don't currently support diffsol gradients due to LLVM integration issues.</p> <p>If you encounter issues consider using Windows Subsystem for Linux (WSL)</p>"},{"location":"getting-started/installation/#building-from-source","title":"Building from Source","text":"<p>If you need to build from source (for development or if pre-built wheels aren't available):</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust &gt;= 1.70</li> <li>Python &gt;= 3.11</li> <li>uv (recommended)</li> </ul>"},{"location":"getting-started/installation/#build-steps","title":"Build Steps","text":"<pre><code># Clone the repository\ngit clone https://github.com/bradyplanden/diffid.git\ncd diffid\n\n# Create Python environment\nuv sync\n\n# Build Rust extension with Python bindings\nuv run maturin develop\n\n# Run tests\nuv run pytest -v\n</code></pre> <p>For additional troubleshooting, see the Troubleshooting Guide.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that Diffid is installed, proceed to the 5-Minute Quickstart to run your first optimisation.</p>"},{"location":"getting-started/quickstart/","title":"5-Minute Quickstart","text":"<p>This quickstart guide demonstrates scalar optimisation using the classic Rosenbrock function.</p>"},{"location":"getting-started/quickstart/#the-problem","title":"The Problem","text":"<p>The Rosenbrock function is defined as:</p> \\[f(x, y) = (1 - x)^2 + 100(y - x^2)^2\\] <p>The global minimum is at \\((x, y) = (1, 1)\\) with \\(f(1, 1) = 0\\).</p>"},{"location":"getting-started/quickstart/#basic-example","title":"Basic Example","text":"<pre><code>import numpy as np\nimport diffid\n\n\ndef rosenbrock(x):\n    \"\"\"The Rosenbrock function - a classic optimisation test problem.\"\"\"\n    value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n    return np.asarray([value])\n\n\n# Build the problem\nbuilder = (\n    diffid.ScalarBuilder()\n    .with_objective(rosenbrock)\n    .with_parameter(\"x\", 1.5)   # Initial guess\n    .with_parameter(\"y\", -1.5)  # Initial guess\n)\nproblem = builder.build()\n\n# Run optimisation (uses Nelder-Mead by default)\nresult = problem.optimise()\n\n# Display results\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\nprint(f\"Success: {result.success}\")\nprint(f\"Iterations: {result.iterations}\")\n</code></pre> <p>Output: <pre><code>Optimal parameters: [1.0, 1.0]\nObjective value: 0.000e+00\nSuccess: True\nIterations: 157\n</code></pre></p>"},{"location":"getting-started/quickstart/#understanding-the-code","title":"Understanding the Code","text":"<ol> <li> <p>Define the objective function: The function must accept a NumPy array and return a NumPy array</p> </li> <li> <p>Create a builder: <code>ScalarBuilder()</code> is used for scalar optimisation problems where you directly evaluate a function</p> </li> <li> <p>Add parameters: Use <code>with_parameter(name, initial_value)</code> to define decision variables</p> </li> <li> <p>Build the problem: Call <code>build()</code> to create an optimisable problem instance</p> </li> <li> <p>Run optimisation: Call <code>optimise()</code> to run the default optimiser (Nelder-Mead)</p> </li> </ol>"},{"location":"getting-started/quickstart/#using-different-optimisers","title":"Using Different Optimisers","text":"<p>You can specify which optimiser to use:</p>"},{"location":"getting-started/quickstart/#cma-es","title":"CMA-ES","text":"<pre><code># Use CMA-ES for global search\noptimiser = diffid.CMAES().with_max_iter(1000).with_step_size(0.5)\nresult = optimiser.run(problem, [1.5, -1.5])\n\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#adam-gradient-based","title":"Adam (Gradient-Based)","text":"<pre><code># Use Adam optimiser\noptimiser = diffid.Adam().with_max_iter(1000).with_step_size(0.01)\nresult = optimiser.run(problem, [1.5, -1.5])\n\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\n</code></pre>"},{"location":"getting-started/quickstart/#visualising-the-optimisation","title":"Visualising the Optimisation","text":"<p>If you installed the <code>plotting</code> extra, you can visualise the optimisation landscape:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport diffid\n\n\ndef rosenbrock(x):\n    value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n    return np.asarray([value])\n\n\n# Create a grid for plotting\nx = np.linspace(-2, 2, 200)\ny = np.linspace(-1, 3, 200)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i, j] = rosenbrock([X[i, j], Y[i, j]])[0]\n\n# Plot contours\nplt.figure(figsize=(10, 8))\nlevels = np.logspace(-1, 3.5, 20)\nplt.contour(X, Y, Z, levels=levels, cmap='viridis')\nplt.colorbar(label='f(x, y)')\n\n# Mark the optimum\nplt.plot(1.0, 1.0, 'r*', markersize=20, label='Global minimum')\n\n# Run optimisation and plot path\nbuilder = (\n    diffid.ScalarBuilder()\n    .with_objective(rosenbrock)\n    .with_parameter(\"x\", -1.5)\n    .with_parameter(\"y\", -0.5)\n)\nproblem = builder.build()\nresult = problem.optimise()\n\nplt.plot(result.x[0], result.x[1], 'go', markersize=10, label='Found optimum')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Rosenbrock Function Optimisation')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n</code></pre> <p></p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>First ODE Fit: Learn how to fit differential equations to data</li> <li>Core Concepts: Understand the builder pattern and problem types</li> <li>Choosing an Optimiser: Learn when to use each optimiser</li> <li>Tutorials: Explore interactive Jupyter notebooks</li> </ul>"},{"location":"guides/","title":"User Guides","text":"<p>In-depth guides for making the most of Diffid's optimisation and sampling capabilities.</p>"},{"location":"guides/#algorithm-selection","title":"Algorithm Selection","text":"<ul> <li> <p> Choosing an Optimiser</p> <p>Decision trees and guidelines for selecting the right optimisation algorithm.</p> <p> Choosing an Optimiser</p> </li> <li> <p> Tuning Optimisers</p> <p>Parameter tuning strategies and troubleshooting for each algorithm.</p> <p> Tuning Guide</p> </li> <li> <p> Choosing a Sampler</p> <p>When to use MCMC vs nested sampling for uncertainty quantification.</p> <p> Choosing a Sampler</p> </li> </ul>"},{"location":"guides/#problem-configuration","title":"Problem Configuration","text":"<ul> <li> <p> Cost Metrics</p> <p>Understanding SSE, RMSE, and GaussianNLL for different use cases.</p> <p> Cost Metrics Guide</p> </li> <li> <p> DiffSL Backend</p> <p>Choosing between dense and sparse solvers for ODE systems.</p> <p> DiffSL Backend Guide</p> </li> <li> <p> Custom Solvers</p> <p>Integrating Diffrax, DifferentialEquations.jl, and other external solvers.</p> <p> Custom Solvers</p> </li> </ul>"},{"location":"guides/#performance","title":"Performance","text":"<ul> <li> <p> Parallel Execution</p> <p>Thread safety, parallelisation strategies, and performance optimisation.</p> <p> Parallel Execution</p> </li> </ul>"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p> Troubleshooting</p> <p>Common errors, solutions, and debugging strategies.</p> <p> Troubleshooting Guide</p> </li> </ul>"},{"location":"guides/#quick-reference","title":"Quick Reference","text":""},{"location":"guides/#when-to-use-each-component","title":"When to Use Each Component","text":"Component Use Case ScalarBuilder Direct function optimisation DiffsolBuilder ODE fitting with DiffSL VectorBuilder Custom solvers (JAX, Julia) Nelder-Mead Local search, &lt; 10 parameters CMA-ES Global search, 10-100+ parameters Adam Gradient-based, smooth objectives SSE Standard least squares RMSE Normalised error GaussianNLL Bayesian inference"},{"location":"guides/#typical-workflows","title":"Typical Workflows","text":"<p>Simple Optimisation: <pre><code>ScalarBuilder \u2192 optimise() \u2192 Result\n</code></pre></p> <p>ODE Fitting: <pre><code>DiffsolBuilder \u2192 CMAES \u2192 Result \u2192 Visualise\n</code></pre></p> <p>Uncertainty Quantification: <pre><code>DiffsolBuilder + GaussianNLL \u2192 Optimise \u2192 MCMC \u2192 Posterior\n</code></pre></p> <p>Model Comparison: <pre><code>Multiple models + GaussianNLL \u2192 Nested Sampling \u2192 Evidence \u2192 Bayes factors\n</code></pre></p>"},{"location":"guides/#see-also","title":"See Also","text":"<ul> <li>Getting Started - Core concepts and installation</li> <li>Tutorials - Interactive notebooks</li> <li>API Reference - Complete API documentation</li> <li>Algorithms - Algorithm-specific documentation</li> </ul>"},{"location":"guides/choosing-optimiser/","title":"Choosing an Optimiser","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for detailed optimiser selection guidance.</p>"},{"location":"guides/choosing-optimiser/#quick-reference","title":"Quick Reference","text":"Problem Characteristics Recommended Optimiser &lt; 10 parameters, noisy Nelder-Mead 10-100+ parameters CMA-ES Smooth, gradients available Adam Need global search CMA-ES Fast local refinement Nelder-Mead"},{"location":"guides/choosing-optimiser/#decision-tree","title":"Decision Tree","text":"<pre><code>graph TD\n    A[Start] --&gt; B{Gradients available?}\n    B --&gt;|Yes| C[Adam]\n    B --&gt;|No| D{Problem size?}\n    D --&gt;|&lt; 10 params| E[Nelder-Mead]\n    D --&gt;|&gt; 10 params| F[CMA-ES]\n    D --&gt;|Need global| F</code></pre>"},{"location":"guides/choosing-optimiser/#see-also","title":"See Also","text":"<ul> <li>Optimisers API</li> <li>Tuning Optimisers</li> <li>Algorithm Details</li> </ul>"},{"location":"guides/choosing-sampler/","title":"Choosing a Sampler","text":"<p>Coming Soon</p> <p>This guide is being written. Sampler Python bindings are also in development.</p>"},{"location":"guides/choosing-sampler/#quick-reference","title":"Quick Reference","text":"Use Case Sampler Uncertainty quantification Metropolis-Hastings Model comparison Dynamic Nested Sampling Just posterior samples Metropolis-Hastings Need evidence/Bayes factors Dynamic Nested Sampling"},{"location":"guides/choosing-sampler/#when-to-sample-vs-optimize","title":"When to Sample vs Optimize","text":"<p>Use optimisers when: - Point estimate is sufficient - Speed is critical - Don't need uncertainty</p> <p>Use samplers when: - Need confidence intervals - Comparing models - Want full posterior</p>"},{"location":"guides/choosing-sampler/#see-also","title":"See Also","text":"<ul> <li>Samplers API</li> <li>Optimisers vs Samplers</li> </ul>"},{"location":"guides/cost-metrics/","title":"Cost Metrics Guide","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for detailed cost metric guidance.</p>"},{"location":"guides/cost-metrics/#quick-reference","title":"Quick Reference","text":"Metric Use Case SSE Standard least squares RMSE Normalised error, model comparison GaussianNLL Bayesian inference, sampling"},{"location":"guides/cost-metrics/#choosing-a-metric","title":"Choosing a Metric","text":"<ul> <li>SSE: Default for most optimisation</li> <li>RMSE: When comparing models with different data sizes</li> <li>GaussianNLL: Required for samplers (MCMC, nested sampling)</li> </ul>"},{"location":"guides/cost-metrics/#see-also","title":"See Also","text":"<ul> <li>Cost Metrics API</li> <li>Samplers</li> </ul>"},{"location":"guides/custom-solvers/","title":"Custom Solvers Guide","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for Diffrax and DifferentialEquations.jl integration examples.</p>"},{"location":"guides/custom-solvers/#overview","title":"Overview","text":"<p>Use <code>VectorBuilder</code> to integrate custom ODE solvers: - JAX/Diffrax - Julia/DifferentialEquations.jl - Custom Python solvers</p>"},{"location":"guides/custom-solvers/#basic-pattern","title":"Basic Pattern","text":"<pre><code>def custom_solver(params):\n    # Your solver here\n    # Return predictions at observation times\n    return predictions\n\nbuilder = (\n    diffid.VectorBuilder()\n    .with_objective(custom_solver)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n)\n</code></pre>"},{"location":"guides/custom-solvers/#examples","title":"Examples","text":"<p>See the predator-prey examples: - predator_prey_diffrax.py - predator_prey_diffeqpy.py</p>"},{"location":"guides/custom-solvers/#see-also","title":"See Also","text":"<ul> <li>VectorBuilder API</li> <li>Examples Gallery</li> </ul>"},{"location":"guides/diffsol-backend/","title":"DiffSL Backend Guide","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for detailed backend selection guidance.</p>"},{"location":"guides/diffsol-backend/#quick-reference","title":"Quick Reference","text":"Backend Best For <code>\"dense\"</code> &lt; 100 state variables <code>\"sparse\"</code> &gt; 100 state variables, sparse Jacobian"},{"location":"guides/diffsol-backend/#usage","title":"Usage","text":"<pre><code>builder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl)\n    .with_data(data)\n    .with_backend(\"dense\")  # or \"sparse\"\n)\n</code></pre>"},{"location":"guides/diffsol-backend/#see-also","title":"See Also","text":"<ul> <li>DiffsolBuilder API</li> <li>First ODE Fit</li> </ul>"},{"location":"guides/parallel-execution/","title":"Parallel Execution Guide","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for parallelisation strategies.</p>"},{"location":"guides/parallel-execution/#quick-tips","title":"Quick Tips","text":"<ul> <li>DiffsolBuilder: Automatically multi-threaded</li> <li>CMA-ES: Parallel population evaluation</li> <li>Dynamic Nested Sampling: Parallel live point evaluation</li> </ul>"},{"location":"guides/parallel-execution/#controlling-threads","title":"Controlling Threads","text":"<pre><code># Limit threads for ODE solving\nbuilder = builder.with_max_threads(4)\n\n# Population size for CMA-ES\noptimiser = diffid.CMAES().with_population_size(20)\n</code></pre>"},{"location":"guides/parallel-execution/#see-also","title":"See Also","text":"<ul> <li>DiffsolBuilder API</li> <li>CMA-ES API</li> </ul>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"guides/troubleshooting/#import-error","title":"Import Error","text":"<pre><code>ImportError: No module named 'diffid'\n</code></pre> <p>Solution: Install Diffid: <code>pip install diffid</code></p>"},{"location":"guides/troubleshooting/#poor-fit-quality","title":"Poor Fit Quality","text":"<p>Solutions:</p> <ol> <li>Try different optimisers</li> <li>Increase iterations, i.e. <code>.with_max_iter(10000)</code></li> <li>Try different initial conditions</li> <li>Normalise data if scales vary widely</li> </ol>"},{"location":"guides/troubleshooting/#slow-performance","title":"Slow Performance","text":"<p>Solutions:</p> <ol> <li>Use CMA-ES for parallelisation</li> <li>Reduce the number of data points</li> <li>Use sparse backend for large ODE systems</li> </ol>"},{"location":"guides/troubleshooting/#see-also","title":"See Also","text":"<ul> <li>Installation</li> <li>Tuning Optimisers</li> <li>GitHub Issues</li> </ul>"},{"location":"guides/tuning-optimisers/","title":"Tuning Optimisers","text":"<p>Coming Soon</p> <p>This guide is being written. Check back soon for parameter tuning strategies.</p>"},{"location":"guides/tuning-optimisers/#quick-tips","title":"Quick Tips","text":""},{"location":"guides/tuning-optimisers/#nelder-mead","title":"Nelder-Mead","text":"<ul> <li>step_size: Start with 10-50% of parameter range</li> <li>threshold: Use 1e-6 for standard precision</li> <li>max_iter: Use <code>100 * n_parameters</code> as minimum</li> </ul>"},{"location":"guides/tuning-optimisers/#cma-es","title":"CMA-ES","text":"<ul> <li>step_size: Start with ~\u2153 of expected parameter range</li> <li>population_size: Default formula works well, increase for more exploration</li> <li>max_iter: Each iteration evaluates <code>population_size</code> candidates</li> </ul>"},{"location":"guides/tuning-optimisers/#adam","title":"Adam","text":"<ul> <li>step_size: Most critical - try 0.1, 0.01, 0.001, 0.0001</li> <li>betas: Defaults (0.9, 0.999) usually work well</li> </ul>"},{"location":"guides/tuning-optimisers/#see-also","title":"See Also","text":"<ul> <li>Optimisers API</li> <li>Choosing an Optimiser</li> <li>Troubleshooting</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#learning-paths","title":"Learning Paths","text":"<p>Follow these progressive learning paths based on your experience level and goals.</p>"},{"location":"tutorials/#beginner-track","title":"Beginner Track","text":"<p>Perfect for those new to Diffid or optimisation:</p> <ul> <li> <p> Optimisation Basics</p> <p>Learn scalar optimisation with the Rosenbrock function. Compare Nelder-Mead, CMA-ES, and Adam optimisers.</p> <p> Optimisation Basics</p> </li> <li> <p> ODE Fitting with DiffSL</p> <p>Fit a logistic growth model to data using DiffSL and Diffsol.</p> <p> ODE Fitting with DiffSL</p> </li> </ul>"},{"location":"tutorials/#intermediate-track","title":"Intermediate Track","text":"<p>Building on the basics with real-world applications:</p> <ul> <li> <p> Parameter Uncertainty</p> <p>Go from optimisation to MCMC sampling. Quantify parameter uncertainty with confidence intervals.</p> <p> Parameter Uncertainty</p> </li> <li> <p> Model Comparison</p> <p>Use Dynamic Nested Sampling to calculate model evidence and Bayes factors.</p> <p> Model Comparison</p> </li> </ul>"},{"location":"tutorials/#advanced-track","title":"Advanced Track","text":"<p>Complex problems and advanced techniques:</p> <ul> <li> <p> Multi-Backend ODE Solving</p> <p>Compare Diffsol, JAX/Diffrax, and Julia/DifferentialEquations.jl for predator-prey models.</p> <p> Multi-Backend ODE Solving</p> </li> </ul>"},{"location":"tutorials/#quick-reference","title":"Quick Reference","text":"Tutorial Difficulty Key Concepts Prerequisites Optimisation Basics Beginner ScalarBuilder, Optimisers None ODE Fitting Beginner DiffsolBuilder, DiffSL Optimisation Basics Parameter Uncertainty Intermediate MCMC, uncertainty Optimisation Basics, ODE Fitting Model Comparison Intermediate Nested sampling, evidence Optimisation Basics, ODE Fitting, Parameter Uncertainty Multi-Backend Advanced VectorBuilder, JAX, Julia Optimisation Basics, ODE Fitting"},{"location":"tutorials/#running-the-notebooks","title":"Running the Notebooks","text":""},{"location":"tutorials/#installation","title":"Installation","text":"<p>Install Diffid with Jupyter and plotting support:</p> pipuv <pre><code>pip install diffid jupyter matplotlib\n</code></pre> <pre><code>uv pip install diffid jupyter matplotlib\n</code></pre>"},{"location":"tutorials/#clone-and-run","title":"Clone and Run","text":"<pre><code># Clone the repository\ngit clone https://github.com/bradyplanden/diffid.git\ncd diffid/docs/tutorials/notebooks\n\n# Launch Jupyter\njupyter notebook\n</code></pre>"},{"location":"tutorials/#alternative-python-scripts","title":"Alternative: Python Scripts","text":"<p>Prefer scripts to notebooks? Check out the examples directory:</p> <ul> <li><code>python_problem.py</code> - Basic scalar optimisation</li> <li><code>logistic_growth.py</code> - ODE fitting</li> <li><code>bouncy_ball.py</code> / <code>bouncy_ball_sampling.py</code> - Optimisation and MCMC</li> <li><code>bicycle_model_evidence.py</code> - Model comparison</li> <li><code>predator_prey/</code> - Multi-backend comparisons</li> </ul>"},{"location":"tutorials/#contributing","title":"Contributing","text":"<p>Found an issue or want to improve a tutorial?</p> <ol> <li>Fork the repository</li> <li>Edit notebooks in <code>docs/tutorials/notebooks/</code></li> <li>Test your changes locally</li> <li>Submit a pull request</li> </ol> <p>See the Contributing Guide for details.</p>"},{"location":"tutorials/#next-steps","title":"Next Steps","text":"<p>After completing the tutorials:</p> <ul> <li> <p> User Guides</p> <p>In-depth guides on choosing and tuning algorithms</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> </li> <li> <p> Examples Gallery</p> <p>More applications and use cases</p> </li> <li> <p> GitHub Repository</p> <p>Source code and development</p> </li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/","title":"Advanced Cost Functions","text":"In\u00a0[1]: Copied! <pre># Import plotting utilities\nfrom functools import partial\n\nimport diffid\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(42)\n</pre> # Import plotting utilities from functools import partial  import diffid import matplotlib.pyplot as plt import numpy as np  np.random.seed(42) In\u00a0[2]: Copied! <pre># Generate simple test data\nx_data = np.linspace(0, 10, 50)\ny_true = 2.5 * x_data + 1.0\ny_observed = y_true + np.random.normal(0, 2.0, len(x_data))\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_data, y_observed, \"o\", label=\"Observed\", alpha=0.6, markersize=6)\nplt.plot(x_data, y_true, \"--\", label=\"True\", linewidth=2)\nplt.xlabel(\"x\", fontsize=12)\nplt.ylabel(\"y\", fontsize=12)\nplt.title(\"Linear Model Test Data\", fontsize=14, fontweight=\"bold\")\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> # Generate simple test data x_data = np.linspace(0, 10, 50) y_true = 2.5 * x_data + 1.0 y_observed = y_true + np.random.normal(0, 2.0, len(x_data))  plt.figure(figsize=(10, 6)) plt.plot(x_data, y_observed, \"o\", label=\"Observed\", alpha=0.6, markersize=6) plt.plot(x_data, y_true, \"--\", label=\"True\", linewidth=2) plt.xlabel(\"x\", fontsize=12) plt.ylabel(\"y\", fontsize=12) plt.title(\"Linear Model Test Data\", fontsize=14, fontweight=\"bold\") plt.legend(fontsize=11) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() In\u00a0[3]: Copied! <pre># Compare built-in metrics\ndef linear_model(params):\n    \"\"\"Simple linear model: y = slope * x + intercept\"\"\"\n    slope, intercept = params\n    return slope * x_data + intercept\n\n\n# Define problem\nbuilder = (\n    diffid.VectorBuilder()\n    .with_objective(linear_model)\n    .with_data(y_observed)\n    .with_parameter(\"slope\", 1.0)\n    .with_parameter(\"intercept\", 0.0)\n)\n\n# Test each metric\nmetrics = {\n    \"SSE\": diffid.SSE(),\n    \"RMSE\": diffid.RMSE(),\n    \"GaussianNLL\": diffid.GaussianNLL(),\n}\n\nresults = {}\nfor name, metric in metrics.items():\n    result = builder.with_cost(metric).build().optimise()\n    results[name] = result\n\n# Display comparison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"COST METRIC COMPARISON\")\nprint(\"=\" * 70)\nprint(\"True parameters:    [2.5, 1.0]\")\nprint(\"-\" * 70)\nprint(f\"{'Metric':&lt;15} {'Slope':&lt;12} {'Intercept':&lt;12} {'Cost Value'}\")\nprint(\"-\" * 70)\n\nfor name, result in results.items():\n    print(f\"{name:&lt;15} {result.x[0]:&lt;12.4f} {result.x[1]:&lt;12.4f} {result.value:&lt;12.3e}\")\n\nprint(\"\\n\ud83d\udca1 Note: All metrics produce similar parameter estimates!\")\nprint(\"   The cost values differ in scale, but optima are equivalent.\")\n</pre> # Compare built-in metrics def linear_model(params):     \"\"\"Simple linear model: y = slope * x + intercept\"\"\"     slope, intercept = params     return slope * x_data + intercept   # Define problem builder = (     diffid.VectorBuilder()     .with_objective(linear_model)     .with_data(y_observed)     .with_parameter(\"slope\", 1.0)     .with_parameter(\"intercept\", 0.0) )  # Test each metric metrics = {     \"SSE\": diffid.SSE(),     \"RMSE\": diffid.RMSE(),     \"GaussianNLL\": diffid.GaussianNLL(), }  results = {} for name, metric in metrics.items():     result = builder.with_cost(metric).build().optimise()     results[name] = result  # Display comparison print(\"\\n\" + \"=\" * 70) print(\"COST METRIC COMPARISON\") print(\"=\" * 70) print(\"True parameters:    [2.5, 1.0]\") print(\"-\" * 70) print(f\"{'Metric':&lt;15} {'Slope':&lt;12} {'Intercept':&lt;12} {'Cost Value'}\") print(\"-\" * 70)  for name, result in results.items():     print(f\"{name:&lt;15} {result.x[0]:&lt;12.4f} {result.x[1]:&lt;12.4f} {result.value:&lt;12.3e}\")  print(\"\\n\ud83d\udca1 Note: All metrics produce similar parameter estimates!\") print(\"   The cost values differ in scale, but optima are equivalent.\") <pre>\n======================================================================\nCOST METRIC COMPARISON\n======================================================================\nTrue parameters:    [2.5, 1.0]\n----------------------------------------------------------------------\nMetric          Slope        Intercept    Cost Value\n----------------------------------------------------------------------\nSSE             2.3840       1.1288       1.650e+02   \nRMSE            2.3840       1.1288       1.668e+02   \nGaussianNLL     2.3840       1.1288       2.953e+02   \n\n\ud83d\udca1 Note: All metrics produce similar parameter estimates!\n   The cost values differ in scale, but optima are equivalent.\n</pre> In\u00a0[4]: Copied! <pre># Generate heteroscedastic data (error increases with x)\nx_hetero = np.linspace(0, 10, 50)\ny_true_hetero = 2.0 * x_hetero + 3.0\n\n# Error increases linearly with x\nerror_std = 0.5 + 0.3 * x_hetero\ny_hetero = y_true_hetero + np.random.normal(0, error_std)\n\n# Visualize heteroscedastic data\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Error bars showing measurement uncertainty\nax.errorbar(\n    x_hetero,\n    y_hetero,\n    yerr=error_std,\n    fmt=\"o\",\n    alpha=0.6,\n    label=\"Observed (with error bars)\",\n    capsize=3,\n    markersize=6,\n)\nax.plot(x_hetero, y_true_hetero, \"r--\", linewidth=2, label=\"True\")\n\nax.set_xlabel(\"x\", fontsize=12)\nax.set_ylabel(\"y\", fontsize=12)\nax.set_title(\n    \"Heteroscedastic Data (Error Increases with x)\", fontsize=14, fontweight=\"bold\"\n)\nax.legend(fontsize=11)\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Error range: [{error_std.min():.2f}, {error_std.max():.2f}]\")\nprint(\"Notice: Uncertainty increases from left to right!\")\n</pre> # Generate heteroscedastic data (error increases with x) x_hetero = np.linspace(0, 10, 50) y_true_hetero = 2.0 * x_hetero + 3.0  # Error increases linearly with x error_std = 0.5 + 0.3 * x_hetero y_hetero = y_true_hetero + np.random.normal(0, error_std)  # Visualize heteroscedastic data fig, ax = plt.subplots(figsize=(10, 6))  # Error bars showing measurement uncertainty ax.errorbar(     x_hetero,     y_hetero,     yerr=error_std,     fmt=\"o\",     alpha=0.6,     label=\"Observed (with error bars)\",     capsize=3,     markersize=6, ) ax.plot(x_hetero, y_true_hetero, \"r--\", linewidth=2, label=\"True\")  ax.set_xlabel(\"x\", fontsize=12) ax.set_ylabel(\"y\", fontsize=12) ax.set_title(     \"Heteroscedastic Data (Error Increases with x)\", fontsize=14, fontweight=\"bold\" ) ax.legend(fontsize=11) ax.grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(f\"Error range: [{error_std.min():.2f}, {error_std.max():.2f}]\") print(\"Notice: Uncertainty increases from left to right!\") <pre>Error range: [0.50, 3.50]\nNotice: Uncertainty increases from left to right!\n</pre> In\u00a0[5]: Copied! <pre># Custom weighted SSE cost function\nclass WeightedSSE:\n    \"\"\"Weighted sum of squared errors.\"\"\"\n\n    def __init__(self, weights):\n        \"\"\"\n        Parameters\n        ----------\n        weights : array_like\n            Weight for each data point (typically 1/sigma^2)\n        \"\"\"\n        self.weights = np.asarray(weights)\n\n    def __call__(self, predicted, observed):\n        \"\"\"Compute weighted SSE.\"\"\"\n        residuals = observed - predicted\n        return np.sum(self.weights * residuals**2)\n\n\n# Weights: inverse variance (1/sigma^2)\nweights = 1.0 / error_std**2\n\n\ndef linear_model_hetero(params):\n    slope, intercept = params\n    return slope * x_hetero + intercept\n\n\n# Construct custom cost\nweighted_sse = WeightedSSE(weights)\n\n# Fit WITHOUT weights (standard SSE)\nresult_unweighted = (\n    diffid.VectorBuilder()\n    .with_objective(linear_model_hetero)\n    .with_data(y_hetero)\n    .with_parameter(\"slope\", 1.0)\n    .with_parameter(\"intercept\", 0.0)\n    .with_cost(diffid.SSE())  # Standard SSE\n    .build()\n    .optimise()\n)\n\n# Fit with weights\nresult_weighted = (\n    diffid.ScalarBuilder()\n    .with_objective(lambda x: weighted_sse(linear_model_hetero(x), y_hetero))\n    .with_parameter(\"slope\", 1.0)\n    .with_parameter(\"intercept\", 0.0)\n    .build()\n    .optimise()\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"WEIGHTED vs UNWEIGHTED FITTING\")\nprint(\"=\" * 70)\nprint(\"True parameters:        [2.0, 3.0]\")\nprint(f\"Unweighted fit:         {result_unweighted.x}\")\nprint(f\"Weighted fit:           {result_weighted.x}\")\nprint(\"-\" * 70)\nprint(f\"Unweighted error:       {np.linalg.norm(result_unweighted.x - [2.0, 3.0]):.4f}\")\nprint(f\"Weighted error:         {np.linalg.norm(result_weighted.x - [2.0, 3.0]):.4f}\")\nprint(\"\\n\ud83d\udca1 Weighted fit is more accurate!\")\nprint(\"   It down-weights noisy (high-x) points appropriately.\")\n</pre> # Custom weighted SSE cost function class WeightedSSE:     \"\"\"Weighted sum of squared errors.\"\"\"      def __init__(self, weights):         \"\"\"         Parameters         ----------         weights : array_like             Weight for each data point (typically 1/sigma^2)         \"\"\"         self.weights = np.asarray(weights)      def __call__(self, predicted, observed):         \"\"\"Compute weighted SSE.\"\"\"         residuals = observed - predicted         return np.sum(self.weights * residuals**2)   # Weights: inverse variance (1/sigma^2) weights = 1.0 / error_std**2   def linear_model_hetero(params):     slope, intercept = params     return slope * x_hetero + intercept   # Construct custom cost weighted_sse = WeightedSSE(weights)  # Fit WITHOUT weights (standard SSE) result_unweighted = (     diffid.VectorBuilder()     .with_objective(linear_model_hetero)     .with_data(y_hetero)     .with_parameter(\"slope\", 1.0)     .with_parameter(\"intercept\", 0.0)     .with_cost(diffid.SSE())  # Standard SSE     .build()     .optimise() )  # Fit with weights result_weighted = (     diffid.ScalarBuilder()     .with_objective(lambda x: weighted_sse(linear_model_hetero(x), y_hetero))     .with_parameter(\"slope\", 1.0)     .with_parameter(\"intercept\", 0.0)     .build()     .optimise() )  print(\"\\n\" + \"=\" * 70) print(\"WEIGHTED vs UNWEIGHTED FITTING\") print(\"=\" * 70) print(\"True parameters:        [2.0, 3.0]\") print(f\"Unweighted fit:         {result_unweighted.x}\") print(f\"Weighted fit:           {result_weighted.x}\") print(\"-\" * 70) print(f\"Unweighted error:       {np.linalg.norm(result_unweighted.x - [2.0, 3.0]):.4f}\") print(f\"Weighted error:         {np.linalg.norm(result_weighted.x - [2.0, 3.0]):.4f}\") print(\"\\n\ud83d\udca1 Weighted fit is more accurate!\") print(\"   It down-weights noisy (high-x) points appropriately.\") <pre>\n======================================================================\nWEIGHTED vs UNWEIGHTED FITTING\n======================================================================\nTrue parameters:        [2.0, 3.0]\nUnweighted fit:         [1.93530391 3.27981493]\nWeighted fit:           [1.97635486 3.10714606]\n----------------------------------------------------------------------\nUnweighted error:       0.2872\nWeighted error:         0.1097\n\n\ud83d\udca1 Weighted fit is more accurate!\n   It down-weights noisy (high-x) points appropriately.\n</pre> In\u00a0[6]: Copied! <pre># Visualize comparison\nx_plot = np.linspace(0, 10, 200)\ny_true_plot = 2.0 * x_plot + 3.0\ny_unweighted = result_unweighted.x[0] * x_plot + result_unweighted.x[1]\ny_weighted = result_weighted.x[0] * x_plot + result_weighted.x[1]\n\nfig, ax = plt.subplots(figsize=(12, 7))\n\n# Data with error bars\nax.errorbar(\n    x_hetero,\n    y_hetero,\n    yerr=error_std,\n    fmt=\"o\",\n    alpha=0.5,\n    label=\"Data (with uncertainties)\",\n    capsize=3,\n    markersize=6,\n    color=\"gray\",\n)\n\n# Fits\nax.plot(x_plot, y_true_plot, \"k--\", linewidth=3, label=\"True\", alpha=0.8)\nax.plot(x_plot, y_unweighted, linewidth=2.5, label=\"Unweighted fit\", color=\"#ff7f0e\")\nax.plot(x_plot, y_weighted, linewidth=2.5, label=\"Weighted fit\", color=\"#2ca02c\")\n\nax.set_xlabel(\"x\", fontsize=13)\nax.set_ylabel(\"y\", fontsize=13)\nax.set_title(\"Weighted vs Unweighted Fitting\", fontsize=15, fontweight=\"bold\")\nax.legend(fontsize=12, loc=\"upper left\")\nax.grid(True, alpha=0.3)\n\n# Highlight the difference in high-error region\nax.axvspan(7, 10, alpha=0.1, color=\"red\", label=\"High uncertainty region\")\nax.text(\n    8.5,\n    5,\n    \"High uncertainty\\nregion\",\n    ha=\"center\",\n    fontsize=10,\n    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8),\n)\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualize comparison x_plot = np.linspace(0, 10, 200) y_true_plot = 2.0 * x_plot + 3.0 y_unweighted = result_unweighted.x[0] * x_plot + result_unweighted.x[1] y_weighted = result_weighted.x[0] * x_plot + result_weighted.x[1]  fig, ax = plt.subplots(figsize=(12, 7))  # Data with error bars ax.errorbar(     x_hetero,     y_hetero,     yerr=error_std,     fmt=\"o\",     alpha=0.5,     label=\"Data (with uncertainties)\",     capsize=3,     markersize=6,     color=\"gray\", )  # Fits ax.plot(x_plot, y_true_plot, \"k--\", linewidth=3, label=\"True\", alpha=0.8) ax.plot(x_plot, y_unweighted, linewidth=2.5, label=\"Unweighted fit\", color=\"#ff7f0e\") ax.plot(x_plot, y_weighted, linewidth=2.5, label=\"Weighted fit\", color=\"#2ca02c\")  ax.set_xlabel(\"x\", fontsize=13) ax.set_ylabel(\"y\", fontsize=13) ax.set_title(\"Weighted vs Unweighted Fitting\", fontsize=15, fontweight=\"bold\") ax.legend(fontsize=12, loc=\"upper left\") ax.grid(True, alpha=0.3)  # Highlight the difference in high-error region ax.axvspan(7, 10, alpha=0.1, color=\"red\", label=\"High uncertainty region\") ax.text(     8.5,     5,     \"High uncertainty\\nregion\",     ha=\"center\",     fontsize=10,     bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8), )  plt.tight_layout() plt.show() In\u00a0[7]: Copied! <pre># Generate data for polynomial fitting\nnp.random.seed(123)\nx_poly = np.linspace(-1, 1, 20)\ny_true_poly = 2 * x_poly - 3 * x_poly**2  # True: quadratic\ny_poly = y_true_poly + np.random.normal(0, 0.3, len(x_poly))\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_poly, y_poly, \"o\", label=\"Observed\", alpha=0.7, markersize=8)\nplt.plot(x_poly, y_true_poly, \"r--\", linewidth=2, label=\"True (quadratic)\")\nplt.xlabel(\"x\", fontsize=12)\nplt.ylabel(\"y\", fontsize=12)\nplt.title(\"Polynomial Fitting Test Data\", fontsize=14, fontweight=\"bold\")\nplt.legend(fontsize=11)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> # Generate data for polynomial fitting np.random.seed(123) x_poly = np.linspace(-1, 1, 20) y_true_poly = 2 * x_poly - 3 * x_poly**2  # True: quadratic y_poly = y_true_poly + np.random.normal(0, 0.3, len(x_poly))  plt.figure(figsize=(10, 6)) plt.plot(x_poly, y_poly, \"o\", label=\"Observed\", alpha=0.7, markersize=8) plt.plot(x_poly, y_true_poly, \"r--\", linewidth=2, label=\"True (quadratic)\") plt.xlabel(\"x\", fontsize=12) plt.ylabel(\"y\", fontsize=12) plt.title(\"Polynomial Fitting Test Data\", fontsize=14, fontweight=\"bold\") plt.legend(fontsize=11) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() In\u00a0[8]: Copied! <pre># Fit high-degree polynomial (degree 8) - prone to over-fitting\ndegree = 8\n\n\ndef polynomial_model(params):\n    \"\"\"Polynomial model: sum of coefficients * x^i\"\"\"\n    y_pred = np.zeros_like(x_poly)\n    for i, coef in enumerate(params):\n        y_pred += coef * x_poly**i\n    return y_pred\n\n\n# Custom cost with L2 regularisation\nclass RegularisedSSE:\n    \"\"\"SSE with L2 regularisation (Ridge).\"\"\"\n\n    def __init__(self, lambda_reg=0.0):\n        self.lambda_reg = lambda_reg\n\n    def __call__(self, predicted, observed, params=None):\n        # Data term\n        sse = np.sum((observed - predicted) ** 2)\n\n        # Regularisation term (note: params must be passed separately)\n        if params is not None and self.lambda_reg &gt; 0:\n            reg_term = self.lambda_reg * np.sum(params**2)\n            return sse + reg_term\n\n        return sse\n\n\n# Note: Diffid's built-in costs don't support parameter access yet,\n# so we'll compare by manually adding regularisation to parameter update\n\n# Unregularised fit\ninitial_params = np.zeros(degree + 1)\ninitial_params[0] = np.mean(y_poly)\n\nbuilder_poly = diffid.VectorBuilder().with_objective(polynomial_model).with_data(y_poly)\n\nfor i in range(degree + 1):\n    builder_poly = builder_poly.with_parameter(f\"c{i}\", initial_params[i])\n\nresult_unreg = (\n    builder_poly.with_cost(diffid.SSE())\n    .with_optimiser(diffid.NelderMead().with_max_iter(2000))\n    .build()\n    .optimise()\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"POLYNOMIAL FITTING (Degree 8)\")\nprint(\"=\" * 70)\nprint(\"Unregularised coefficients:\")\nprint(f\"  {result_unreg.x}\")\nprint(f\"  Max |coef|: {np.max(np.abs(result_unreg.x)):.2f}\")\nprint(f\"  SSE: {result_unreg.value:.4f}\")\n</pre> # Fit high-degree polynomial (degree 8) - prone to over-fitting degree = 8   def polynomial_model(params):     \"\"\"Polynomial model: sum of coefficients * x^i\"\"\"     y_pred = np.zeros_like(x_poly)     for i, coef in enumerate(params):         y_pred += coef * x_poly**i     return y_pred   # Custom cost with L2 regularisation class RegularisedSSE:     \"\"\"SSE with L2 regularisation (Ridge).\"\"\"      def __init__(self, lambda_reg=0.0):         self.lambda_reg = lambda_reg      def __call__(self, predicted, observed, params=None):         # Data term         sse = np.sum((observed - predicted) ** 2)          # Regularisation term (note: params must be passed separately)         if params is not None and self.lambda_reg &gt; 0:             reg_term = self.lambda_reg * np.sum(params**2)             return sse + reg_term          return sse   # Note: Diffid's built-in costs don't support parameter access yet, # so we'll compare by manually adding regularisation to parameter update  # Unregularised fit initial_params = np.zeros(degree + 1) initial_params[0] = np.mean(y_poly)  builder_poly = diffid.VectorBuilder().with_objective(polynomial_model).with_data(y_poly)  for i in range(degree + 1):     builder_poly = builder_poly.with_parameter(f\"c{i}\", initial_params[i])  result_unreg = (     builder_poly.with_cost(diffid.SSE())     .with_optimiser(diffid.NelderMead().with_max_iter(2000))     .build()     .optimise() )  print(\"\\n\" + \"=\" * 70) print(\"POLYNOMIAL FITTING (Degree 8)\") print(\"=\" * 70) print(\"Unregularised coefficients:\") print(f\"  {result_unreg.x}\") print(f\"  Max |coef|: {np.max(np.abs(result_unreg.x)):.2f}\") print(f\"  SSE: {result_unreg.value:.4f}\") <pre>\n======================================================================\nPOLYNOMIAL FITTING (Degree 8)\n======================================================================\nUnregularised coefficients:\n  [ 1.76916563e-03  1.96793262e+00 -4.26606389e+00  1.70512990e+00\n  3.98781374e+00 -2.59931773e+00 -3.98125430e-01  1.08590028e+00\n -2.43719499e+00]\n  Max |coef|: 4.27\n  SSE: 1.8069\n</pre> In\u00a0[9]: Copied! <pre># Visualize fits\nx_dense = np.linspace(-1, 1, 200)\n\n\ndef eval_poly(x, coeffs):\n    y = np.zeros_like(x)\n    for i, c in enumerate(coeffs):\n        y += c * x**i\n    return y\n\n\ny_true_dense = 2 * x_dense - 3 * x_dense**2\ny_unreg_dense = eval_poly(x_dense, result_unreg.x)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Left: Training data fit\nax1.plot(\n    x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\"\n)\nax1.plot(\n    x_dense, y_true_dense, \"k--\", linewidth=2.5, label=\"True (quadratic)\", alpha=0.8\n)\nax1.plot(x_dense, y_unreg_dense, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\")\nax1.set_xlabel(\"x\", fontsize=12)\nax1.set_ylabel(\"y\", fontsize=12)\nax1.set_title(\"Training Data: Over-fitting\", fontsize=14, fontweight=\"bold\")\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\nax1.set_ylim(-4, 3)\n\n# Right: Extrapolation (shows over-fitting clearly)\nx_extrap = np.linspace(-1.5, 1.5, 200)\ny_true_extrap = 2 * x_extrap - 3 * x_extrap**2\ny_unreg_extrap = eval_poly(x_extrap, result_unreg.x)\n\nax2.plot(\n    x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\"\n)\nax2.plot(x_extrap, y_true_extrap, \"k--\", linewidth=2.5, label=\"True\", alpha=0.8)\nax2.plot(x_extrap, y_unreg_extrap, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\")\nax2.axvline(x=-1, color=\"red\", linestyle=\":\", alpha=0.5)\nax2.axvline(x=1, color=\"red\", linestyle=\":\", alpha=0.5)\nax2.set_xlabel(\"x\", fontsize=12)\nax2.set_ylabel(\"y\", fontsize=12)\nax2.set_title(\"Extrapolation: Catastrophic Failure\", fontsize=14, fontweight=\"bold\")\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\nax2.set_ylim(-10, 5)\nax2.text(-1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\")\nax2.text(1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\u26a0\ufe0f  Over-fitting Alert!\")\nprint(\"   The high-degree polynomial fits training data well but\")\nprint(\"   extrapolates poorly. Regularisation would help!\")\n</pre> # Visualize fits x_dense = np.linspace(-1, 1, 200)   def eval_poly(x, coeffs):     y = np.zeros_like(x)     for i, c in enumerate(coeffs):         y += c * x**i     return y   y_true_dense = 2 * x_dense - 3 * x_dense**2 y_unreg_dense = eval_poly(x_dense, result_unreg.x)  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))  # Left: Training data fit ax1.plot(     x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\" ) ax1.plot(     x_dense, y_true_dense, \"k--\", linewidth=2.5, label=\"True (quadratic)\", alpha=0.8 ) ax1.plot(x_dense, y_unreg_dense, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\") ax1.set_xlabel(\"x\", fontsize=12) ax1.set_ylabel(\"y\", fontsize=12) ax1.set_title(\"Training Data: Over-fitting\", fontsize=14, fontweight=\"bold\") ax1.legend(fontsize=11) ax1.grid(True, alpha=0.3) ax1.set_ylim(-4, 3)  # Right: Extrapolation (shows over-fitting clearly) x_extrap = np.linspace(-1.5, 1.5, 200) y_true_extrap = 2 * x_extrap - 3 * x_extrap**2 y_unreg_extrap = eval_poly(x_extrap, result_unreg.x)  ax2.plot(     x_poly, y_poly, \"o\", label=\"Training data\", alpha=0.7, markersize=8, color=\"gray\" ) ax2.plot(x_extrap, y_true_extrap, \"k--\", linewidth=2.5, label=\"True\", alpha=0.8) ax2.plot(x_extrap, y_unreg_extrap, linewidth=2.5, label=\"Degree-8 fit\", color=\"#d62728\") ax2.axvline(x=-1, color=\"red\", linestyle=\":\", alpha=0.5) ax2.axvline(x=1, color=\"red\", linestyle=\":\", alpha=0.5) ax2.set_xlabel(\"x\", fontsize=12) ax2.set_ylabel(\"y\", fontsize=12) ax2.set_title(\"Extrapolation: Catastrophic Failure\", fontsize=14, fontweight=\"bold\") ax2.legend(fontsize=11) ax2.grid(True, alpha=0.3) ax2.set_ylim(-10, 5) ax2.text(-1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\") ax2.text(1.25, -8, \"Extrapolation\\nregion\", fontsize=10, ha=\"center\")  plt.tight_layout() plt.show()  print(\"\\n\u26a0\ufe0f  Over-fitting Alert!\") print(\"   The high-degree polynomial fits training data well but\") print(\"   extrapolates poorly. Regularisation would help!\") <pre>\n\u26a0\ufe0f  Over-fitting Alert!\n   The high-degree polynomial fits training data well but\n   extrapolates poorly. Regularisation would help!\n</pre> In\u00a0[10]: Copied! <pre># Example: Fit with smoothness penalty\nclass MultiObjectiveCost:\n    \"\"\"Combined data fit + smoothness penalty.\"\"\"\n\n    def __init__(self, weight_fit=1.0, weight_smooth=0.1):\n        self.weight_fit = weight_fit\n        self.weight_smooth = weight_smooth\n\n    def __call__(self, predicted, observed):\n        # Data fit term (SSE)\n        fit_cost = np.sum((observed - predicted) ** 2)\n\n        # Smoothness term (penalise large second derivatives)\n        second_deriv = np.diff(predicted, n=2)\n        smooth_cost = np.sum(second_deriv**2)\n\n        return self.weight_fit * fit_cost + self.weight_smooth * smooth_cost\n\n\n# Fit with different smoothness weights\nweights_smooth = [0.0, 0.1, 1.0, 10.0]\nresults_multi = {}\n\n\n# Cost wrapper\ndef wrapper(x, y_poly, cost_func):\n    return cost_func(polynomial_model(x), y_poly)\n\n\nfor w_smooth in weights_smooth:\n    # Construct custom cost\n    multi_cost = MultiObjectiveCost(weight_fit=1.0, weight_smooth=w_smooth)\n    result = diffid.ScalarBuilder().with_objective(\n        partial(wrapper, y_poly=y_poly, cost_func=multi_cost)\n    )\n\n    for i in range(degree + 1):\n        result = result.with_parameter(f\"c{i}\", initial_params[i])\n\n    result = (\n        result.with_optimiser(diffid.NelderMead().with_max_iter(2000))\n        .build()\n        .optimise()\n    )\n\n    results_multi[w_smooth] = result\n    print(\n        f\"Smoothness weight={w_smooth:5.1f}: SSE={np.sum((eval_poly(x_poly, result.x) - y_poly) ** 2):.3f}\"\n    )\n</pre> # Example: Fit with smoothness penalty class MultiObjectiveCost:     \"\"\"Combined data fit + smoothness penalty.\"\"\"      def __init__(self, weight_fit=1.0, weight_smooth=0.1):         self.weight_fit = weight_fit         self.weight_smooth = weight_smooth      def __call__(self, predicted, observed):         # Data fit term (SSE)         fit_cost = np.sum((observed - predicted) ** 2)          # Smoothness term (penalise large second derivatives)         second_deriv = np.diff(predicted, n=2)         smooth_cost = np.sum(second_deriv**2)          return self.weight_fit * fit_cost + self.weight_smooth * smooth_cost   # Fit with different smoothness weights weights_smooth = [0.0, 0.1, 1.0, 10.0] results_multi = {}   # Cost wrapper def wrapper(x, y_poly, cost_func):     return cost_func(polynomial_model(x), y_poly)   for w_smooth in weights_smooth:     # Construct custom cost     multi_cost = MultiObjectiveCost(weight_fit=1.0, weight_smooth=w_smooth)     result = diffid.ScalarBuilder().with_objective(         partial(wrapper, y_poly=y_poly, cost_func=multi_cost)     )      for i in range(degree + 1):         result = result.with_parameter(f\"c{i}\", initial_params[i])      result = (         result.with_optimiser(diffid.NelderMead().with_max_iter(2000))         .build()         .optimise()     )      results_multi[w_smooth] = result     print(         f\"Smoothness weight={w_smooth:5.1f}: SSE={np.sum((eval_poly(x_poly, result.x) - y_poly) ** 2):.3f}\"     ) <pre>Smoothness weight=  0.0: SSE=1.807\nSmoothness weight=  0.1: SSE=1.816\nSmoothness weight=  1.0: SSE=1.976\nSmoothness weight= 10.0: SSE=2.267\n</pre> In\u00a0[11]: Copied! <pre># Visualize effect of smoothness weight\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor idx, (w_smooth, result) in enumerate(results_multi.items()):\n    ax = axes[idx]\n\n    y_fit = eval_poly(x_dense, result.x)\n\n    ax.plot(x_poly, y_poly, \"o\", label=\"Data\", alpha=0.6, markersize=7, color=\"gray\")\n    ax.plot(x_dense, y_true_dense, \"k--\", linewidth=2, label=\"True\", alpha=0.7)\n    ax.plot(x_dense, y_fit, linewidth=2.5, label=f\"Fit (\u03bb={w_smooth})\", color=\"#2ca02c\")\n\n    ax.set_xlabel(\"x\", fontsize=11)\n    ax.set_ylabel(\"y\", fontsize=11)\n    ax.set_title(f\"Smoothness Weight \u03bb = {w_smooth}\", fontsize=13, fontweight=\"bold\")\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    ax.set_ylim(-4, 3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 Smoothness Penalty Effect:\")\nprint(\"   \u03bb=0.0:  No penalty \u2192 over-fitting\")\nprint(\"   \u03bb=0.1:  Slight smoothing\")\nprint(\"   \u03bb=1.0:  Balanced fit\")\nprint(\"   \u03bb=10.0: Too smooth \u2192 under-fitting\")\n</pre> # Visualize effect of smoothness weight fig, axes = plt.subplots(2, 2, figsize=(14, 10)) axes = axes.flatten()  for idx, (w_smooth, result) in enumerate(results_multi.items()):     ax = axes[idx]      y_fit = eval_poly(x_dense, result.x)      ax.plot(x_poly, y_poly, \"o\", label=\"Data\", alpha=0.6, markersize=7, color=\"gray\")     ax.plot(x_dense, y_true_dense, \"k--\", linewidth=2, label=\"True\", alpha=0.7)     ax.plot(x_dense, y_fit, linewidth=2.5, label=f\"Fit (\u03bb={w_smooth})\", color=\"#2ca02c\")      ax.set_xlabel(\"x\", fontsize=11)     ax.set_ylabel(\"y\", fontsize=11)     ax.set_title(f\"Smoothness Weight \u03bb = {w_smooth}\", fontsize=13, fontweight=\"bold\")     ax.legend(fontsize=10)     ax.grid(True, alpha=0.3)     ax.set_ylim(-4, 3)  plt.tight_layout() plt.show()  print(\"\\n\ud83d\udca1 Smoothness Penalty Effect:\") print(\"   \u03bb=0.0:  No penalty \u2192 over-fitting\") print(\"   \u03bb=0.1:  Slight smoothing\") print(\"   \u03bb=1.0:  Balanced fit\") print(\"   \u03bb=10.0: Too smooth \u2192 under-fitting\") <pre>\n\ud83d\udca1 Smoothness Penalty Effect:\n   \u03bb=0.0:  No penalty \u2192 over-fitting\n   \u03bb=0.1:  Slight smoothing\n   \u03bb=1.0:  Balanced fit\n   \u03bb=10.0: Too smooth \u2192 under-fitting\n</pre> In\u00a0[12]: Copied! <pre>class ConstrainedCost:\n    \"\"\"Cost with soft constraints via penalties.\"\"\"\n\n    def __init__(self, penalty_weight=1000.0):\n        self.penalty_weight = penalty_weight\n\n    def __call__(self, predicted, observed, params=None):\n        # Data fit term\n        sse = np.sum((observed - predicted) ** 2)\n\n        # Example constraint: parameters should sum to a target value\n        if params is not None:\n            # Soft constraint: sum(params) \u2248 0\n            constraint_violation = (np.sum(params) - 0.0) ** 2\n\n            # Positivity constraint: penalise negative parameters\n            negative_penalty = np.sum(np.minimum(0, params) ** 2)\n\n            penalty = self.penalty_weight * (constraint_violation + negative_penalty)\n            return sse + penalty\n\n        return sse\n\n\nprint(\"Example constraint penalties:\")\nprint(\"  \u2022 Sum constraint: forces \u03a3\u03b8\u1d62 \u2248 target\")\nprint(\"  \u2022 Positivity: penalises \u03b8\u1d62 &lt; 0\")\nprint(\"  \u2022 Bounds: penalises \u03b8\u1d62 outside [a, b]\")\nprint(\"  \u2022 Monotonicity: penalises \u03b8\u1d62\u208a\u2081 &lt; \u03b8\u1d62\")\nprint(\"\\n\ud83d\udca1 Tip: Start with large penalty weights, then tune.\")\n</pre> class ConstrainedCost:     \"\"\"Cost with soft constraints via penalties.\"\"\"      def __init__(self, penalty_weight=1000.0):         self.penalty_weight = penalty_weight      def __call__(self, predicted, observed, params=None):         # Data fit term         sse = np.sum((observed - predicted) ** 2)          # Example constraint: parameters should sum to a target value         if params is not None:             # Soft constraint: sum(params) \u2248 0             constraint_violation = (np.sum(params) - 0.0) ** 2              # Positivity constraint: penalise negative parameters             negative_penalty = np.sum(np.minimum(0, params) ** 2)              penalty = self.penalty_weight * (constraint_violation + negative_penalty)             return sse + penalty          return sse   print(\"Example constraint penalties:\") print(\"  \u2022 Sum constraint: forces \u03a3\u03b8\u1d62 \u2248 target\") print(\"  \u2022 Positivity: penalises \u03b8\u1d62 &lt; 0\") print(\"  \u2022 Bounds: penalises \u03b8\u1d62 outside [a, b]\") print(\"  \u2022 Monotonicity: penalises \u03b8\u1d62\u208a\u2081 &lt; \u03b8\u1d62\") print(\"\\n\ud83d\udca1 Tip: Start with large penalty weights, then tune.\") <pre>Example constraint penalties:\n  \u2022 Sum constraint: forces \u03a3\u03b8\u1d62 \u2248 target\n  \u2022 Positivity: penalises \u03b8\u1d62 &lt; 0\n  \u2022 Bounds: penalises \u03b8\u1d62 outside [a, b]\n  \u2022 Monotonicity: penalises \u03b8\u1d62\u208a\u2081 &lt; \u03b8\u1d62\n\n\ud83d\udca1 Tip: Start with large penalty weights, then tune.\n</pre>"},{"location":"tutorials/notebooks/advanced_cost_functions/#advanced-cost-functions","title":"Advanced Cost Functions\u00b6","text":"<p>Learning Objectives:</p> <ul> <li>Understand built-in cost metrics (SSE, RMSE, GaussianNLL)</li> <li>Implement custom cost functions</li> <li>Apply weighted fitting for heteroscedastic data</li> <li>Use regularisation to prevent over-fitting</li> <li>Combine multiple objectives</li> </ul> <p>Prerequisites: Optimisation Basics, ODE Fitting, basic statistics</p> <p>Runtime: ~15 minutes</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#introduction","title":"Introduction\u00b6","text":"<p>The cost function (or loss function) quantifies how well model predictions match observations. Diffid provides built-in metrics, but real-world problems often require:</p> <ul> <li>Weighted fitting when measurement errors vary</li> <li>Custom metrics for domain-specific requirements</li> <li>Regularisation to prevent over-fitting</li> <li>Multi-objective combinations</li> </ul> <p>This tutorial demonstrates advanced cost function techniques.</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#built-in-cost-metrics","title":"Built-in Cost Metrics\u00b6","text":"<p>Diffid provides three standard metrics:</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#1-sum-of-squared-errors-sse","title":"1. Sum of Squared Errors (SSE)\u00b6","text":"<p>$$ \\text{SSE} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$</p> <ul> <li>Default metric</li> <li>Penalises large errors heavily</li> <li>Assumes constant variance</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#2-root-mean-squared-error-rmse","title":"2. Root Mean Squared Error (RMSE)\u00b6","text":"<p>$$ \\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$</p> <ul> <li>Normalised by sample size</li> <li>Same units as data</li> <li>Better for comparing across datasets</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#3-gaussian-negative-log-likelihood-gaussiannll","title":"3. Gaussian Negative Log-Likelihood (GaussianNLL)\u00b6","text":"<p>$$ \\text{NLL} = \\frac{n}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$</p> <ul> <li>Statistically principled</li> <li>Enables Bayesian inference</li> <li>Can estimate noise parameter $\\sigma$</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#weighted-fitting-heteroscedastic-data","title":"Weighted Fitting: Heteroscedastic Data\u00b6","text":"<p>Real data often has non-uniform measurement errors (heteroscedasticity). Points with larger errors should contribute less to the cost function.</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#weighted-least-squares","title":"Weighted Least Squares\u00b6","text":"<p>$$\\text{WSSE} = \\sum_{i=1}^{n} w_i (y_i - \\hat{y}_i)^2$$</p> <p>where $w_i = 1/\\sigma_i^2$ (inverse variance weighting).</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#regularisation-preventing-over-fitting","title":"Regularisation: Preventing Over-fitting\u00b6","text":"<p>When fitting complex models with many parameters, regularisation prevents over-fitting by penalising large parameter values.</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#l2-regularisation-ridge","title":"L2 Regularisation (Ridge)\u00b6","text":"<p>$$\\text{Cost} = \\text{Data Term} + \\lambda \\sum_{i=1}^{p} \\theta_i^2$$</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#l1-regularisation-lasso","title":"L1 Regularisation (Lasso)\u00b6","text":"<p>$$\\text{Cost} = \\text{Data Term} + \\lambda \\sum_{i=1}^{p} |\\theta_i|$$</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#visualise-overfitting","title":"Visualise Overfitting\u00b6","text":"<p>To visualise model overfitting, an extrapolation region is plotted. This provides insight into whether the model accurately captures the system dynamics and not the underlying noise model.</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#multi-objective-cost-functions","title":"Multi-Objective Cost Functions\u00b6","text":"<p>Sometimes we want to balance multiple objectives simultaneously:</p> <p>$$\\text{Cost} = w_1 \\cdot \\text{Fit Quality} + w_2 \\cdot \\text{Smoothness} + w_3 \\cdot \\text{Physical Constraints}$$</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#physical-constraints-as-cost-penalties","title":"Physical Constraints as Cost Penalties\u00b6","text":"<p>In scientific applications, we often have physical constraints:</p> <ul> <li>Parameters must be positive (e.g., rate constants)</li> <li>Conservation laws (e.g., mass balance)</li> <li>Monotonicity requirements</li> </ul> <p>These can be enforced via penalty terms:</p>"},{"location":"tutorials/notebooks/advanced_cost_functions/#best-practices","title":"Best Practices\u00b6","text":""},{"location":"tutorials/notebooks/advanced_cost_functions/#choosing-a-cost-function","title":"Choosing a Cost Function\u00b6","text":"<ol> <li>Default (SSE): Good starting point for most problems</li> <li>RMSE: When comparing across datasets with different sizes</li> <li>GaussianNLL: For Bayesian inference and uncertainty quantification</li> <li>Weighted: When measurement errors vary (heteroscedastic data)</li> <li>Custom: For domain-specific requirements</li> </ol>"},{"location":"tutorials/notebooks/advanced_cost_functions/#regularisation-guidelines","title":"Regularisation Guidelines\u00b6","text":"<ul> <li>When to use: High-dimensional problems, limited data, polynomial fitting</li> <li>L2 (Ridge): Shrinks all coefficients smoothly</li> <li>L1 (Lasso): Promotes sparsity (some coefficients \u2192 0)</li> <li>Tuning \u03bb: Cross-validation or information criteria (AIC, BIC)</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#multi-objective-balancing","title":"Multi-Objective Balancing\u00b6","text":"<ul> <li>Start simple: Fit data first, add penalties incrementally</li> <li>Scale matters: Normalise objectives to similar magnitudes</li> <li>Weight tuning: Use logarithmic search (0.001, 0.01, 0.1, 1, 10, ...)</li> <li>Validation: Check if constraints are actually satisfied</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#summary-cost-function-design-pattern","title":"Summary: Cost Function Design Pattern\u00b6","text":"<pre>class CustomCost:\n    def __init__(self, **hyperparameters):\n        # Store configuration\n        pass\n    \n    def __call__(self, predicted, observed, params=None):\n        # 1. Data fidelity term\n        fit_cost = compute_fit(predicted, observed)\n        \n        # 2. Regularisation term (optional)\n        reg_cost = compute_regularisation(params)\n        \n        # 3. Physical constraints (optional)\n        constraint_cost = compute_penalties(params)\n        \n        # 4. Combine with weights\n        return w1*fit_cost + w2*reg_cost + w3*constraint_cost\n</pre>"},{"location":"tutorials/notebooks/advanced_cost_functions/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>Built-in metrics (SSE, RMSE, GaussianNLL) cover most use cases</li> <li>Weighted fitting essential for heteroscedastic data (varying errors)</li> <li>Regularisation prevents over-fitting in high-dimensional problems</li> <li>Multi-objective costs balance competing goals (fit vs smoothness)</li> <li>Constraint penalties enforce physical requirements</li> <li>Custom costs are easy to implement via <code>__call__</code> interface</li> <li>Scale and normalisation critical for multi-term objectives</li> </ol>"},{"location":"tutorials/notebooks/advanced_cost_functions/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Guide: Cost Metrics - Detailed metric selection</li> <li>Parameter Uncertainty - Bayesian inference with GaussianNLL</li> <li>API Reference: Cost Metrics - Complete API</li> </ul>"},{"location":"tutorials/notebooks/advanced_cost_functions/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Implement L1 Regularisation: Create a <code>LassoSSE</code> class and compare with L2</p> </li> <li><p>Cross-Validation: Implement k-fold cross-validation to tune regularisation strength</p> </li> <li><p>Robust Fitting: Implement Huber loss (robust to outliers): $$L_\\delta(r) = \\begin{cases} \\frac{1}{2}r^2 &amp; \\text{for } |r| \\leq \\delta \\\\ \\delta(|r| - \\frac{1}{2}\\delta) &amp; \\text{otherwise} \\end{cases}$$</p> </li> <li><p>Time-Series Smoothness: Add a penalty on parameter changes over time for dynamic fitting</p> </li> <li><p>Information Criteria: Compute AIC and BIC for model selection:</p> <ul> <li>AIC = $2k - 2\\ln(\\mathcal{L})$</li> <li>BIC = $\\ln(n)k - 2\\ln(\\mathcal{L})$</li> </ul> </li> </ol>"},{"location":"tutorials/notebooks/advanced_predator_prey/","title":"Multi-Backend ODE Solving","text":""},{"location":"tutorials/notebooks/advanced_predator_prey/#multi-backend-ode-solving","title":"Multi-Backend ODE Solving\u00b6","text":"<p>Learning Objectives:</p> <ul> <li>Use VectorBuilder for custom ODE solvers</li> <li>Compare Diffsol, Diffrax (JAX), and DifferentialEquations.jl</li> <li>Understand performance trade-offs</li> <li>Integrate external solvers with Diffid</li> </ul> <p>Prerequisites: Optimisation Basics, ODE Fitting, basic JAX or Julia knowledge (optional)</p> <p>Runtime: ~20 minutes</p>"},{"location":"tutorials/notebooks/advanced_predator_prey/#introduction","title":"Introduction\u00b6","text":"<p>Diffid's DiffsolBuilder provides high-performance ODE solving for most cases. But sometimes you need:</p> <ul> <li>JAX/Diffrax: Automatic differentiation, GPU acceleration</li> <li>Julia/DifferentialEquations.jl: Specialized solvers, stiff equations</li> <li>Custom simulators: Agent-based models, PDEs, hybrid systems</li> </ul> <p>VectorBuilder lets you integrate any Python-callable forward model with Diffid's optimisers.</p>"},{"location":"tutorials/notebooks/advanced_predator_prey/#the-lotka-volterra-model","title":"The Lotka-Volterra Model\u00b6","text":"<p>The predator-prey equations:</p> <p>$$\\begin{aligned} \\frac{dx}{dt} &amp;= \\alpha x - \\beta x y \\\\ \\frac{dy}{dt} &amp;= \\delta x y - \\gamma y \\end{aligned}$$</p> <p>where:</p> <ul> <li>$x$ is prey population</li> <li>$y$ is predator population</li> <li>$\\alpha, \\beta, \\gamma, \\delta$ are interaction rates</li> </ul> <p>This tutorial demonstrates parameter fitting with three different solver backends.</p>"},{"location":"tutorials/notebooks/advanced_predator_prey/#coming-soon","title":"Coming Soon\u00b6","text":"<p>This advanced tutorial is under development. It will cover:</p>"},{"location":"tutorials/notebooks/advanced_predator_prey/#backend-1-diffsol-built-in","title":"Backend 1: Diffsol (Built-in)\u00b6","text":"<pre>builder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(lotka_volterra_dsl)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n    # ... more parameters\n)\n</pre>"},{"location":"tutorials/notebooks/advanced_predator_prey/#backend-2-jaxdiffrax","title":"Backend 2: JAX/Diffrax\u00b6","text":"<pre>import jax\nfrom diffrax import diffeqsolve, ODETerm, Tsit5\n\ndef diffrax_solver(params):\n    # Your Diffrax integration\n    return predictions\n\nbuilder = (\n    diffid.VectorBuilder()\n    .with_objective(diffrax_solver)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n)\n</pre>"},{"location":"tutorials/notebooks/advanced_predator_prey/#backend-3-juliadifferentialequationsjl","title":"Backend 3: Julia/DifferentialEquations.jl\u00b6","text":"<pre>from diffeqpy import de\n\ndef diffeqpy_solver(params):\n    # Your Julia integration\n    return predictions\n\nbuilder = (\n    diffid.VectorBuilder()\n    .with_objective(diffeqpy_solver)\n    .with_data(data)\n    .with_parameter(\"alpha\", 1.0)\n)\n</pre>"},{"location":"tutorials/notebooks/advanced_predator_prey/#performance-comparison","title":"Performance Comparison\u00b6","text":"<p>The tutorial will benchmark all three backends:</p> <ul> <li>Accuracy: Parameter recovery quality</li> <li>Speed: Time per function evaluation</li> <li>Ease of use: Setup complexity</li> <li>Special features: Gradients, GPU, stiff solvers</li> </ul>"},{"location":"tutorials/notebooks/advanced_predator_prey/#when-to-use-each-backend","title":"When to Use Each Backend\u00b6","text":"Backend Best For Diffsol General purpose, fast, built-in JAX/Diffrax Gradients, GPU, neural ODEs Julia/DiffEq Stiff systems, specialized solvers, DAEs Custom Non-ODE models, complex physics"},{"location":"tutorials/notebooks/advanced_predator_prey/#example-data","title":"Example Data\u00b6","text":"<p>The predator-prey examples directory contains:</p> <ul> <li><code>generate_data_diffrax.py</code>: Creates synthetic data</li> <li><code>predator_prey_diffsol.py</code>: Diffsol backend</li> <li><code>predator_prey_diffrax.py</code>: JAX/Diffrax backend</li> <li><code>predator_prey_diffeqpy.py</code>: Julia backend</li> </ul> <p>Run these scripts directly to see the backends in action!</p>"},{"location":"tutorials/notebooks/advanced_predator_prey/#installation","title":"Installation\u00b6","text":"<p>For JAX/Diffrax:</p> <pre>pip install jax diffrax\n</pre> <p>For Julia/DifferentialEquations.jl:</p> <pre>pip install diffeqpy\npython -c \"from diffeqpy import de; de.install()\"\n</pre>"},{"location":"tutorials/notebooks/advanced_predator_prey/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>VectorBuilder integrates any Python callable</li> <li>Diffsol is the default - fast and easy</li> <li>JAX/Diffrax for gradients and GPU</li> <li>Julia/DiffEq for specialized solvers</li> <li>All backends work with Diffid's optimisers</li> </ol>"},{"location":"tutorials/notebooks/advanced_predator_prey/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Custom Solvers Guide - Detailed integration guide</li> <li>VectorBuilder API</li> <li>Examples Gallery - More backend examples</li> </ul>"},{"location":"tutorials/notebooks/model_comparison/","title":"Model Comparison","text":""},{"location":"tutorials/notebooks/model_comparison/#model-comparison","title":"Model Comparison\u00b6","text":"<p>Learning Objectives:</p> <ul> <li>Use Dynamic Nested Sampling for model evidence</li> <li>Calculate Bayes factors for model comparison</li> <li>Interpret evidence values</li> <li>Compare bicycle dynamics models</li> </ul> <p>Prerequisites: Optimisation Basics, ODE Fitting, Parameter Uncertainty, Bayesian model comparison basics</p> <p>Runtime: ~20 minutes</p> <p>Note: This tutorial requires the Dynamic Nested Sampling feature. Coming soon!</p>"},{"location":"tutorials/notebooks/model_comparison/#introduction","title":"Introduction\u00b6","text":"<p>When you have multiple models, which one is best?</p> <p>Dynamic Nested Sampling calculates the model evidence (marginal likelihood), allowing rigorous Bayesian model comparison via Bayes factors.</p> <p>$$\\text{Bayes Factor} = \\frac{p(\\text{Data}|\\text{Model}_1)}{p(\\text{Data}|\\text{Model}_2)} = \\frac{Z_1}{Z_2}$$</p> <p>This tutorial will demonstrate model comparison using bicycle dynamics models.</p>"},{"location":"tutorials/notebooks/model_comparison/#the-problem-bicycle-model","title":"The Problem: Bicycle Model\u00b6","text":"<p>A bicycle traveling at constant velocity $v$ with steer angle $\\delta$ follows curved motion determined by wheelbase $L$:</p> <p>$$\\begin{aligned} \\frac{dx}{dt} &amp;= v \\cos(\\theta) \\\\ \\frac{dy}{dt} &amp;= v \\sin(\\theta) \\\\ \\frac{d\\theta}{dt} &amp;= \\frac{v}{L} \\tan(\\delta) \\end{aligned}$$</p> <p>Task: Estimate wheelbase $L$ and compare different model formulations.</p>"},{"location":"tutorials/notebooks/model_comparison/#coming-soon","title":"Coming Soon\u00b6","text":"<p>This tutorial is under development. Check back soon for:</p> <ul> <li>Setting up multiple model variants</li> <li>Running Dynamic Nested Sampling</li> <li>Calculating log evidence for each model</li> <li>Interpreting Bayes factors</li> <li>Making model selection decisions</li> </ul>"},{"location":"tutorials/notebooks/model_comparison/#preview-model-evidence-interpretation","title":"Preview: Model Evidence Interpretation\u00b6","text":"$\\log(Z_1 / Z_2)$ Bayes Factor Evidence for Model 1 &lt; 0 &lt; 1 Negative (prefer Model 2) 0-1 1-3 Barely worth mentioning 1-2.5 3-12 Positive 2.5-5 12-150 Strong &gt; 5 &gt; 150 Very strong"},{"location":"tutorials/notebooks/model_comparison/#references","title":"References\u00b6","text":"<ul> <li>Skilling, J. (2006). Nested sampling for general Bayesian computation.</li> <li>Higson, E. et al. (2019). Dynamic nested sampling.</li> </ul>"},{"location":"tutorials/notebooks/model_comparison/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Predator-Prey Models - Multi-backend comparison</li> <li>Choosing a Sampler</li> <li>API Reference: Samplers</li> </ul>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/","title":"ODE Fitting with DiffSL","text":"In\u00a0[\u00a0]: Copied! <pre># Import plotting utilities\nimport diffid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom diffid.plotting import ode_fit\n</pre> # Import plotting utilities import diffid import matplotlib.pyplot as plt import numpy as np from diffid.plotting import ode_fit In\u00a0[2]: Copied! <pre>dsl_model = \"\"\"\nin_i {r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\"\"\"\n\nprint(\"DiffSL Model Definition:\")\nprint(\"=\" * 50)\nprint(dsl_model)\nprint(\"\\nExplanation:\")\nprint(\"  in_i {r = 1, k = 1 }         \u2192 Parameters to fit\")\nprint(\"  u_i { y = 0.1 }     \u2192 Initial condition: y(0) = 0.1\")\nprint(\"  F_i { ... }         \u2192 Right-hand side: dy/dt = ...\")\n</pre> dsl_model = \"\"\" in_i {r = 1, k = 1 } u_i { y = 0.1 } F_i { (r * y) * (1 - (y / k)) } \"\"\"  print(\"DiffSL Model Definition:\") print(\"=\" * 50) print(dsl_model) print(\"\\nExplanation:\") print(\"  in_i {r = 1, k = 1 }         \u2192 Parameters to fit\") print(\"  u_i { y = 0.1 }     \u2192 Initial condition: y(0) = 0.1\") print(\"  F_i { ... }         \u2192 Right-hand side: dy/dt = ...\") <pre>DiffSL Model Definition:\n==================================================\n\nin_i {r = 1, k = 1 }\nu_i { y = 0.1 }\nF_i { (r * y) * (1 - (y / k)) }\n\n\nExplanation:\n  in_i {r = 1, k = 1 }        \u2192 Parameters to fit\n  u_i { y = 0.1 }     \u2192 Initial condition: y(0) = 0.1\n  F_i { ... }         \u2192 Right-hand side: dy/dt = ...\n</pre> In\u00a0[\u00a0]: Copied! <pre># Time points\nt_span = np.linspace(0, 4, 100)\n\n# True logistic growth solution with known parameters\n# y(t) = y0 * exp(r*t) / (1 + y0 * (exp(r*t) - 1) / k)\ny0 = 0.1\nr_true = 1.0\nk_true = 1.0\n\n# Generate clean data\nexp_rt = np.exp(r_true * t_span)\ny_data = y0 * exp_rt / (1 + y0 * (exp_rt - 1) / k_true)\n\n# Add some noise\nnp.random.seed(42)\nnoise_level = 0.01\ny_noisy = y_data + np.random.normal(0, noise_level, size=y_data.shape)\n\n# Diffid expects data as [time, observation] columns\ndata = np.column_stack((t_span, y_noisy))\n\nprint(f\"Generated {len(t_span)} data points\")\nprint(f\"Time range: [{t_span[0]:.2f}, {t_span[-1]:.2f}]\")\nprint(f\"Value range: [{y_noisy.min():.3f}, {y_noisy.max():.3f}]\")\nprint(f\"Noise level: {noise_level}\")\nprint(f\"\\nTrue parameters: r = {r_true}, k = {k_true}\")\n</pre> # Time points t_span = np.linspace(0, 4, 100)  # True logistic growth solution with known parameters # y(t) = y0 * exp(r*t) / (1 + y0 * (exp(r*t) - 1) / k) y0 = 0.1 r_true = 1.0 k_true = 1.0  # Generate clean data exp_rt = np.exp(r_true * t_span) y_data = y0 * exp_rt / (1 + y0 * (exp_rt - 1) / k_true)  # Add some noise np.random.seed(42) noise_level = 0.01 y_noisy = y_data + np.random.normal(0, noise_level, size=y_data.shape)  # Diffid expects data as [time, observation] columns data = np.column_stack((t_span, y_noisy))  print(f\"Generated {len(t_span)} data points\") print(f\"Time range: [{t_span[0]:.2f}, {t_span[-1]:.2f}]\") print(f\"Value range: [{y_noisy.min():.3f}, {y_noisy.max():.3f}]\") print(f\"Noise level: {noise_level}\") print(f\"\\nTrue parameters: r = {r_true}, k = {k_true}\") In\u00a0[4]: Copied! <pre>fig, ax = ode_fit(\n    t_span,\n    y_noisy,\n    t_span,\n    y_data,\n    title=\"Synthetic Logistic Growth Data\",\n    xlabel=\"Time\",\n    ylabel=\"Population\",\n    show=False,\n)\n\n# Update legend\nax.legend([\"Noisy observations\", \"True solution\"])\nplt.show()\n</pre> fig, ax = ode_fit(     t_span,     y_noisy,     t_span,     y_data,     title=\"Synthetic Logistic Growth Data\",     xlabel=\"Time\",     ylabel=\"Population\",     show=False, )  # Update legend ax.legend([\"Noisy observations\", \"True solution\"]) plt.show() In\u00a0[\u00a0]: Copied! <pre># Create builder\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl_model)\n    .with_data(data)\n    .with_tolerances(1e-6, 1e-8)  # Relative and absolute tolerances\n    .with_parameter(\"r\", 10.0)  # Initial guess (deliberately wrong)\n    .with_parameter(\"k\", 10.0)  # Initial guess (deliberately wrong)\n    .with_parallel(True)  # Enable parallel evaluation\n)\n\nproblem = builder.build()\n\nprint(\"Problem built successfully!\")\nprint(\"\\nInitial parameter guesses: r = 50.0, k = 50.0\")\nprint(f\"(Far from true values: r = {r_true}, k = {k_true})\")\n</pre> # Create builder builder = (     diffid.DiffsolBuilder()     .with_diffsl(dsl_model)     .with_data(data)     .with_tolerances(1e-6, 1e-8)  # Relative and absolute tolerances     .with_parameter(\"r\", 10.0)  # Initial guess (deliberately wrong)     .with_parameter(\"k\", 10.0)  # Initial guess (deliberately wrong)     .with_parallel(True)  # Enable parallel evaluation )  problem = builder.build()  print(\"Problem built successfully!\") print(\"\\nInitial parameter guesses: r = 50.0, k = 50.0\") print(f\"(Far from true values: r = {r_true}, k = {k_true})\") In\u00a0[\u00a0]: Copied! <pre># Create optimiser\noptimiser = diffid.CMAES().with_max_iter(1000).with_threshold(1e-12)\n\n# Run optimisation\nresult = optimiser.run(problem, problem.initial_values())\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OPTIMISATION RESULTS\")\nprint(\"=\" * 60)\nprint(f\"Success: {result.success}\")\nprint(\"\\nFitted parameters:\")\nprint(f\"  r = {result.x[0]:.6f}  (true: {r_true})\")\nprint(f\"  k = {result.x[1]:.6f}  (true: {k_true})\")\nprint(\"\\nOptimisation details:\")\nprint(f\"  Final cost: {result.value:.3e}\")\nprint(f\"  Iterations: {result.iterations}\")\nprint(f\"  Function evaluations: {result.evaluations}\")\nprint(f\"  Time: {result.time.microseconds / 1e3:.3f} milliseconds\")\nprint(f\"  Message: {result.message}\")\n\n# Calculate parameter errors\nr_error = abs(result.x[0] - r_true) / r_true * 100\nk_error = abs(result.x[1] - k_true) / k_true * 100\nprint(\"\\nParameter errors:\")\nprint(f\"  r: {r_error:.3f}%\")\nprint(f\"  k: {k_error:.3f}%\")\n</pre> # Create optimiser optimiser = diffid.CMAES().with_max_iter(1000).with_threshold(1e-12)  # Run optimisation result = optimiser.run(problem, problem.initial_values())  print(\"\\n\" + \"=\" * 60) print(\"OPTIMISATION RESULTS\") print(\"=\" * 60) print(f\"Success: {result.success}\") print(\"\\nFitted parameters:\") print(f\"  r = {result.x[0]:.6f}  (true: {r_true})\") print(f\"  k = {result.x[1]:.6f}  (true: {k_true})\") print(\"\\nOptimisation details:\") print(f\"  Final cost: {result.value:.3e}\") print(f\"  Iterations: {result.iterations}\") print(f\"  Function evaluations: {result.evaluations}\") print(f\"  Time: {result.time.microseconds / 1e3:.3f} milliseconds\") print(f\"  Message: {result.message}\")  # Calculate parameter errors r_error = abs(result.x[0] - r_true) / r_true * 100 k_error = abs(result.x[1] - k_true) / k_true * 100 print(\"\\nParameter errors:\") print(f\"  r: {r_error:.3f}%\") print(f\"  k: {k_error:.3f}%\") In\u00a0[7]: Copied! <pre># Generate predictions with fitted parameters\nr_fit, k_fit = result.x\nexp_rt_fit = np.exp(r_fit * t_span)\ny_fit = y0 * exp_rt_fit / (1 + y0 * (exp_rt_fit - 1) / k_fit)\n\n# Plot\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\n# Top: Data and fits\nax1 = axes[0]\nax1.plot(t_span, y_noisy, \"o\", label=\"Noisy data\", alpha=0.6, markersize=6)\nax1.plot(t_span, y_fit, \"-\", label=\"Fitted model\", linewidth=2)\nax1.plot(t_span, y_data, \"--\", label=\"True solution\", linewidth=2, alpha=0.7)\nax1.set_xlabel(\"Time\")\nax1.set_ylabel(\"Population\")\nax1.set_title(\"Logistic Growth Model Fit\")\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Bottom: Residuals\nax2 = axes[1]\nresiduals = y_noisy - y_fit\nax2.plot(t_span, residuals, \"o-\", alpha=0.6)\nax2.axhline(0, color=\"r\", linestyle=\"--\", linewidth=2, alpha=0.7)\nax2.set_xlabel(\"Time\")\nax2.set_ylabel(\"Residuals\")\nax2.set_title(\"Fitting Residuals\")\nax2.grid(True, alpha=0.3)\n\n# Add statistics\nrmse = np.sqrt(np.mean(residuals**2))\nax2.text(\n    0.02,\n    0.98,\n    f\"RMSE = {rmse:.4f}\",\n    transform=ax2.transAxes,\n    verticalalignment=\"top\",\n    bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFit quality:\")\nprint(f\"  RMSE: {rmse:.6f}\")\nprint(f\"  Max residual: {np.abs(residuals).max():.6f}\")\nprint(f\"  Mean residual: {np.mean(residuals):.6f}\")\n</pre> # Generate predictions with fitted parameters r_fit, k_fit = result.x exp_rt_fit = np.exp(r_fit * t_span) y_fit = y0 * exp_rt_fit / (1 + y0 * (exp_rt_fit - 1) / k_fit)  # Plot fig, axes = plt.subplots(2, 1, figsize=(10, 10))  # Top: Data and fits ax1 = axes[0] ax1.plot(t_span, y_noisy, \"o\", label=\"Noisy data\", alpha=0.6, markersize=6) ax1.plot(t_span, y_fit, \"-\", label=\"Fitted model\", linewidth=2) ax1.plot(t_span, y_data, \"--\", label=\"True solution\", linewidth=2, alpha=0.7) ax1.set_xlabel(\"Time\") ax1.set_ylabel(\"Population\") ax1.set_title(\"Logistic Growth Model Fit\") ax1.legend() ax1.grid(True, alpha=0.3)  # Bottom: Residuals ax2 = axes[1] residuals = y_noisy - y_fit ax2.plot(t_span, residuals, \"o-\", alpha=0.6) ax2.axhline(0, color=\"r\", linestyle=\"--\", linewidth=2, alpha=0.7) ax2.set_xlabel(\"Time\") ax2.set_ylabel(\"Residuals\") ax2.set_title(\"Fitting Residuals\") ax2.grid(True, alpha=0.3)  # Add statistics rmse = np.sqrt(np.mean(residuals**2)) ax2.text(     0.02,     0.98,     f\"RMSE = {rmse:.4f}\",     transform=ax2.transAxes,     verticalalignment=\"top\",     bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5), )  plt.tight_layout() plt.show()  print(\"\\nFit quality:\") print(f\"  RMSE: {rmse:.6f}\") print(f\"  Max residual: {np.abs(residuals).max():.6f}\") print(f\"  Mean residual: {np.mean(residuals):.6f}\") <pre>\nFit quality:\n  RMSE: 0.009066\n  Max residual: 0.025379\n  Mean residual: -0.000468\n</pre> In\u00a0[8]: Copied! <pre># Try several initial guesses\ninitial_guesses = [\n    [0.5, 0.5],\n    [2.0, 2.0],\n    [10.0, 10.0],\n    [100.0, 100.0],\n]\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"INITIAL GUESS SENSITIVITY\")\nprint(\"=\" * 70)\nprint(f\"{'Initial [r, k]':&lt;20} {'Final [r, k]':&lt;30} {'Iterations':&lt;12} {'Success'}\")\nprint(\"-\" * 70)\n\nfor guess in initial_guesses:\n    result = optimiser.run(problem, guess)\n    final_str = f\"[{result.x[0]:.4f}, {result.x[1]:.4f}]\"\n    guess_str = f\"{guess}\"\n    print(f\"{guess_str:&lt;20} {final_str:&lt;30} {result.iterations:&lt;12} {result.success}\")\n\nprint(f\"\\nTrue values: [r, k] = [{r_true}, {k_true}]\")\n</pre> # Try several initial guesses initial_guesses = [     [0.5, 0.5],     [2.0, 2.0],     [10.0, 10.0],     [100.0, 100.0], ]  print(\"\\n\" + \"=\" * 70) print(\"INITIAL GUESS SENSITIVITY\") print(\"=\" * 70) print(f\"{'Initial [r, k]':&lt;20} {'Final [r, k]':&lt;30} {'Iterations':&lt;12} {'Success'}\") print(\"-\" * 70)  for guess in initial_guesses:     result = optimiser.run(problem, guess)     final_str = f\"[{result.x[0]:.4f}, {result.x[1]:.4f}]\"     guess_str = f\"{guess}\"     print(f\"{guess_str:&lt;20} {final_str:&lt;30} {result.iterations:&lt;12} {result.success}\")  print(f\"\\nTrue values: [r, k] = [{r_true}, {k_true}]\") <pre>\n======================================================================\nINITIAL GUESS SENSITIVITY\n======================================================================\nInitial [r, k]       Final [r, k]                   Iterations   Success\n----------------------------------------------------------------------\n[0.5, 0.5]           [0.9967, 1.0029]               73           True\n[2.0, 2.0]           [0.9967, 1.0029]               77           True\n[10.0, 10.0]         [0.9967, 1.0029]               183          True\n[100.0, 100.0]       [0.5947, 23.7961]              410          True\n\nTrue values: [r, k] = [1.0, 1.0]\n</pre> In\u00a0[\u00a0]: Copied! <pre>optimisers = {\n    \"Nelder-Mead\": diffid.NelderMead().with_max_iter(1000),\n    \"CMA-ES\": diffid.CMAES().with_max_iter(500),\n    \"Adam\": diffid.Adam().with_max_iter(1000).with_step_size(0.01),\n}\n\ninitial = [2.0, 2.0]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"OPTIMISER COMPARISON\")\nprint(\"=\" * 80)\nprint(\n    f\"{'Algorithm':&lt;15} {'r (fitted)':&lt;15} {'k (fitted)':&lt;15} {'Cost':&lt;15} {'Iters':&lt;8} {'Time (s)'}\"\n)\nprint(\"-\" * 80)\n\nfor name, opt in optimisers.items():\n    result = opt.run(problem, initial)\n    print(\n        f\"{name:&lt;15} {result.x[0]:&lt;15.6f} {result.x[1]:&lt;15.6f} \"\n        f\"{result.value:&lt;15.3e} {result.iterations:&lt;8} {result.time.microseconds / 1e6:.3f}\"\n    )\n\nprint(f\"\\nTrue values:    {r_true:&lt;15.6f} {k_true:&lt;15.6f}\")\n</pre> optimisers = {     \"Nelder-Mead\": diffid.NelderMead().with_max_iter(1000),     \"CMA-ES\": diffid.CMAES().with_max_iter(500),     \"Adam\": diffid.Adam().with_max_iter(1000).with_step_size(0.01), }  initial = [2.0, 2.0]  print(\"\\n\" + \"=\" * 80) print(\"OPTIMISER COMPARISON\") print(\"=\" * 80) print(     f\"{'Algorithm':&lt;15} {'r (fitted)':&lt;15} {'k (fitted)':&lt;15} {'Cost':&lt;15} {'Iters':&lt;8} {'Time (s)'}\" ) print(\"-\" * 80)  for name, opt in optimisers.items():     result = opt.run(problem, initial)     print(         f\"{name:&lt;15} {result.x[0]:&lt;15.6f} {result.x[1]:&lt;15.6f} \"         f\"{result.value:&lt;15.3e} {result.iterations:&lt;8} {result.time.microseconds / 1e6:.3f}\"     )  print(f\"\\nTrue values:    {r_true:&lt;15.6f} {k_true:&lt;15.6f}\") In\u00a0[10]: Copied! <pre># Example 1: Multiple state variables (Lotka-Volterra)\npredator_prey = \"\"\"\nin_i { alpha = 1, beta = 0.1, gamma = 1.5, delta = 0.075 }\nu_i {\n    prey = 10.0\n    predator = 5.0\n}\nF_i {\n    alpha * prey - beta * prey * predator,\n    delta * prey * predator - gamma * predator\n}\nout_i { prey, predator }\n\"\"\"\n\nprint(\"Example: Predator-Prey Model (Lotka-Volterra)\")\nprint(\"=\" * 50)\nprint(predator_prey)\n\n# Example 2: With algebraic equations\nwith_algebra = \"\"\"\nin_i { k1 = 0.5, k2 = 0.3 }\nu_i { A = 1.0 }\nF_i {\n    -k1 * A,\n    k1 * A - k2 * B\n}\nout_i { A, B }\n\"\"\"\n\nprint(\"\\nExample: Sequential Reactions (A \u2192 B \u2192 C)\")\nprint(\"=\" * 50)\nprint(with_algebra)\n</pre> # Example 1: Multiple state variables (Lotka-Volterra) predator_prey = \"\"\" in_i { alpha = 1, beta = 0.1, gamma = 1.5, delta = 0.075 } u_i {     prey = 10.0     predator = 5.0 } F_i {     alpha * prey - beta * prey * predator,     delta * prey * predator - gamma * predator } out_i { prey, predator } \"\"\"  print(\"Example: Predator-Prey Model (Lotka-Volterra)\") print(\"=\" * 50) print(predator_prey)  # Example 2: With algebraic equations with_algebra = \"\"\" in_i { k1 = 0.5, k2 = 0.3 } u_i { A = 1.0 } F_i {     -k1 * A,     k1 * A - k2 * B } out_i { A, B } \"\"\"  print(\"\\nExample: Sequential Reactions (A \u2192 B \u2192 C)\") print(\"=\" * 50) print(with_algebra) <pre>Example: Predator-Prey Model (Lotka-Volterra)\n==================================================\n\nin_i { alpha = 1, beta = 0.1, gamma = 1.5, delta = 0.075 }\nu_i {\n    prey = 10.0\n    predator = 5.0\n}\nF_i {\n    alpha * prey - beta * prey * predator,\n    delta * prey * predator - gamma * predator\n}\nout_i { prey, predator }\n\n\nExample: Sequential Reactions (A \u2192 B \u2192 C)\n==================================================\n\nin_i { k1 = 0.5, k2 = 0.3 }\nu_i { A = 1.0 }\nF_i {\n    -k1 * A,\n    k1 * A - k2 * B\n}\nout_i { A, B }\n\n</pre>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#ode-fitting-with-diffsl","title":"ODE Fitting with DiffSL\u00b6","text":"<p>Objectives:</p> <ul> <li>Understand the DiffsolBuilder API</li> <li>Learn DiffSL syntax for defining ODEs</li> <li>Fit parameters of a logistic growth model</li> <li>Visualise model fits and residuals</li> </ul>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#introduction","title":"Introduction\u00b6","text":"<p>The logistic growth model describes population dynamics with limited resources:</p> <p>$$\\frac{dy}{dt} = r \\cdot y \\cdot \\left(1 - \\frac{y}{k}\\right)$$</p> <p>where:</p> <ul> <li>$y$ is the population</li> <li>$r$ is the growth rate</li> <li>$k$ is the carrying capacity (maximum population)</li> </ul> <p>This tutorial demonstrates parameter fitting using experimental or simulated data.</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#define-the-ode-in-diffsl","title":"Define the ODE in DiffSL\u00b6","text":"<p>DiffSL (Differential Specification Language) is a concise syntax for ODEs:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":"<p>For this tutorial, we'll create synthetic data from the logistic function:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#visualise-the-data","title":"Visualise the Data\u00b6","text":""},{"location":"tutorials/notebooks/ode_fitting_diffsol/#build-the-optimisation-problem","title":"Build the Optimisation Problem\u00b6","text":"<p>Use <code>DiffsolBuilder</code> for ODE parameter fitting:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#optimise-with-cma-es","title":"Optimise with CMA-ES\u00b6","text":"<p>CMA-ES works well for ODE parameter fitting:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#visualise-the-fit","title":"Visualise the Fit\u00b6","text":"<p>Let's plot the fitted model against the data:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#comparing-different-initial-guesses","title":"Comparing Different Initial Guesses\u00b6","text":"<p>Let's see how different starting points affect convergence:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#comparing-optimisers","title":"Comparing Optimisers\u00b6","text":"<p>Let's compare different optimization algorithms:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#understanding-diffsl-syntax","title":"Understanding DiffSL Syntax\u00b6","text":"<p>Let's explore more complex DiffSL examples:</p>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>DiffsolBuilder is used for ODE parameter fitting</li> <li>DiffSL provides concise ODE syntax: <code>in</code>, <code>u_i</code>, <code>F_i</code></li> <li>Data format: <code>[time, observation(s)]</code> in columns</li> <li>CMA-ES typically works well for ODE fitting</li> <li>Parallel evaluation improves performance</li> <li>Always check residuals to assess fit quality</li> </ol>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Parameter Uncertainty - Quantify parameter uncertainty with MCMC</li> <li>DiffSL Backend Guide - Dense vs sparse solvers</li> <li>Custom Solvers - Using JAX/Diffrax or Julia</li> </ul>"},{"location":"tutorials/notebooks/ode_fitting_diffsol/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Add Noise: Increase <code>noise_level</code> to 0.05 and see how it affects fitting</p> </li> <li><p>Different Model: Implement exponential decay: $\\frac{dy}{dt} = -k \\cdot y$</p> </li> <li><p>Multiple Variables: Try fitting the predator-prey model with synthetic data</p> </li> <li><p>Sparse Data: Reduce the number of data points (e.g., 20 instead of 100) and observe the impact</p> </li> </ol>"},{"location":"tutorials/notebooks/optimisation_basics/","title":"Optimisation Basics","text":"In\u00a0[\u00a0]: Copied! <pre># Import plotting utilities\nimport diffid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom diffid.plotting import contour_2d\n</pre> # Import plotting utilities import diffid import matplotlib.pyplot as plt import numpy as np from diffid.plotting import contour_2d In\u00a0[2]: Copied! <pre>def rosenbrock(x):\n    \"\"\"The Rosenbrock function.\"\"\"\n    value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2\n    return np.array([value], dtype=float)\n</pre> def rosenbrock(x):     \"\"\"The Rosenbrock function.\"\"\"     value = (1 - x[0]) ** 2 + 100 * (x[1] - x[0] ** 2) ** 2     return np.array([value], dtype=float) In\u00a0[\u00a0]: Copied! <pre>builder = (\n    diffid.ScalarBuilder()\n    .with_objective(rosenbrock)\n    .with_parameter(\"x\", 10.0)  # Initial guess\n    .with_parameter(\"y\", 10.0)  # Initial guess\n)\nproblem = builder.build()\n\nprint(\"Problem built successfully!\")\nprint(\"Number of parameters: 2\")\n</pre> builder = (     diffid.ScalarBuilder()     .with_objective(rosenbrock)     .with_parameter(\"x\", 10.0)  # Initial guess     .with_parameter(\"y\", 10.0)  # Initial guess ) problem = builder.build()  print(\"Problem built successfully!\") print(\"Number of parameters: 2\") In\u00a0[4]: Copied! <pre>result = problem.optimise()  # Uses initial guesses provided\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"OPTIMISATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Success: {result.success}\")\nprint(f\"Optimal parameters: {result.x}\")\nprint(f\"Objective value: {result.value:.3e}\")\nprint(f\"Iterations: {result.iterations}\")\nprint(f\"Function evaluations: {result.evaluations}\")\nprint(f\"Message: {result.message}\")\n</pre> result = problem.optimise()  # Uses initial guesses provided  print(\"\\n\" + \"=\" * 50) print(\"OPTIMISATION RESULTS\") print(\"=\" * 50) print(f\"Success: {result.success}\") print(f\"Optimal parameters: {result.x}\") print(f\"Objective value: {result.value:.3e}\") print(f\"Iterations: {result.iterations}\") print(f\"Function evaluations: {result.evaluations}\") print(f\"Message: {result.message}\") <pre>\n==================================================\nOPTIMISATION RESULTS\n==================================================\nSuccess: True\nOptimal parameters: [1.00082688 1.0017059 ]\nObjective value: 9.484e-07\nIterations: 159\nFunction evaluations: 342\nMessage: Function tolerance met\n</pre> In\u00a0[5]: Copied! <pre># Create contour plot\nfig, ax = contour_2d(\n    rosenbrock,\n    xlim=(-2, 2),\n    ylim=(-1, 3),\n    levels=np.logspace(-1, 3.5, 20),\n    optimum=(1.0, 1.0),\n    found=(result.x[0], result.x[1]),\n    title=\"Rosenbrock Function Landscape\",\n    show=False,\n)\n\nplt.show()\n\nprint(f\"Distance from true optimum: {np.linalg.norm(result.x - [1.0, 1.0]):.3e}\")\n</pre> # Create contour plot fig, ax = contour_2d(     rosenbrock,     xlim=(-2, 2),     ylim=(-1, 3),     levels=np.logspace(-1, 3.5, 20),     optimum=(1.0, 1.0),     found=(result.x[0], result.x[1]),     title=\"Rosenbrock Function Landscape\",     show=False, )  plt.show()  print(f\"Distance from true optimum: {np.linalg.norm(result.x - [1.0, 1.0]):.3e}\") <pre>Distance from true optimum: 1.896e-03\n</pre> In\u00a0[\u00a0]: Copied! <pre># Define optimisers\noptimisers = {\n    \"Nelder-Mead\": diffid.NelderMead().with_max_iter(1000),\n    \"CMA-ES\": diffid.CMAES().with_max_iter(300).with_step_size(0.5),\n    \"Adam\": diffid.Adam()\n    .with_max_iter(2000)\n    .with_step_size(0.25)\n    .with_threshold(1e-12),\n}\n\n# Test starting point\ninitial_guess = [-1.0, -3.0]\n\n# Run all optimisers\nresults = {}\nfor name, optimiser in optimisers.items():\n    result = optimiser.run(problem, initial_guess)\n    results[name] = result\n\n# Display comparison\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OPTIMISER COMPARISON\")\nprint(\"=\" * 70)\nprint(\n    f\"{'Optimiser':&lt;15} {'Success':&lt;10} {'Final Value':&lt;15} {'Iterations':&lt;12} {'Evaluations'}\"\n)\nprint(\"-\" * 70)\n\nfor name, result in results.items():\n    print(\n        f\"{name:&lt;15} {str(result.success):&lt;10} {result.value:&lt;15.3e} \"\n        f\"{result.iterations:&lt;12} {result.evaluations}\"\n    )\n\nprint(\"\\nFinal parameters:\")\nfor name, result in results.items():\n    print(f\"{name:&lt;15} x = {result.x}\")\n</pre> # Define optimisers optimisers = {     \"Nelder-Mead\": diffid.NelderMead().with_max_iter(1000),     \"CMA-ES\": diffid.CMAES().with_max_iter(300).with_step_size(0.5),     \"Adam\": diffid.Adam()     .with_max_iter(2000)     .with_step_size(0.25)     .with_threshold(1e-12), }  # Test starting point initial_guess = [-1.0, -3.0]  # Run all optimisers results = {} for name, optimiser in optimisers.items():     result = optimiser.run(problem, initial_guess)     results[name] = result  # Display comparison print(\"\\n\" + \"=\" * 70) print(\"OPTIMISER COMPARISON\") print(\"=\" * 70) print(     f\"{'Optimiser':&lt;15} {'Success':&lt;10} {'Final Value':&lt;15} {'Iterations':&lt;12} {'Evaluations'}\" ) print(\"-\" * 70)  for name, result in results.items():     print(         f\"{name:&lt;15} {str(result.success):&lt;10} {result.value:&lt;15.3e} \"         f\"{result.iterations:&lt;12} {result.evaluations}\"     )  print(\"\\nFinal parameters:\") for name, result in results.items():     print(f\"{name:&lt;15} x = {result.x}\") In\u00a0[7]: Copied! <pre># Create contour plot with all optimisers' results\nx = np.linspace(-2, 2, 200)\ny = np.linspace(-5, 3, 200)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\n\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i, j] = rosenbrock([X[i, j], Y[i, j]])[0]\n\nfig, ax = plt.subplots(figsize=(12, 9))\n\n# Plot contours\nlevels = np.logspace(-1, 3.5, 20)\ncs = ax.contour(X, Y, Z, levels=levels, cmap=\"viridis\", alpha=0.6)\nax.clabel(cs, inline=True, fontsize=8)\n\n# Plot true optimum\nax.plot(1.0, 1.0, \"r*\", markersize=20, label=\"True minimum\", zorder=5)\n\n# Plot starting point\nax.plot(\n    initial_guess[0],\n    initial_guess[1],\n    \"kx\",\n    markersize=15,\n    markeredgewidth=3,\n    label=\"Starting point\",\n    zorder=5,\n)\n\n# Plot optimiser results\ncolors = {\"Nelder-Mead\": \"blue\", \"CMA-ES\": \"green\", \"Adam\": \"orange\"}\nmarkers = {\"Nelder-Mead\": \"o\", \"CMA-ES\": \"s\", \"Adam\": \"^\"}\n\nfor name, result in results.items():\n    ax.plot(\n        result.x[0],\n        result.x[1],\n        marker=markers[name],\n        color=colors[name],\n        markersize=12,\n        label=name,\n        zorder=5,\n        markeredgecolor=\"black\",\n        markeredgewidth=1.5,\n    )\n\nax.set_xlabel(\"x\", fontsize=12)\nax.set_ylabel(\"y\", fontsize=12)\nax.set_title(\"Optimiser Comparison on Rosenbrock Function\", fontsize=14)\nax.grid(True, alpha=0.3)\nax.legend(loc=\"upper left\", fontsize=10)\n\nplt.tight_layout()\nplt.show()\n</pre> # Create contour plot with all optimisers' results x = np.linspace(-2, 2, 200) y = np.linspace(-5, 3, 200) X, Y = np.meshgrid(x, y) Z = np.zeros_like(X)  for i in range(X.shape[0]):     for j in range(X.shape[1]):         Z[i, j] = rosenbrock([X[i, j], Y[i, j]])[0]  fig, ax = plt.subplots(figsize=(12, 9))  # Plot contours levels = np.logspace(-1, 3.5, 20) cs = ax.contour(X, Y, Z, levels=levels, cmap=\"viridis\", alpha=0.6) ax.clabel(cs, inline=True, fontsize=8)  # Plot true optimum ax.plot(1.0, 1.0, \"r*\", markersize=20, label=\"True minimum\", zorder=5)  # Plot starting point ax.plot(     initial_guess[0],     initial_guess[1],     \"kx\",     markersize=15,     markeredgewidth=3,     label=\"Starting point\",     zorder=5, )  # Plot optimiser results colors = {\"Nelder-Mead\": \"blue\", \"CMA-ES\": \"green\", \"Adam\": \"orange\"} markers = {\"Nelder-Mead\": \"o\", \"CMA-ES\": \"s\", \"Adam\": \"^\"}  for name, result in results.items():     ax.plot(         result.x[0],         result.x[1],         marker=markers[name],         color=colors[name],         markersize=12,         label=name,         zorder=5,         markeredgecolor=\"black\",         markeredgewidth=1.5,     )  ax.set_xlabel(\"x\", fontsize=12) ax.set_ylabel(\"y\", fontsize=12) ax.set_title(\"Optimiser Comparison on Rosenbrock Function\", fontsize=14) ax.grid(True, alpha=0.3) ax.legend(loc=\"upper left\", fontsize=10)  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Try multiple starting points\nstarting_points = [\n    [-1.5, -0.5],\n    [1.5, 1.5],\n    [0.0, 2.0],\n    [-1.0, 1.0],\n]\n\noptimiser = diffid.NelderMead().with_max_iter(1000)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STARTING POINT SENSITIVITY\")\nprint(\"=\" * 60)\n\nfor i, start in enumerate(starting_points, 1):\n    result = optimiser.run(problem, start)\n    error = np.linalg.norm(result.x - [1.0, 1.0])\n\n    print(f\"\\nStart {i}: {start}\")\n    print(f\"  Final:      {result.x}\")\n    print(f\"  Iterations: {result.iterations}\")\n    print(f\"  Error:      {error:.3e}\")\n    print(f\"  Success:    {result.success}\")\n</pre> # Try multiple starting points starting_points = [     [-1.5, -0.5],     [1.5, 1.5],     [0.0, 2.0],     [-1.0, 1.0], ]  optimiser = diffid.NelderMead().with_max_iter(1000)  print(\"\\n\" + \"=\" * 60) print(\"STARTING POINT SENSITIVITY\") print(\"=\" * 60)  for i, start in enumerate(starting_points, 1):     result = optimiser.run(problem, start)     error = np.linalg.norm(result.x - [1.0, 1.0])      print(f\"\\nStart {i}: {start}\")     print(f\"  Final:      {result.x}\")     print(f\"  Iterations: {result.iterations}\")     print(f\"  Error:      {error:.3e}\")     print(f\"  Success:    {result.success}\")"},{"location":"tutorials/notebooks/optimisation_basics/#optimisation-basics","title":"Optimisation Basics\u00b6","text":"<p>Objectives:</p> <ul> <li>Understand the ScalarBuilder API</li> <li>Optimise the classic Rosenbrock function</li> <li>Visualise optimisation landscapes with contour plots</li> <li>Compare different optimisers</li> </ul>"},{"location":"tutorials/notebooks/optimisation_basics/#introduction","title":"Introduction\u00b6","text":"<p>The Rosenbrock function is a classic test problem in optimisation. It's defined as:</p> <p>$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$</p> <p>The global minimum is at $(1, 1)$ with $f(1, 1) = 0$. Despite being simple to state, it's challenging for optimisers because of its narrow, curved valley.</p>"},{"location":"tutorials/notebooks/optimisation_basics/#define-the-objective-function","title":"Define the Objective Function\u00b6","text":"<p>In Diffid, objective functions must:</p> <ol> <li>Accept a NumPy array as input</li> <li>Return a NumPy array as output (even for scalar values)</li> </ol>"},{"location":"tutorials/notebooks/optimisation_basics/#build-the-problem","title":"Build the Problem\u00b6","text":"<p>Use <code>ScalarBuilder</code> for direct function optimisation:</p>"},{"location":"tutorials/notebooks/optimisation_basics/#optimise-with-default-settings","title":"Optimise with Default Settings\u00b6","text":"<p>The problem class is constructed as the core object in diffid, as such an <code>optimise()</code> method if provided for fast optimisation. This uses Nelder-Mead by default:</p>"},{"location":"tutorials/notebooks/optimisation_basics/#visualise-the-optimisation-landscape","title":"Visualise the Optimisation Landscape\u00b6","text":"<p>Let's create a contour plot to see the function's shape:</p>"},{"location":"tutorials/notebooks/optimisation_basics/#compare-different-optimisers","title":"Compare Different Optimisers\u00b6","text":"<p>Let's compare Nelder-Mead, CMA-ES, and Adam:</p>"},{"location":"tutorials/notebooks/optimisation_basics/#visualise-all-results","title":"Visualise All Results\u00b6","text":""},{"location":"tutorials/notebooks/optimisation_basics/#effect-of-starting-point","title":"Effect of Starting Point\u00b6","text":"<p>Let's see how starting position affects convergence:</p>"},{"location":"tutorials/notebooks/optimisation_basics/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>ScalarBuilder is used for direct function optimisation</li> <li>Nelder-Mead is provided as the default optimiser - good for small problems</li> <li>CMA-ES is more robust for global search but requires more evaluations</li> <li>Adam can be fast on smooth problems but may struggle on complex landscapes</li> <li>Starting point can significantly affect convergence speed</li> </ol>"},{"location":"tutorials/notebooks/optimisation_basics/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>ODE Fitting with DiffSL - Learn how to fit differential equations</li> <li>Choosing an Optimiser - Detailed optimiser selection guide</li> <li>API Reference: Optimisers - Complete API documentation</li> </ul>"},{"location":"tutorials/notebooks/optimisation_basics/#exercises","title":"Exercises\u00b6","text":"<p>Try these challenges:</p> <ol> <li><p>Rastrigin Function: Implement and optimise: $$f(x, y) = 20 + x^2 + y^2 - 10(\\cos(2\\pi x) + \\cos(2\\pi y))$$</p> </li> <li><p>Parameter Tuning: Experiment with CMA-ES <code>step_size</code> parameter (try 0.1, 0.5, 1.0, 2.0)</p> </li> <li><p>Constraint Handling: How would you restrict the search to $x, y \\in [-2, 2]$?</p> </li> </ol>"},{"location":"tutorials/notebooks/parallel_optimisation/","title":"Parallel Optimisation","text":"In\u00a0[\u00a0]: Copied! <pre>import multiprocessing\nimport time\n\nimport diffid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\n# Detect available cores\nn_cores = multiprocessing.cpu_count()\nprint(f\"Available CPU cores: {n_cores}\")\n</pre> import multiprocessing import time  import diffid import matplotlib.pyplot as plt import numpy as np from scipy.integrate import solve_ivp  # Detect available cores n_cores = multiprocessing.cpu_count() print(f\"Available CPU cores: {n_cores}\") In\u00a0[2]: Copied! <pre># DiffSL model for Lotka-Volterra\nmodel_str = \"\"\"\nin_i { alpha = 1.0, beta = 0.5, delta = 0.1, gamma = 0.5 }\nx0 { 10.0 } y0 { 5.0 }\nu_i {\n    prey = x0,\n    predator = y0,\n}\nF_i {\n    alpha * prey - beta * prey * predator,\n    delta * prey * predator - gamma * predator,\n}\n\"\"\"\n\n# True parameters for data generation\ntrue_params = {\"alpha\": 1.1, \"beta\": 0.4, \"delta\": 0.1, \"gamma\": 0.4}\n\n# Generate synthetic data using scipy\n# Longer time span = more expensive integration\nnp.random.seed(42)\nt_data = np.linspace(0, 100, 500)  # Long integration with many points\n\n\ndef lotka_volterra(t, state, alpha, beta, delta, gamma):\n    x, y = state\n    return [alpha * x - beta * x * y, delta * x * y - gamma * y]\n\n\nsol = solve_ivp(\n    lotka_volterra,\n    [t_data[0], t_data[-1]],\n    [10.0, 5.0],\n    args=(\n        true_params[\"alpha\"],\n        true_params[\"beta\"],\n        true_params[\"delta\"],\n        true_params[\"gamma\"],\n    ),\n    t_eval=t_data,\n    method=\"RK45\",\n)\n\ny_true = sol.y.T\ny_observed = y_true + np.random.normal(0, 0.3, y_true.shape)\n\n# Combine for DiffsolBuilder (time in first column)\ndata = np.column_stack((t_data, y_observed))\n\nprint(f\"Data points: {len(t_data)}\")\nprint(\"Time span: [0, 100]\")\nprint(\"Parameters to fit: 4\")\nprint(f\"True parameters: {list(true_params.values())}\")\n</pre> # DiffSL model for Lotka-Volterra model_str = \"\"\" in_i { alpha = 1.0, beta = 0.5, delta = 0.1, gamma = 0.5 } x0 { 10.0 } y0 { 5.0 } u_i {     prey = x0,     predator = y0, } F_i {     alpha * prey - beta * prey * predator,     delta * prey * predator - gamma * predator, } \"\"\"  # True parameters for data generation true_params = {\"alpha\": 1.1, \"beta\": 0.4, \"delta\": 0.1, \"gamma\": 0.4}  # Generate synthetic data using scipy # Longer time span = more expensive integration np.random.seed(42) t_data = np.linspace(0, 100, 500)  # Long integration with many points   def lotka_volterra(t, state, alpha, beta, delta, gamma):     x, y = state     return [alpha * x - beta * x * y, delta * x * y - gamma * y]   sol = solve_ivp(     lotka_volterra,     [t_data[0], t_data[-1]],     [10.0, 5.0],     args=(         true_params[\"alpha\"],         true_params[\"beta\"],         true_params[\"delta\"],         true_params[\"gamma\"],     ),     t_eval=t_data,     method=\"RK45\", )  y_true = sol.y.T y_observed = y_true + np.random.normal(0, 0.3, y_true.shape)  # Combine for DiffsolBuilder (time in first column) data = np.column_stack((t_data, y_observed))  print(f\"Data points: {len(t_data)}\") print(\"Time span: [0, 100]\") print(\"Parameters to fit: 4\") print(f\"True parameters: {list(true_params.values())}\") <pre>Data points: 500\nTime span: [0, 100]\nParameters to fit: 4\nTrue parameters: [1.1, 0.4, 0.1, 0.4]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Sequential execution (parallel=False)\nprint(\"Running CMA-ES (sequential)...\")\n\nstart = time.time()\n\nresult_seq = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(model_str)\n    .with_data(data)\n    .with_parameter(\"alpha\", 0.8)\n    .with_parameter(\"beta\", 0.3)\n    .with_parameter(\"delta\", 0.05)\n    .with_parameter(\"gamma\", 0.3)\n    .with_cost(diffid.SSE())\n    .with_parallel(False)  # Sequential evaluation\n    .with_optimiser(diffid.CMAES().with_max_iter(100).with_step_size(0.3))\n    .build()\n    .optimise()\n)\n\ntime_seq = time.time() - start\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CMA-ES (Sequential)\")\nprint(\"=\" * 60)\nprint(f\"Solution:      {result_seq.x}\")\nprint(f\"True params:   {list(true_params.values())}\")\nprint(f\"Final SSE:     {result_seq.value:.3f}\")\nprint(f\"Evaluations:   {result_seq.evaluations}\")\nprint(f\"Time:          {time_seq:.2f}s\")\nprint(f\"Time per eval: {time_seq / result_seq.evaluations * 1000:.2f}ms\")\n</pre> # Sequential execution (parallel=False) print(\"Running CMA-ES (sequential)...\")  start = time.time()  result_seq = (     diffid.DiffsolBuilder()     .with_diffsl(model_str)     .with_data(data)     .with_parameter(\"alpha\", 0.8)     .with_parameter(\"beta\", 0.3)     .with_parameter(\"delta\", 0.05)     .with_parameter(\"gamma\", 0.3)     .with_cost(diffid.SSE())     .with_parallel(False)  # Sequential evaluation     .with_optimiser(diffid.CMAES().with_max_iter(100).with_step_size(0.3))     .build()     .optimise() )  time_seq = time.time() - start  print(\"\\n\" + \"=\" * 60) print(\"CMA-ES (Sequential)\") print(\"=\" * 60) print(f\"Solution:      {result_seq.x}\") print(f\"True params:   {list(true_params.values())}\") print(f\"Final SSE:     {result_seq.value:.3f}\") print(f\"Evaluations:   {result_seq.evaluations}\") print(f\"Time:          {time_seq:.2f}s\") print(f\"Time per eval: {time_seq / result_seq.evaluations * 1000:.2f}ms\") In\u00a0[\u00a0]: Copied! <pre># Parallel execution (parallel=True)\nprint(\"Running CMA-ES (parallel)...\")\n\nstart = time.time()\n\nresult_par = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(model_str)\n    .with_data(data)\n    .with_parameter(\"alpha\", 0.8)\n    .with_parameter(\"beta\", 0.3)\n    .with_parameter(\"delta\", 0.05)\n    .with_parameter(\"gamma\", 0.3)\n    .with_cost(diffid.SSE())\n    .with_parallel(True)  # Parallel evaluation!\n    .with_optimiser(diffid.CMAES().with_max_iter(100).with_step_size(0.3))\n    .build()\n    .optimise()\n)\n\ntime_par = time.time() - start\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"CMA-ES (Parallel)\")\nprint(\"=\" * 60)\nprint(f\"Solution:      {result_par.x}\")\nprint(f\"True params:   {list(true_params.values())}\")\nprint(f\"Final SSE:     {result_par.value:.3f}\")\nprint(f\"Evaluations:   {result_par.evaluations}\")\nprint(f\"Time:          {time_par:.2f}s\")\nprint(f\"Time per eval: {time_par / result_par.evaluations * 1000:.2f}ms\")\n\nspeedup = time_seq / time_par\nprint(f\"\\n\ud83d\ude80 Speedup: {speedup:.2f}x (with {n_cores} cores)\")\n</pre> # Parallel execution (parallel=True) print(\"Running CMA-ES (parallel)...\")  start = time.time()  result_par = (     diffid.DiffsolBuilder()     .with_diffsl(model_str)     .with_data(data)     .with_parameter(\"alpha\", 0.8)     .with_parameter(\"beta\", 0.3)     .with_parameter(\"delta\", 0.05)     .with_parameter(\"gamma\", 0.3)     .with_cost(diffid.SSE())     .with_parallel(True)  # Parallel evaluation!     .with_optimiser(diffid.CMAES().with_max_iter(100).with_step_size(0.3))     .build()     .optimise() )  time_par = time.time() - start  print(\"\\n\" + \"=\" * 60) print(\"CMA-ES (Parallel)\") print(\"=\" * 60) print(f\"Solution:      {result_par.x}\") print(f\"True params:   {list(true_params.values())}\") print(f\"Final SSE:     {result_par.value:.3f}\") print(f\"Evaluations:   {result_par.evaluations}\") print(f\"Time:          {time_par:.2f}s\") print(f\"Time per eval: {time_par / result_par.evaluations * 1000:.2f}ms\")  speedup = time_seq / time_par print(f\"\\n\ud83d\ude80 Speedup: {speedup:.2f}x (with {n_cores} cores)\") In\u00a0[\u00a0]: Copied! <pre># Parallel with larger population (more parallel work per generation)\nprint(\"Running CMA-ES (parallel, large population)...\")\n\nstart = time.time()\n\nresult_par_large = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(model_str)\n    .with_data(data)\n    .with_parameter(\"alpha\", 0.8)\n    .with_parameter(\"beta\", 0.3)\n    .with_parameter(\"delta\", 0.05)\n    .with_parameter(\"gamma\", 0.3)\n    .with_cost(diffid.SSE())\n    .with_parallel(True)\n    .with_optimiser(\n        diffid.CMAES()\n        .with_max_iter(50)\n        .with_step_size(0.3)\n        .with_population_size(2 * n_cores)  # Match population to available cores\n    )\n    .build()\n    .optimise()\n)\n\ntime_par_large = time.time() - start\n\nprint(\"\\n\" + \"=\" * 60)\nprint(f\"CMA-ES (Parallel, Population={2 * n_cores})\")\nprint(\"=\" * 60)\nprint(f\"Solution:      {result_par_large.x}\")\nprint(f\"True params:   {list(true_params.values())}\")\nprint(f\"Final SSE:     {result_par_large.value:.3f}\")\nprint(f\"Evaluations:   {result_par_large.evaluations}\")\nprint(f\"Time:          {time_par_large:.2f}s\")\nprint(f\"Time per eval: {time_par_large / result_par_large.evaluations * 1000:.2f}ms\")\n\nspeedup_large = time_seq / time_par_large\nprint(f\"\\n\ud83d\ude80 Speedup: {speedup_large:.2f}x\")\n</pre> # Parallel with larger population (more parallel work per generation) print(\"Running CMA-ES (parallel, large population)...\")  start = time.time()  result_par_large = (     diffid.DiffsolBuilder()     .with_diffsl(model_str)     .with_data(data)     .with_parameter(\"alpha\", 0.8)     .with_parameter(\"beta\", 0.3)     .with_parameter(\"delta\", 0.05)     .with_parameter(\"gamma\", 0.3)     .with_cost(diffid.SSE())     .with_parallel(True)     .with_optimiser(         diffid.CMAES()         .with_max_iter(50)         .with_step_size(0.3)         .with_population_size(2 * n_cores)  # Match population to available cores     )     .build()     .optimise() )  time_par_large = time.time() - start  print(\"\\n\" + \"=\" * 60) print(f\"CMA-ES (Parallel, Population={2 * n_cores})\") print(\"=\" * 60) print(f\"Solution:      {result_par_large.x}\") print(f\"True params:   {list(true_params.values())}\") print(f\"Final SSE:     {result_par_large.value:.3f}\") print(f\"Evaluations:   {result_par_large.evaluations}\") print(f\"Time:          {time_par_large:.2f}s\") print(f\"Time per eval: {time_par_large / result_par_large.evaluations * 1000:.2f}ms\")  speedup_large = time_seq / time_par_large print(f\"\\n\ud83d\ude80 Speedup: {speedup_large:.2f}x\") In\u00a0[\u00a0]: Copied! <pre># Define an expensive objective function\ndef expensive_objective(params):\n    \"\"\"\n    An expensive objective function that simulates computation.\n    In practice, this could be a complex simulation, ML model, etc.\n    \"\"\"\n    alpha, beta, delta, gamma = params\n\n    # Simulate expensive computation (ODE integration with scipy)\n    sol = solve_ivp(\n        lotka_volterra,\n        [0, 100],\n        [10.0, 5.0],\n        args=(alpha, beta, delta, gamma),\n        t_eval=t_data,\n        method=\"RK45\",\n    )\n\n    if not sol.success:\n        return 1e10  # Return large value for failed integrations\n\n    # Compute SSE\n    y_pred = sol.y.T\n    sse = np.sum((y_pred - y_observed) ** 2)\n    return sse\n\n\n# Test single evaluation time\nn_evals = 20\ntest_params = [[0.8 + i * 0.02, 0.3, 0.05, 0.3] for i in range(n_evals)]\n\nprint(f\"Testing {n_evals} sequential evaluations...\")\n\nstart = time.time()\nseq_results = [expensive_objective(p) for p in test_params]\ntime_seq_python = time.time() - start\nprint(\n    f\"Sequential: {time_seq_python:.2f}s ({time_seq_python / n_evals * 1000:.1f}ms/eval)\"\n)\n\n# Note: Multiprocessing in notebooks has pickling limitations\n# In a regular Python script, you would use:\nprint(\"\"\"\n\ud83d\udca1 To parallelise in a script, use multiprocessing:\n\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef evaluate_batch_parallel(params_list, n_workers=8):\n    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n        return list(executor.map(expensive_objective, params_list))\n\n# This provides near-linear speedup for expensive functions\n\"\"\")\n</pre> # Define an expensive objective function def expensive_objective(params):     \"\"\"     An expensive objective function that simulates computation.     In practice, this could be a complex simulation, ML model, etc.     \"\"\"     alpha, beta, delta, gamma = params      # Simulate expensive computation (ODE integration with scipy)     sol = solve_ivp(         lotka_volterra,         [0, 100],         [10.0, 5.0],         args=(alpha, beta, delta, gamma),         t_eval=t_data,         method=\"RK45\",     )      if not sol.success:         return 1e10  # Return large value for failed integrations      # Compute SSE     y_pred = sol.y.T     sse = np.sum((y_pred - y_observed) ** 2)     return sse   # Test single evaluation time n_evals = 20 test_params = [[0.8 + i * 0.02, 0.3, 0.05, 0.3] for i in range(n_evals)]  print(f\"Testing {n_evals} sequential evaluations...\")  start = time.time() seq_results = [expensive_objective(p) for p in test_params] time_seq_python = time.time() - start print(     f\"Sequential: {time_seq_python:.2f}s ({time_seq_python / n_evals * 1000:.1f}ms/eval)\" )  # Note: Multiprocessing in notebooks has pickling limitations # In a regular Python script, you would use: print(\"\"\" \ud83d\udca1 To parallelise in a script, use multiprocessing:  from concurrent.futures import ProcessPoolExecutor  def evaluate_batch_parallel(params_list, n_workers=8):     with ProcessPoolExecutor(max_workers=n_workers) as executor:         return list(executor.map(expensive_objective, params_list))  # This provides near-linear speedup for expensive functions \"\"\") In\u00a0[7]: Copied! <pre># Visualize DiffsolBuilder parallel performance\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Time comparison for DiffsolBuilder\nmethods = [\"Sequential\", \"Parallel\", f\"Parallel\\n(pop={2 * n_cores})\"]\ntimes = [time_seq, time_par, time_par_large]\ncolors = [\"#d62728\", \"#2ca02c\", \"#1f77b4\"]\n\nbars = ax1.bar(methods, times, color=colors, alpha=0.8, edgecolor=\"black\")\nax1.set_ylabel(\"Time (s)\", fontsize=12)\nax1.set_title(\"DiffsolBuilder: Optimisation Time\", fontsize=14, fontweight=\"bold\")\nax1.grid(True, axis=\"y\", alpha=0.3)\n\nfor bar, t in zip(bars, times, strict=False):\n    ax1.text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height(),\n        f\"{t:.2f}s\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=11,\n        fontweight=\"bold\",\n    )\n\n# Single evaluation time for Python callable\nax2.bar(\n    [\"Sequential\\nPython\"],\n    [time_seq_python],\n    color=\"#9467bd\",\n    alpha=0.8,\n    edgecolor=\"black\",\n)\nax2.set_ylabel(\"Time (s)\", fontsize=12)\nax2.set_title(f\"Python Callable: {n_evals} evaluations\", fontsize=14, fontweight=\"bold\")\nax2.grid(True, axis=\"y\", alpha=0.3)\nax2.text(\n    0,\n    time_seq_python,\n    f\"{time_seq_python:.2f}s\",\n    ha=\"center\",\n    va=\"bottom\",\n    fontsize=11,\n    fontweight=\"bold\",\n)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n\ud83d\udca1 Single Python evaluation: {time_seq_python / n_evals * 1000:.1f}ms\")\nprint(\"   With multiprocessing, you could achieve near-linear speedup.\")\n</pre> # Visualize DiffsolBuilder parallel performance fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))  # Time comparison for DiffsolBuilder methods = [\"Sequential\", \"Parallel\", f\"Parallel\\n(pop={2 * n_cores})\"] times = [time_seq, time_par, time_par_large] colors = [\"#d62728\", \"#2ca02c\", \"#1f77b4\"]  bars = ax1.bar(methods, times, color=colors, alpha=0.8, edgecolor=\"black\") ax1.set_ylabel(\"Time (s)\", fontsize=12) ax1.set_title(\"DiffsolBuilder: Optimisation Time\", fontsize=14, fontweight=\"bold\") ax1.grid(True, axis=\"y\", alpha=0.3)  for bar, t in zip(bars, times, strict=False):     ax1.text(         bar.get_x() + bar.get_width() / 2,         bar.get_height(),         f\"{t:.2f}s\",         ha=\"center\",         va=\"bottom\",         fontsize=11,         fontweight=\"bold\",     )  # Single evaluation time for Python callable ax2.bar(     [\"Sequential\\nPython\"],     [time_seq_python],     color=\"#9467bd\",     alpha=0.8,     edgecolor=\"black\", ) ax2.set_ylabel(\"Time (s)\", fontsize=12) ax2.set_title(f\"Python Callable: {n_evals} evaluations\", fontsize=14, fontweight=\"bold\") ax2.grid(True, axis=\"y\", alpha=0.3) ax2.text(     0,     time_seq_python,     f\"{time_seq_python:.2f}s\",     ha=\"center\",     va=\"bottom\",     fontsize=11,     fontweight=\"bold\", )  plt.tight_layout() plt.show()  print(f\"\\n\ud83d\udca1 Single Python evaluation: {time_seq_python / n_evals * 1000:.1f}ms\") print(\"   With multiprocessing, you could achieve near-linear speedup.\") <pre>\n\ud83d\udca1 Single Python evaluation: 9.8ms\n   With multiprocessing, you could achieve near-linear speedup.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Test DiffsolBuilder scaling with data size\ndata_sizes = [100, 200, 500]\nscaling_results = []\n\nprint(\"Testing DiffsolBuilder scaling with data size...\\n\")\n\nfor n_points in data_sizes:\n    # Generate data with different sizes\n    t_test = np.linspace(0, 100, n_points)\n    sol_test = solve_ivp(\n        lotka_volterra,\n        [t_test[0], t_test[-1]],\n        [10.0, 5.0],\n        args=(\n            true_params[\"alpha\"],\n            true_params[\"beta\"],\n            true_params[\"delta\"],\n            true_params[\"gamma\"],\n        ),\n        t_eval=t_test,\n        method=\"RK45\",\n    )\n    y_test = sol_test.y.T + np.random.normal(0, 0.3, (n_points, 2))\n    data_test = np.column_stack((t_test, y_test))\n\n    # Sequential\n    start = time.time()\n    _ = (\n        diffid.DiffsolBuilder()\n        .with_diffsl(model_str)\n        .with_data(data_test)\n        .with_parameter(\"alpha\", 0.8)\n        .with_parameter(\"beta\", 0.3)\n        .with_parameter(\"delta\", 0.05)\n        .with_parameter(\"gamma\", 0.3)\n        .with_cost(diffid.SSE())\n        .with_parallel(False)\n        .with_optimiser(diffid.CMAES().with_max_iter(30).with_step_size(0.3))\n        .build()\n        .optimise()\n    )\n    t_seq_scale = time.time() - start\n\n    # Parallel\n    start = time.time()\n    _ = (\n        diffid.DiffsolBuilder()\n        .with_diffsl(model_str)\n        .with_data(data_test)\n        .with_parameter(\"alpha\", 0.8)\n        .with_parameter(\"beta\", 0.3)\n        .with_parameter(\"delta\", 0.05)\n        .with_parameter(\"gamma\", 0.3)\n        .with_cost(diffid.SSE())\n        .with_parallel(True)\n        .with_optimiser(diffid.CMAES().with_max_iter(30).with_step_size(0.3))\n        .build()\n        .optimise()\n    )\n    t_par_scale = time.time() - start\n\n    sp = t_seq_scale / t_par_scale\n    scaling_results.append(\n        {\n            \"n_points\": n_points,\n            \"time_seq\": t_seq_scale,\n            \"time_par\": t_par_scale,\n            \"speedup\": sp,\n        }\n    )\n    print(\n        f\"N={n_points:3d}: Sequential={t_seq_scale:.2f}s, Parallel={t_par_scale:.2f}s, Speedup={sp:.2f}x\"\n    )\n</pre> # Test DiffsolBuilder scaling with data size data_sizes = [100, 200, 500] scaling_results = []  print(\"Testing DiffsolBuilder scaling with data size...\\n\")  for n_points in data_sizes:     # Generate data with different sizes     t_test = np.linspace(0, 100, n_points)     sol_test = solve_ivp(         lotka_volterra,         [t_test[0], t_test[-1]],         [10.0, 5.0],         args=(             true_params[\"alpha\"],             true_params[\"beta\"],             true_params[\"delta\"],             true_params[\"gamma\"],         ),         t_eval=t_test,         method=\"RK45\",     )     y_test = sol_test.y.T + np.random.normal(0, 0.3, (n_points, 2))     data_test = np.column_stack((t_test, y_test))      # Sequential     start = time.time()     _ = (         diffid.DiffsolBuilder()         .with_diffsl(model_str)         .with_data(data_test)         .with_parameter(\"alpha\", 0.8)         .with_parameter(\"beta\", 0.3)         .with_parameter(\"delta\", 0.05)         .with_parameter(\"gamma\", 0.3)         .with_cost(diffid.SSE())         .with_parallel(False)         .with_optimiser(diffid.CMAES().with_max_iter(30).with_step_size(0.3))         .build()         .optimise()     )     t_seq_scale = time.time() - start      # Parallel     start = time.time()     _ = (         diffid.DiffsolBuilder()         .with_diffsl(model_str)         .with_data(data_test)         .with_parameter(\"alpha\", 0.8)         .with_parameter(\"beta\", 0.3)         .with_parameter(\"delta\", 0.05)         .with_parameter(\"gamma\", 0.3)         .with_cost(diffid.SSE())         .with_parallel(True)         .with_optimiser(diffid.CMAES().with_max_iter(30).with_step_size(0.3))         .build()         .optimise()     )     t_par_scale = time.time() - start      sp = t_seq_scale / t_par_scale     scaling_results.append(         {             \"n_points\": n_points,             \"time_seq\": t_seq_scale,             \"time_par\": t_par_scale,             \"speedup\": sp,         }     )     print(         f\"N={n_points:3d}: Sequential={t_seq_scale:.2f}s, Parallel={t_par_scale:.2f}s, Speedup={sp:.2f}x\"     ) In\u00a0[9]: Copied! <pre># Visualize DiffsolBuilder scaling\nfig, ax = plt.subplots(figsize=(10, 6))\n\nn_pts = [r[\"n_points\"] for r in scaling_results]\nspeedups_scale = [r[\"speedup\"] for r in scaling_results]\n\nax.bar(n_pts, speedups_scale, color=\"#2ca02c\", alpha=0.8, edgecolor=\"black\", width=30)\nax.axhline(y=1.0, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"No speedup\")\nax.axhline(\n    y=n_cores, color=\"blue\", linestyle=\":\", alpha=0.7, label=f\"Ideal ({n_cores}x)\"\n)\n\nax.set_xlabel(\"Number of Data Points\", fontsize=12)\nax.set_ylabel(\"Speedup\", fontsize=12)\nax.set_title(\n    \"DiffsolBuilder Parallel Speedup vs Data Size\", fontsize=14, fontweight=\"bold\"\n)\nax.set_xticks(n_pts)\nax.grid(True, axis=\"y\", alpha=0.3)\nax.legend(fontsize=11)\n\nfor x, y in zip(n_pts, speedups_scale, strict=False):\n    ax.text(x, y + 0.1, f\"{y:.2f}x\", ha=\"center\", fontsize=11, fontweight=\"bold\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\ud83d\udca1 DiffsolBuilder speedup is modest due to efficient solver caching.\")\nprint(\"   For more parallelism, use multiprocessing with Python callables.\")\n</pre> # Visualize DiffsolBuilder scaling fig, ax = plt.subplots(figsize=(10, 6))  n_pts = [r[\"n_points\"] for r in scaling_results] speedups_scale = [r[\"speedup\"] for r in scaling_results]  ax.bar(n_pts, speedups_scale, color=\"#2ca02c\", alpha=0.8, edgecolor=\"black\", width=30) ax.axhline(y=1.0, color=\"gray\", linestyle=\"--\", alpha=0.7, label=\"No speedup\") ax.axhline(     y=n_cores, color=\"blue\", linestyle=\":\", alpha=0.7, label=f\"Ideal ({n_cores}x)\" )  ax.set_xlabel(\"Number of Data Points\", fontsize=12) ax.set_ylabel(\"Speedup\", fontsize=12) ax.set_title(     \"DiffsolBuilder Parallel Speedup vs Data Size\", fontsize=14, fontweight=\"bold\" ) ax.set_xticks(n_pts) ax.grid(True, axis=\"y\", alpha=0.3) ax.legend(fontsize=11)  for x, y in zip(n_pts, speedups_scale, strict=False):     ax.text(x, y + 0.1, f\"{y:.2f}x\", ha=\"center\", fontsize=11, fontweight=\"bold\")  plt.tight_layout() plt.show()  print(\"\\n\ud83d\udca1 DiffsolBuilder speedup is modest due to efficient solver caching.\") print(\"   For more parallelism, use multiprocessing with Python callables.\") <pre>\n\ud83d\udca1 DiffsolBuilder speedup is modest due to efficient solver caching.\n   For more parallelism, use multiprocessing with Python callables.\n</pre> In\u00a0[10]: Copied! <pre># Final summary\nprint(\"=\" * 70)\nprint(\"PERFORMANCE SUMMARY\")\nprint(\"=\" * 70)\n\nprint(\"\\n\ud83d\udcca DiffsolBuilder (rayon parallelism):\")\nprint(f\"   Sequential:     {time_seq:.2f}s\")\nprint(f\"   Parallel:       {time_par:.2f}s  ({time_seq / time_par:.2f}x speedup)\")\nprint(\"   Note: Limited speedup due to efficient solver caching\")\n\nprint(\"\\n\ud83d\udcca Python Callables:\")\nprint(\n    f\"   Sequential ({n_evals} evals): {time_seq_python:.2f}s ({time_seq_python / n_evals * 1000:.1f}ms/eval)\"\n)\nprint(\"   For parallelism, use multiprocessing in scripts\")\n\nprint(f\"\\n\ud83d\udd27 System: {n_cores} CPU cores\")\n</pre> # Final summary print(\"=\" * 70) print(\"PERFORMANCE SUMMARY\") print(\"=\" * 70)  print(\"\\n\ud83d\udcca DiffsolBuilder (rayon parallelism):\") print(f\"   Sequential:     {time_seq:.2f}s\") print(f\"   Parallel:       {time_par:.2f}s  ({time_seq / time_par:.2f}x speedup)\") print(\"   Note: Limited speedup due to efficient solver caching\")  print(\"\\n\ud83d\udcca Python Callables:\") print(     f\"   Sequential ({n_evals} evals): {time_seq_python:.2f}s ({time_seq_python / n_evals * 1000:.1f}ms/eval)\" ) print(\"   For parallelism, use multiprocessing in scripts\")  print(f\"\\n\ud83d\udd27 System: {n_cores} CPU cores\") <pre>======================================================================\nPERFORMANCE SUMMARY\n======================================================================\n\n\ud83d\udcca DiffsolBuilder (rayon parallelism):\n   Sequential:     0.98s\n   Parallel:       0.14s  (6.90x speedup)\n   Note: Limited speedup due to efficient solver caching\n\n\ud83d\udcca Python Callables:\n   Sequential (20 evals): 0.20s (9.8ms/eval)\n   For parallelism, use multiprocessing in scripts\n\n\ud83d\udd27 System: 8 CPU cores\n</pre>"},{"location":"tutorials/notebooks/parallel_optimisation/#parallel-optimisation","title":"Parallel Optimisation\u00b6","text":"<p>Learning Objectives:</p> <ul> <li>Understand which optimisers support parallelism</li> <li>Configure population-based optimisers for parallel execution</li> <li>Benchmark scaling performance</li> <li>Identify performance bottlenecks</li> <li>Apply best practices for thread-safe optimisation</li> </ul> <p>Prerequisites: Optimisation Basics, ODE Fitting, basic understanding of parallelism</p> <p>Runtime: ~10 minutes</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#introduction","title":"Introduction\u00b6","text":"<p>Diffid supports parallel evaluation for population-based optimisers (CMA-ES, Dynamic Nested Sampling). However, the effectiveness depends on the evaluation cost and backend used.</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#key-insights","title":"Key Insights\u00b6","text":"<ol> <li>DiffsolBuilder with <code>.with_parallel(True)</code> uses Rust's rayon for parallel ODE integration</li> <li>Fast evaluations (&lt; 1ms) may see limited speedup due to efficient solver caching</li> <li>Python callables cannot be parallelised with threads (GIL), but multiprocessing works</li> </ol> <p>This tutorial covers:</p> <ul> <li>Parallel ODE fitting with <code>DiffsolBuilder</code></li> <li>Using <code>multiprocessing</code> for expensive Python callables</li> <li>Understanding when parallelism helps</li> </ul>"},{"location":"tutorials/notebooks/parallel_optimisation/#test-problem-lotka-volterra-ode-fitting","title":"Test Problem: Lotka-Volterra ODE Fitting\u00b6","text":"<p>We'll use the predator-prey model from Tutorial 6. ODE integration is computationally expensive, making it ideal for demonstrating parallel speedup.</p> <p>$$\\frac{dx}{dt} = \\alpha x - \\beta xy \\quad \\text{(prey)}$$ $$\\frac{dy}{dt} = \\delta xy - \\gamma y \\quad \\text{(predator)}$$</p> <p>The DiffSL model definition and synthetic data generation:</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#sequential-vs-parallel-diffsolbuilder","title":"Sequential vs Parallel: DiffsolBuilder\u00b6","text":"<p>The key difference is the <code>.with_parallel()</code> method on <code>DiffsolBuilder</code>. When enabled, population-based optimisers evaluate multiple parameter sets concurrently.</p> <p>Let's compare sequential and parallel CMA-ES on our ODE fitting problem:</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#parallel-execution","title":"Parallel Execution\u00b6","text":"<p>Now let's enable parallelism with <code>.with_parallel(True)</code>:</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#parallelising-python-callables-with-multiprocessing","title":"Parallelising Python Callables with Multiprocessing\u00b6","text":"<p>Python's GIL prevents thread-based parallelism for Python code. However, you can use multiprocessing to parallelise expensive Python functions by wrapping them appropriately.</p> <p>Here's how to create a parallel-capable objective function:</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#understanding-parallel-performance","title":"Understanding Parallel Performance\u00b6","text":""},{"location":"tutorials/notebooks/parallel_optimisation/#why-diffsolbuilder-speedups-may-be-modest","title":"Why DiffsolBuilder Speedups May Be Modest\u00b6","text":"<p>DiffsolBuilder uses solver caching - each thread maintains cached ODE solver state that gets reused across evaluations. This makes sequential evaluation very fast, reducing the relative benefit of parallelism.</p> <p>Parallelism helps more when:</p> <ul> <li>Evaluations are expensive (&gt;10ms each)</li> <li>Large populations are used</li> <li>Cache reuse is limited (e.g., parameters vary widely)</li> </ul>"},{"location":"tutorials/notebooks/parallel_optimisation/#multiprocessing-vs-threading","title":"Multiprocessing vs Threading\u00b6","text":"Approach Use Case Overhead <code>.with_parallel(True)</code> DiffsolBuilder only Low (shared memory) <code>multiprocessing</code> Any Python callable Higher (process creation) <code>threading</code> I/O-bound tasks GIL blocks CPU work <p>For expensive Python simulations, multiprocessing provides the best parallelism.</p>"},{"location":"tutorials/notebooks/parallel_optimisation/#when-to-use-each-approach","title":"When to Use Each Approach\u00b6","text":""},{"location":"tutorials/notebooks/parallel_optimisation/#diffsolbuilder-with-with_paralleltrue","title":"DiffsolBuilder with <code>.with_parallel(True)</code>\u00b6","text":"<p>Best for:</p> <ul> <li>ODE fitting problems where evaluations are moderately expensive</li> <li>When you want automatic parallelism without code changes</li> <li>Lower overhead than multiprocessing</li> </ul> <pre>problem = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(model)\n    .with_data(data)\n    .with_parallel(True)  # Enable rayon parallelism\n    .build()\n)\n</pre>"},{"location":"tutorials/notebooks/parallel_optimisation/#multiprocessing-for-python-callables","title":"Multiprocessing for Python Callables\u00b6","text":"<p>Best for:</p> <ul> <li>Expensive Python simulations (&gt;10ms per evaluation)</li> <li>Complex custom objective functions</li> <li>When DiffsolBuilder isn't applicable</li> </ul> <pre>from concurrent.futures import ProcessPoolExecutor\n\ndef evaluate_parallel(params_list):\n    with ProcessPoolExecutor(max_workers=n_cores) as executor:\n        return list(executor.map(expensive_objective, params_list))\n</pre>"},{"location":"tutorials/notebooks/parallel_optimisation/#best-practices-summary","title":"Best Practices Summary\u00b6","text":"Scenario Recommended Approach Expected Speedup DiffsolBuilder ODE fitting <code>.with_parallel(True)</code> 1-2x (limited by caching) Expensive Python callable <code>multiprocessing</code> Near-linear with cores Fast Python callable (&lt;1ms) Sequential N/A (overhead dominates) I/O-bound operations <code>threading</code> Varies"},{"location":"tutorials/notebooks/parallel_optimisation/#tips-for-maximum-speedup","title":"Tips for Maximum Speedup\u00b6","text":"<ol> <li>Profile first: Measure single evaluation time to assess parallel benefit</li> <li>Use multiprocessing for Python: Bypass the GIL for CPU-bound work</li> <li>Match workers to cores: <code>n_workers = multiprocessing.cpu_count()</code></li> <li>Batch evaluations: Amortise process creation overhead</li> <li>Avoid shared state: Ensure objective functions are stateless</li> </ol>"},{"location":"tutorials/notebooks/parallel_optimisation/#performance-summary","title":"Performance Summary\u00b6","text":""},{"location":"tutorials/notebooks/parallel_optimisation/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li><p>DiffsolBuilder supports parallel evaluation with <code>.with_parallel(True)</code>, but speedup is limited by efficient solver caching</p> </li> <li><p>Python callables can achieve excellent parallelism using <code>multiprocessing.ProcessPoolExecutor</code></p> </li> <li><p>Threading doesn't work for CPU-bound Python code due to the GIL</p> </li> <li><p>Profile first - measure single evaluation time to determine if parallelism will help</p> </li> <li><p>Multiprocessing is best for expensive Python simulations (&gt;10ms per evaluation)</p> </li> </ol>"},{"location":"tutorials/notebooks/parallel_optimisation/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Advanced Cost Functions - Custom objective functions</li> <li>API Reference: DiffsolBuilder - Complete API</li> <li>API Reference: CMA-ES - Population configuration</li> </ul>"},{"location":"tutorials/notebooks/parallel_optimisation/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Multiprocessing Integration: Modify the <code>expensive_objective</code> function to use a different ODE system and measure the parallel speedup</p> </li> <li><p>Process Pool Reuse: Create a persistent <code>ProcessPoolExecutor</code> and measure the overhead reduction from reusing it across multiple batches</p> </li> <li><p>Hybrid Approach: Combine DiffsolBuilder's parallel evaluation with multiprocessing by running multiple independent optimisation problems in parallel</p> </li> <li><p>Scaling Study: Measure how multiprocessing speedup changes with:</p> <ul> <li>Number of workers (1, 2, 4, 8, ...)</li> <li>Evaluation cost (1ms, 10ms, 100ms, 1s)</li> <li>Batch size (10, 50, 100, 500)</li> </ul> </li> </ol>"},{"location":"tutorials/notebooks/parameter_uncertainty/","title":"Parameter Uncertainty","text":"In\u00a0[\u00a0]: Copied! <pre># Import plotting utilities\nimport diffid\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom diffid.plotting import parameter_distributions, parameter_traces\n\nnp.random.seed(42)  # For reproducibility\n</pre> # Import plotting utilities import diffid import matplotlib.pyplot as plt import numpy as np from diffid.plotting import parameter_distributions, parameter_traces  np.random.seed(42)  # For reproducibility In\u00a0[\u00a0]: Copied! <pre>def ball_states(t, g, h):\n    \"\"\"Analytical solution for ball trajectory.\"\"\"\n    height = h - 0.5 * g * t**2\n    height = np.maximum(height, 0.0)  # Can't go below ground\n    velocity = -g * t\n    return height, velocity\n\n\n# True parameters\ng_true = 9.81  # m/s\u00b2\nh_true = 10.0  # meters\n\n# Time to hit ground: t = sqrt(2h/g)\nt_stop = np.sqrt(2.0 * h_true / g_true)\nt_final = 0.7 * t_stop  # Stop before hitting ground\nt_span = np.linspace(0.0, t_final, 61)\n\n# Generate clean data\nheight, velocity = ball_states(t_span, g_true, h_true)\n\n# Add measurement noise\nnoise_std = 0.1\nheight_noisy = height + np.random.normal(0, noise_std, len(t_span))\nvelocity_noisy = velocity + np.random.normal(0, noise_std, len(t_span))\n\n# Format for Diffid: [time, height, velocity]\ndata = np.column_stack((t_span, height_noisy, velocity_noisy))\n\nprint(f\"Generated {len(t_span)} observations\")\nprint(f\"Time span: [0, {t_final:.3f}] seconds\")\nprint(f\"Noise level: \u03c3 = {noise_std}\")\nprint(\"\\nTrue parameters:\")\nprint(f\"  g = {g_true} m/s\u00b2\")\nprint(f\"  h = {h_true} m\")\n</pre> def ball_states(t, g, h):     \"\"\"Analytical solution for ball trajectory.\"\"\"     height = h - 0.5 * g * t**2     height = np.maximum(height, 0.0)  # Can't go below ground     velocity = -g * t     return height, velocity   # True parameters g_true = 9.81  # m/s\u00b2 h_true = 10.0  # meters  # Time to hit ground: t = sqrt(2h/g) t_stop = np.sqrt(2.0 * h_true / g_true) t_final = 0.7 * t_stop  # Stop before hitting ground t_span = np.linspace(0.0, t_final, 61)  # Generate clean data height, velocity = ball_states(t_span, g_true, h_true)  # Add measurement noise noise_std = 0.1 height_noisy = height + np.random.normal(0, noise_std, len(t_span)) velocity_noisy = velocity + np.random.normal(0, noise_std, len(t_span))  # Format for Diffid: [time, height, velocity] data = np.column_stack((t_span, height_noisy, velocity_noisy))  print(f\"Generated {len(t_span)} observations\") print(f\"Time span: [0, {t_final:.3f}] seconds\") print(f\"Noise level: \u03c3 = {noise_std}\") print(\"\\nTrue parameters:\") print(f\"  g = {g_true} m/s\u00b2\") print(f\"  h = {h_true} m\") In\u00a0[3]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Height\naxes[0].plot(t_span, height, \"--\", label=\"True\", linewidth=2, alpha=0.7)\naxes[0].plot(t_span, height_noisy, \"o\", label=\"Observed\", alpha=0.6)\naxes[0].set_xlabel(\"Time (s)\")\naxes[0].set_ylabel(\"Height (m)\")\naxes[0].set_title(\"Ball Height\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Velocity\naxes[1].plot(t_span, velocity, \"--\", label=\"True\", linewidth=2, alpha=0.7)\naxes[1].plot(t_span, velocity_noisy, \"o\", label=\"Observed\", alpha=0.6)\naxes[1].set_xlabel(\"Time (s)\")\naxes[1].set_ylabel(\"Velocity (m/s)\")\naxes[1].set_title(\"Ball Velocity\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Height axes[0].plot(t_span, height, \"--\", label=\"True\", linewidth=2, alpha=0.7) axes[0].plot(t_span, height_noisy, \"o\", label=\"Observed\", alpha=0.6) axes[0].set_xlabel(\"Time (s)\") axes[0].set_ylabel(\"Height (m)\") axes[0].set_title(\"Ball Height\") axes[0].legend() axes[0].grid(True, alpha=0.3)  # Velocity axes[1].plot(t_span, velocity, \"--\", label=\"True\", linewidth=2, alpha=0.7) axes[1].plot(t_span, velocity_noisy, \"o\", label=\"Observed\", alpha=0.6) axes[1].set_xlabel(\"Time (s)\") axes[1].set_ylabel(\"Velocity (m/s)\") axes[1].set_title(\"Ball Velocity\") axes[1].legend() axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show() In\u00a0[4]: Copied! <pre># DiffSL model for falling ball\ndsl_model = \"\"\"\nin_i {g = 1, h = 1 }\nu_i {x = h, v = 0}\nF_i {v, -g}\nstop {x}\n\"\"\"\n\nprint(\"DiffSL Model:\")\nprint(dsl_model)\nprint(\"\\nExplanation:\")\nprint(\"  x = height, v = velocity\")\nprint(\"  dx/dt = v (velocity determines height change)\")\nprint(\"  dv/dt = -g (gravity accelerates downward)\")\nprint(\"  stop {x} (terminate when height reaches zero)\")\n</pre> # DiffSL model for falling ball dsl_model = \"\"\" in_i {g = 1, h = 1 } u_i {x = h, v = 0} F_i {v, -g} stop {x} \"\"\"  print(\"DiffSL Model:\") print(dsl_model) print(\"\\nExplanation:\") print(\"  x = height, v = velocity\") print(\"  dx/dt = v (velocity determines height change)\") print(\"  dv/dt = -g (gravity accelerates downward)\") print(\"  stop {x} (terminate when height reaches zero)\") <pre>DiffSL Model:\n\nin_i { g = 1, h = 1 }\nu_i {x = h, v = 0}\nF_i {v, -g}\nstop {x}\n\n\nExplanation:\n  x = height, v = velocity\n  dx/dt = v (velocity determines height change)\n  dv/dt = -g (gravity accelerates downward)\n  stop {x} (terminate when height reaches zero)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Build problem\nbuilder = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl_model)\n    .with_data(data)\n    .with_parameter(\"g\", 5.0)  # Initial guess\n    .with_parameter(\"h\", 5.0)  # Initial guess\n    .with_cost(diffid.RMSE(2.0))  # 2 observables (height + velocity)\n)\n\nproblem = builder.build()\n\n# Optimise\noptimiser = diffid.Adam().with_step_size(0.05).with_max_iter(1500)\nopt_result = optimiser.run(problem, [5.0, 5.0])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OPTIMISATION RESULTS (MAP Estimate)\")\nprint(\"=\" * 60)\nprint(f\"Success: {opt_result.success}\")\nprint(\"\\nFitted parameters:\")\nprint(f\"  g = {opt_result.x[0]:.4f} m/s\u00b2  (true: {g_true})\")\nprint(f\"  h = {opt_result.x[1]:.4f} m     (true: {h_true})\")\nprint(f\"\\nCost: {opt_result.value:.6f}\")\nprint(f\"Iterations: {opt_result.iterations}\")\n\ng_map, h_map = opt_result.x\n</pre> # Build problem builder = (     diffid.DiffsolBuilder()     .with_diffsl(dsl_model)     .with_data(data)     .with_parameter(\"g\", 5.0)  # Initial guess     .with_parameter(\"h\", 5.0)  # Initial guess     .with_cost(diffid.RMSE(2.0))  # 2 observables (height + velocity) )  problem = builder.build()  # Optimise optimiser = diffid.Adam().with_step_size(0.05).with_max_iter(1500) opt_result = optimiser.run(problem, [5.0, 5.0])  print(\"\\n\" + \"=\" * 60) print(\"OPTIMISATION RESULTS (MAP Estimate)\") print(\"=\" * 60) print(f\"Success: {opt_result.success}\") print(\"\\nFitted parameters:\") print(f\"  g = {opt_result.x[0]:.4f} m/s\u00b2  (true: {g_true})\") print(f\"  h = {opt_result.x[1]:.4f} m     (true: {h_true})\") print(f\"\\nCost: {opt_result.value:.6f}\") print(f\"Iterations: {opt_result.iterations}\")  g_map, h_map = opt_result.x In\u00a0[\u00a0]: Copied! <pre># Rebuild problem with GaussianNLL (required for sampling)\nbuilder_sampling = (\n    diffid.DiffsolBuilder()\n    .with_diffsl(dsl_model)\n    .with_data(data)\n    .with_parameter(\"g\", g_map)  # Start from MAP\n    .with_parameter(\"h\", h_map)\n    # .with_parallel(True)\n    .with_cost(diffid.GaussianNLL(variance=noise_std**2))\n)\n\nproblem_sampling = builder_sampling.build()\n\n# Setup MCMC sampler\nsampler = (\n    diffid.MetropolisHastings()\n    .with_num_chains(10)\n    .with_iterations(1000)\n    .with_step_size(0.035)\n)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"MCMC SAMPLING\")\nprint(\"=\" * 60)\nprint(f\"\\nStarting from MAP estimate: g = {g_map:.4f}, h = {h_map:.4f}\")\nprint(\"\\nRunning MCMC... (this may take a minute)\")\n\n# Run sampling\nmcmc_result = sampler.run(problem_sampling, [g_map, h_map])\n\nprint(\"\\nSampling complete!\")\nprint(f\"Samples shape: {mcmc_result.samples.shape}\")\nprint(f\"Acceptance rate: {mcmc_result.acceptance_rate}\")\nprint(\"Target acceptance: 0.20 - 0.40\")\n\nif np.any(mcmc_result.acceptance_rate &lt; 0.15) or np.any(\n    mcmc_result.acceptance_rate &gt; 0.50\n):\n    print(\"\u26a0\ufe0f  Warning: Acceptance rate is outside optimal range\")\n    print(\"   Consider adjusting step_size\")\nelse:\n    print(\"\u2713 Acceptance rate looks good!\")\n</pre> # Rebuild problem with GaussianNLL (required for sampling) builder_sampling = (     diffid.DiffsolBuilder()     .with_diffsl(dsl_model)     .with_data(data)     .with_parameter(\"g\", g_map)  # Start from MAP     .with_parameter(\"h\", h_map)     # .with_parallel(True)     .with_cost(diffid.GaussianNLL(variance=noise_std**2)) )  problem_sampling = builder_sampling.build()  # Setup MCMC sampler sampler = (     diffid.MetropolisHastings()     .with_num_chains(10)     .with_iterations(1000)     .with_step_size(0.035) )  print(\"\\n\" + \"=\" * 60) print(\"MCMC SAMPLING\") print(\"=\" * 60) print(f\"\\nStarting from MAP estimate: g = {g_map:.4f}, h = {h_map:.4f}\") print(\"\\nRunning MCMC... (this may take a minute)\")  # Run sampling mcmc_result = sampler.run(problem_sampling, [g_map, h_map])  print(\"\\nSampling complete!\") print(f\"Samples shape: {mcmc_result.samples.shape}\") print(f\"Acceptance rate: {mcmc_result.acceptance_rate}\") print(\"Target acceptance: 0.20 - 0.40\")  if np.any(mcmc_result.acceptance_rate &lt; 0.15) or np.any(     mcmc_result.acceptance_rate &gt; 0.50 ):     print(\"\u26a0\ufe0f  Warning: Acceptance rate is outside optimal range\")     print(\"   Consider adjusting step_size\") else:     print(\"\u2713 Acceptance rate looks good!\") In\u00a0[7]: Copied! <pre># Plot traces\nfig, axes = parameter_traces(\n    mcmc_result.samples,\n    param_names=[\"g (m/s\u00b2)\", \"h (m)\"],\n    true_values=[g_true, h_true],\n    show=False,\n)\n\nplt.show()\n\nprint(\"\\nWhat to look for in traces:\")\nprint(\"  - Good mixing (wiggly, no trends)\")\nprint(\"  - Stationary (mean stays constant)\")\nprint(\"  - No long excursions\")\nprint(\"  - Rapid exploration of parameter space\")\n</pre> # Plot traces fig, axes = parameter_traces(     mcmc_result.samples,     param_names=[\"g (m/s\u00b2)\", \"h (m)\"],     true_values=[g_true, h_true],     show=False, )  plt.show()  print(\"\\nWhat to look for in traces:\") print(\"  - Good mixing (wiggly, no trends)\") print(\"  - Stationary (mean stays constant)\") print(\"  - No long excursions\") print(\"  - Rapid exploration of parameter space\") <pre>\nWhat to look for in traces:\n  - Good mixing (wiggly, no trends)\n  - Stationary (mean stays constant)\n  - No long excursions\n  - Rapid exploration of parameter space\n</pre> In\u00a0[8]: Copied! <pre># Plot distributions\nfig, axes = parameter_distributions(\n    mcmc_result.samples,\n    param_names=[\"g (m/s\u00b2)\", \"h (m)\"],\n    true_values=[g_true, h_true],\n    show=False,\n)\n\nplt.show()\n</pre> # Plot distributions fig, axes = parameter_distributions(     mcmc_result.samples,     param_names=[\"g (m/s\u00b2)\", \"h (m)\"],     true_values=[g_true, h_true],     show=False, )  plt.show() In\u00a0[9]: Copied! <pre># Burn-in: discard first 20% of samples\nburn_in = int(0.2 * len(mcmc_result.samples))\nsamples_burned = mcmc_result.samples[burn_in:]\n\n# Calculate statistics\ng_samples = samples_burned[:, 0]\nh_samples = samples_burned[:, 1]\n\n# Means and standard deviations\ng_mean = np.mean(g_samples)\ng_std = np.std(g_samples)\nh_mean = np.mean(h_samples)\nh_std = np.std(h_samples)\n\n# 95% credible intervals\ng_ci = np.percentile(g_samples, [2.5, 97.5])\nh_ci = np.percentile(h_samples, [2.5, 97.5])\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"POSTERIOR STATISTICS (after burn-in)\")\nprint(\"=\" * 60)\nprint(f\"Samples used: {len(samples_burned):,}\")\nprint(\"\\nGravitational acceleration (g):\")\nprint(f\"  True value:  {g_true:.4f} m/s\u00b2\")\nprint(f\"  MAP:         {g_map:.4f} m/s\u00b2\")\nprint(f\"  Posterior:   {g_mean:.4f} \u00b1 {g_std:.4f} m/s\u00b2\")\nprint(f\"  95% CI:      [{g_ci[0]:.4f}, {g_ci[1]:.4f}]\")\nprint(\"\\nInitial height (h):\")\nprint(f\"  True value:  {h_true:.4f} m\")\nprint(f\"  MAP:         {h_map:.4f} m\")\nprint(f\"  Posterior:   {h_mean:.4f} \u00b1 {h_std:.4f} m\")\nprint(f\"  95% CI:      [{h_ci[0]:.4f}, {h_ci[1]:.4f}]\")\n\n# Check if true values are in credible intervals\ng_in_ci = g_ci[0] &lt;= g_true &lt;= g_ci[1]\nh_in_ci = h_ci[0] &lt;= h_true &lt;= h_ci[1]\n\nprint(\"\\nTrue values in 95% CI:\")\nprint(f\"  g: {'\u2713' if g_in_ci else '\u2717'}\")\nprint(f\"  h: {'\u2713' if h_in_ci else '\u2717'}\")\n</pre> # Burn-in: discard first 20% of samples burn_in = int(0.2 * len(mcmc_result.samples)) samples_burned = mcmc_result.samples[burn_in:]  # Calculate statistics g_samples = samples_burned[:, 0] h_samples = samples_burned[:, 1]  # Means and standard deviations g_mean = np.mean(g_samples) g_std = np.std(g_samples) h_mean = np.mean(h_samples) h_std = np.std(h_samples)  # 95% credible intervals g_ci = np.percentile(g_samples, [2.5, 97.5]) h_ci = np.percentile(h_samples, [2.5, 97.5])  print(\"\\n\" + \"=\" * 60) print(\"POSTERIOR STATISTICS (after burn-in)\") print(\"=\" * 60) print(f\"Samples used: {len(samples_burned):,}\") print(\"\\nGravitational acceleration (g):\") print(f\"  True value:  {g_true:.4f} m/s\u00b2\") print(f\"  MAP:         {g_map:.4f} m/s\u00b2\") print(f\"  Posterior:   {g_mean:.4f} \u00b1 {g_std:.4f} m/s\u00b2\") print(f\"  95% CI:      [{g_ci[0]:.4f}, {g_ci[1]:.4f}]\") print(\"\\nInitial height (h):\") print(f\"  True value:  {h_true:.4f} m\") print(f\"  MAP:         {h_map:.4f} m\") print(f\"  Posterior:   {h_mean:.4f} \u00b1 {h_std:.4f} m\") print(f\"  95% CI:      [{h_ci[0]:.4f}, {h_ci[1]:.4f}]\")  # Check if true values are in credible intervals g_in_ci = g_ci[0] &lt;= g_true &lt;= g_ci[1] h_in_ci = h_ci[0] &lt;= h_true &lt;= h_ci[1]  print(\"\\nTrue values in 95% CI:\") print(f\"  g: {'\u2713' if g_in_ci else '\u2717'}\") print(f\"  h: {'\u2713' if h_in_ci else '\u2717'}\") <pre>\n============================================================\nPOSTERIOR STATISTICS (after burn-in)\n============================================================\nSamples used: 8,008\n\nGravitational acceleration (g):\n  True value:  9.8100 m/s\u00b2\n  MAP:         9.8035 m/s\u00b2\n  Posterior:   9.8061 \u00b1 0.0210 m/s\u00b2\n  95% CI:      [9.7679, 9.8473]\n\nInitial height (h):\n  True value:  10.0000 m\n  MAP:         9.9841 m\n  Posterior:   9.9834 \u00b1 0.0134 m\n  95% CI:      [9.9569, 10.0096]\n\nTrue values in 95% CI:\n  g: \u2713\n  h: \u2713\n</pre> In\u00a0[10]: Copied! <pre># Scatter plot of joint distribution\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Hexbin plot for large sample sizes\nhb = ax.hexbin(g_samples, h_samples, gridsize=50, cmap=\"Blues\", mincnt=1)\nax.plot(g_true, h_true, \"r*\", markersize=20, label=\"True values\", zorder=5)\nax.plot(g_map, h_map, \"go\", markersize=12, label=\"MAP estimate\", zorder=5)\nax.plot(g_mean, h_mean, \"mo\", markersize=12, label=\"Posterior mean\", zorder=5)\n\nax.set_xlabel(\"g (m/s\u00b2)\", fontsize=12)\nax.set_ylabel(\"h (m)\", fontsize=12)\nax.set_title(\"Joint Posterior Distribution\", fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\n\nplt.colorbar(hb, ax=ax, label=\"Sample density\")\nplt.tight_layout()\nplt.show()\n\n# Calculate correlation\ncorrelation = np.corrcoef(g_samples, h_samples)[0, 1]\nprint(f\"\\nParameter correlation: {correlation:.4f}\")\n\nif abs(correlation) &gt; 0.7:\n    print(\"  High correlation - parameters are not independently identifiable\")\nelif abs(correlation) &gt; 0.3:\n    print(\"  Moderate correlation\")\nelse:\n    print(\"  Low correlation - parameters are nearly independent\")\n</pre> # Scatter plot of joint distribution fig, ax = plt.subplots(figsize=(8, 8))  # Hexbin plot for large sample sizes hb = ax.hexbin(g_samples, h_samples, gridsize=50, cmap=\"Blues\", mincnt=1) ax.plot(g_true, h_true, \"r*\", markersize=20, label=\"True values\", zorder=5) ax.plot(g_map, h_map, \"go\", markersize=12, label=\"MAP estimate\", zorder=5) ax.plot(g_mean, h_mean, \"mo\", markersize=12, label=\"Posterior mean\", zorder=5)  ax.set_xlabel(\"g (m/s\u00b2)\", fontsize=12) ax.set_ylabel(\"h (m)\", fontsize=12) ax.set_title(\"Joint Posterior Distribution\", fontsize=14) ax.legend() ax.grid(True, alpha=0.3)  plt.colorbar(hb, ax=ax, label=\"Sample density\") plt.tight_layout() plt.show()  # Calculate correlation correlation = np.corrcoef(g_samples, h_samples)[0, 1] print(f\"\\nParameter correlation: {correlation:.4f}\")  if abs(correlation) &gt; 0.7:     print(\"  High correlation - parameters are not independently identifiable\") elif abs(correlation) &gt; 0.3:     print(\"  Moderate correlation\") else:     print(\"  Low correlation - parameters are nearly independent\") <pre>\nParameter correlation: 0.2667\n  Low correlation - parameters are nearly independent\n</pre> In\u00a0[11]: Copied! <pre># Take random subset of samples for predictions\nn_pred_samples = 200\nsample_indices = np.random.choice(len(samples_burned), n_pred_samples, replace=False)\n\n# Generate predictions for each sample\npredictions_height = []\npredictions_velocity = []\n\nfor idx in sample_indices:\n    g_sample, h_sample = samples_burned[idx]\n    h_pred, v_pred = ball_states(t_span, g_sample, h_sample)\n    predictions_height.append(h_pred)\n    predictions_velocity.append(v_pred)\n\npredictions_height = np.array(predictions_height)\npredictions_velocity = np.array(predictions_velocity)\n\n# Calculate percentiles\nh_median = np.median(predictions_height, axis=0)\nh_lower = np.percentile(predictions_height, 2.5, axis=0)\nh_upper = np.percentile(predictions_height, 97.5, axis=0)\n\nv_median = np.median(predictions_velocity, axis=0)\nv_lower = np.percentile(predictions_velocity, 2.5, axis=0)\nv_upper = np.percentile(predictions_velocity, 97.5, axis=0)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Height\naxes[0].fill_between(t_span, h_lower, h_upper, alpha=0.3, label=\"95% CI\")\naxes[0].plot(t_span, h_median, \"-\", linewidth=2, label=\"Median prediction\")\naxes[0].plot(t_span, height, \"--\", linewidth=2, label=\"True\", alpha=0.7)\naxes[0].plot(t_span, height_noisy, \"o\", label=\"Observed\", alpha=0.4, markersize=4)\naxes[0].set_xlabel(\"Time (s)\")\naxes[0].set_ylabel(\"Height (m)\")\naxes[0].set_title(\"Posterior Predictive: Height\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Velocity\naxes[1].fill_between(t_span, v_lower, v_upper, alpha=0.3, label=\"95% CI\")\naxes[1].plot(t_span, v_median, \"-\", linewidth=2, label=\"Median prediction\")\naxes[1].plot(t_span, velocity, \"--\", linewidth=2, label=\"True\", alpha=0.7)\naxes[1].plot(t_span, velocity_noisy, \"o\", label=\"Observed\", alpha=0.4, markersize=4)\naxes[1].set_xlabel(\"Time (s)\")\naxes[1].set_ylabel(\"Velocity (m/s)\")\naxes[1].set_title(\"Posterior Predictive: Velocity\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"The shaded region shows the 95% credible interval for predictions\")\nprint(\"This accounts for both parameter uncertainty and observation noise\")\n</pre> # Take random subset of samples for predictions n_pred_samples = 200 sample_indices = np.random.choice(len(samples_burned), n_pred_samples, replace=False)  # Generate predictions for each sample predictions_height = [] predictions_velocity = []  for idx in sample_indices:     g_sample, h_sample = samples_burned[idx]     h_pred, v_pred = ball_states(t_span, g_sample, h_sample)     predictions_height.append(h_pred)     predictions_velocity.append(v_pred)  predictions_height = np.array(predictions_height) predictions_velocity = np.array(predictions_velocity)  # Calculate percentiles h_median = np.median(predictions_height, axis=0) h_lower = np.percentile(predictions_height, 2.5, axis=0) h_upper = np.percentile(predictions_height, 97.5, axis=0)  v_median = np.median(predictions_velocity, axis=0) v_lower = np.percentile(predictions_velocity, 2.5, axis=0) v_upper = np.percentile(predictions_velocity, 97.5, axis=0)  # Plot fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Height axes[0].fill_between(t_span, h_lower, h_upper, alpha=0.3, label=\"95% CI\") axes[0].plot(t_span, h_median, \"-\", linewidth=2, label=\"Median prediction\") axes[0].plot(t_span, height, \"--\", linewidth=2, label=\"True\", alpha=0.7) axes[0].plot(t_span, height_noisy, \"o\", label=\"Observed\", alpha=0.4, markersize=4) axes[0].set_xlabel(\"Time (s)\") axes[0].set_ylabel(\"Height (m)\") axes[0].set_title(\"Posterior Predictive: Height\") axes[0].legend() axes[0].grid(True, alpha=0.3)  # Velocity axes[1].fill_between(t_span, v_lower, v_upper, alpha=0.3, label=\"95% CI\") axes[1].plot(t_span, v_median, \"-\", linewidth=2, label=\"Median prediction\") axes[1].plot(t_span, velocity, \"--\", linewidth=2, label=\"True\", alpha=0.7) axes[1].plot(t_span, velocity_noisy, \"o\", label=\"Observed\", alpha=0.4, markersize=4) axes[1].set_xlabel(\"Time (s)\") axes[1].set_ylabel(\"Velocity (m/s)\") axes[1].set_title(\"Posterior Predictive: Velocity\") axes[1].legend() axes[1].grid(True, alpha=0.3)  plt.tight_layout() plt.show()  print(\"The shaded region shows the 95% credible interval for predictions\") print(\"This accounts for both parameter uncertainty and observation noise\") <pre>The shaded region shows the 95% credible interval for predictions\nThis accounts for both parameter uncertainty and observation noise\n</pre>"},{"location":"tutorials/notebooks/parameter_uncertainty/#parameter-uncertainty","title":"Parameter Uncertainty\u00b6","text":"<p>Learning Objectives:</p> <ul> <li>Go from optimisation to uncertainty quantification</li> <li>Use MCMC sampling to explore parameter distributions</li> <li>Interpret MCMC diagnostics and traces</li> <li>Calculate confidence intervals</li> </ul>"},{"location":"tutorials/notebooks/parameter_uncertainty/#introduction","title":"Introduction\u00b6","text":"<p>Optimisation gives us a single best parameter estimate. But how certain are we about these values?</p> <p>MCMC (Markov Chain Monte Carlo) sampling explores the full posterior distribution, letting us:</p> <ul> <li>Quantify parameter uncertainty</li> <li>Calculate confidence intervals</li> <li>Detect parameter correlations</li> <li>Make probabilistic predictions</li> </ul> <p>We'll use a bouncy ball physics model as our example.</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#the-physics-problem-falling-ball","title":"The Physics Problem: Falling Ball\u00b6","text":"<p>A ball falls from height $h$ with gravitational acceleration $g$:</p> <p>$$\\begin{aligned} \\frac{dx}{dt} &amp;= v \\\\ \\frac{dv}{dt} &amp;= -g \\end{aligned}$$</p> <p>where:</p> <ul> <li>$x$ is height</li> <li>$v$ is velocity</li> <li>$g$ is gravitational acceleration (Earth: ~9.81 m/s\u00b2)</li> <li>$h$ is initial height</li> </ul> <p>Task: Estimate $g$ and $h$ from noisy observations.</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#generate-synthetic-data","title":"Generate Synthetic Data\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#visualize-the-data","title":"Visualize the Data\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#define-the-ode-model","title":"Define the ODE Model\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#step-1-optimisation","title":"Step 1: Optimisation\u00b6","text":"<p>First, find the maximum a posteriori (MAP) estimate:</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#step-2-mcmc-sampling","title":"Step 2: MCMC Sampling\u00b6","text":"<p>Now explore the full posterior distribution:</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#analyze-mcmc-results","title":"Analyze MCMC Results\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#parameter-traces","title":"Parameter Traces\u00b6","text":"<p>Trace plots show how parameters evolved during sampling:</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#parameter-distributions","title":"Parameter Distributions\u00b6","text":"<p>Histograms show the posterior distribution:</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#calculate-statistics","title":"Calculate Statistics\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#parameter-correlations","title":"Parameter Correlations\u00b6","text":""},{"location":"tutorials/notebooks/parameter_uncertainty/#posterior-predictive-distribution","title":"Posterior Predictive Distribution\u00b6","text":"<p>Use samples to make probabilistic predictions:</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>Optimisation gives point estimates; MCMC quantifies uncertainty</li> <li>GaussianNLL cost metric is required for sampling</li> <li>Acceptance rate should be 20-40% for efficient exploration</li> <li>Burn-in period discards initial non-stationary samples</li> <li>Credible intervals provide uncertainty bounds</li> <li>Posterior predictive distributions account for parameter uncertainty</li> </ol>"},{"location":"tutorials/notebooks/parameter_uncertainty/#mcmc-diagnostics-checklist","title":"MCMC Diagnostics Checklist\u00b6","text":"<p>\u2713 Acceptance rate in [0.2, 0.4] \u2713 Trace plots show good mixing \u2713 No trends in traces \u2713 Distributions look reasonable \u2713 True values in credible intervals</p>"},{"location":"tutorials/notebooks/parameter_uncertainty/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Model Comparison - Use nested sampling for Bayes factors</li> <li>Choosing a Sampler - MCMC vs Nested Sampling</li> <li>API Reference: Samplers</li> </ul>"},{"location":"tutorials/notebooks/parameter_uncertainty/#exercises","title":"Exercises\u00b6","text":"<ol> <li><p>Step Size: Try different <code>step_size</code> values (0.1, 0.5, 1.0) and observe acceptance rates</p> </li> <li><p>More Data: Increase the number of observations and see how uncertainty decreases</p> </li> <li><p>More Noise: Increase <code>noise_std</code> to 0.5 and observe wider credible intervals</p> </li> <li><p>Longer Chains: Run with 10,000 iterations per chain for better convergence</p> </li> </ol>"}]}